==PROF== Connected to process 138958 (/teamspace/studios/this_studio/QuantizedMHA/bin/profile_fa_tc_v1)
==PROF== Profiling "extract_mat" - 0: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 1: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 2: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 3: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 4: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 5: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 6: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 7: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 8: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 9: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 10: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 11: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 12: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 13: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 14: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 15: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 16: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 17: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 18: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 19: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 20: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 21: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 22: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 23: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 24: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 25: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 26: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 27: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 28: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 29: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 30: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 31: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 32: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 33: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 34: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 35: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 36: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 37: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 38: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 39: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 40: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 41: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 42: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 43: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 44: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 45: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 46: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 47: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 48: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 49: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 50: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 51: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 52: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 53: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 54: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 55: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 56: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 57: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 58: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 59: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 60: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 61: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 62: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 63: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 64: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 65: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 66: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 67: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 68: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 69: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 70: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 71: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 72: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 73: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 74: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 75: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 76: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 77: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 78: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 79: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 80: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 81: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 82: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 83: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 84: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 85: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 86: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 87: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 88: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 89: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 90: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 91: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 92: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 93: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 94: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 95: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 96: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 97: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 98: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 99: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 100: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 101: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 102: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 103: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 104: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 105: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 106: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 107: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 108: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 109: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 110: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 111: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 112: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 113: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 114: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 115: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 116: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 117: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 118: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 119: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 120: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 121: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 122: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 123: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 124: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 125: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 126: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 127: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 128: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 129: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 130: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 131: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 132: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 133: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 134: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 135: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 136: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 137: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 138: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 139: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 140: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 141: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 142: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 143: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 144: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 145: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 146: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 147: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 148: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 149: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 150: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 151: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 152: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 153: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 154: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 155: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 156: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 157: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 158: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 159: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 160: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 161: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 162: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 163: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 164: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 165: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 166: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 167: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 168: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 169: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 170: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 171: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 172: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 173: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 174: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 175: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 176: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 177: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 178: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 179: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 180: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 181: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 182: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 183: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 184: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 185: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 186: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 187: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 188: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 189: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 190: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 191: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 192: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 193: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 194: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 195: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 196: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 197: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 198: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 199: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 200: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 201: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 202: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 203: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 204: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 205: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 206: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 207: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 208: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 209: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 210: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 211: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 212: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 213: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 214: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 215: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 216: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 217: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 218: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 219: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 220: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 221: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 222: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 223: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 224: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 225: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 226: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 227: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 228: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 229: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 230: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 231: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 232: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 233: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 234: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 235: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 236: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 237: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 238: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 239: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 240: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 241: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 242: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 243: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 244: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 245: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 246: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 247: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 248: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 249: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 250: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 251: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 252: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 253: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 254: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 255: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 256: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 257: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 258: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 259: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 260: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 261: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 262: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 263: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 264: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 265: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 266: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 267: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 268: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 269: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 270: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 271: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 272: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 273: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 274: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 275: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 276: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 277: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 278: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 279: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 280: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 281: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 282: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 283: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 284: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 285: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 286: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 287: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 288: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 289: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 290: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 291: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 292: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 293: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 294: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 295: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 296: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 297: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 298: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 299: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 300: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 301: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 302: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 303: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 304: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 305: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 306: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 307: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 308: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 309: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 310: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 311: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 312: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 313: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 314: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 315: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 316: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 317: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 318: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 319: 0%....50%....100% - 8 passes
Initializing host data (constant values for correctness check)...
Running correctness check 
Loaded reference output from .cache/ref_N8192_d1024.bin
Correctness check PASSED.
Loaded input matrices from .cache/input_random_N8192_d1024.bin
Running 0 warmup iterations...
Running 1 profiling iterations...
Profiling complete.
==PROF== Disconnected from process 138958
[138958] profile_fa_tc_v1@127.0.0.1
  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       783.21
    Elapsed Cycles                cycle         7123
    Memory Throughput                 %        49.74
    DRAM Throughput                   %        49.74
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        24.08
    L2 Cache Throughput               %        28.00
    SM Active Cycles              cycle      4889.14
    Compute (SM) Throughput           %        16.06
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.59
    Achieved Active Warps Per SM           warp        35.80
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.41%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27757.33
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      4889.14
    Total L1 Elapsed Cycles          cycle       407946
    Average L2 Active Cycles         cycle      4482.75
    Total L2 Elapsed Cycles          cycle       177096
    Average SM Active Cycles         cycle      4889.14
    Total SM Elapsed Cycles          cycle       407946
    Average SMSP Active Cycles       cycle      4902.15
    Total SMSP Elapsed Cycles        cycle      1631784
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       786.37
    Elapsed Cycles                cycle         7249
    Memory Throughput                 %        48.83
    DRAM Throughput                   %        48.83
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        24.18
    L2 Cache Throughput               %        27.49
    SM Active Cycles              cycle      4914.93
    Compute (SM) Throughput           %        16.13
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.31
    Achieved Active Warps Per SM           warp        35.67
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.69%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27749.33
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4914.93
    Total L1 Elapsed Cycles          cycle       406202
    Average L2 Active Cycles         cycle      4432.96
    Total L2 Elapsed Cycles          cycle       180432
    Average SM Active Cycles         cycle      4914.93
    Total SM Elapsed Cycles          cycle       406202
    Average SMSP Active Cycles       cycle      4876.36
    Total SMSP Elapsed Cycles        cycle      1624808
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.20
    Elapsed Cycles                cycle         7182
    Memory Throughput                 %        49.30
    DRAM Throughput                   %        49.30
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.79
    L2 Cache Throughput               %        27.71
    SM Active Cycles              cycle      4894.90
    Compute (SM) Throughput           %        15.87
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.34
    Achieved Active Warps Per SM           warp        35.68
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.66%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27768
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4894.90
    Total L1 Elapsed Cycles          cycle       413068
    Average L2 Active Cycles         cycle      4510.67
    Total L2 Elapsed Cycles          cycle       178944
    Average SM Active Cycles         cycle      4894.90
    Total SM Elapsed Cycles          cycle       413068
    Average SMSP Active Cycles       cycle      5042.30
    Total SMSP Elapsed Cycles        cycle      1652272
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4643931
    Memory Throughput                 %        48.92
    DRAM Throughput                   %         0.22
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.58
    SM Active Cycles              cycle   3592128.71
    Compute (SM) Throughput           %        45.41
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     80197.33
    Total DRAM Elapsed Cycles        cycle    218869760
    Average L1 Active Cycles         cycle   3592128.71
    Total L1 Elapsed Cycles          cycle    269350630
    Average L2 Active Cycles         cycle   1207428.75
    Total L2 Elapsed Cycles          cycle    115659840
    Average SM Active Cycles         cycle   3592128.71
    Total SM Elapsed Cycles          cycle    269350630
    Average SMSP Active Cycles       cycle   3592164.37
    Total SMSP Elapsed Cycles        cycle   1077402520
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.48%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.60% above the average, while the minimum instance value is 7.93% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.52%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.65% above the average, while the minimum instance value is 7.89% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.48%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.60% above the average, while the minimum instance value is 7.93% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       781.73
    Elapsed Cycles                cycle         6555
    Memory Throughput                 %        46.01
    DRAM Throughput                   %        46.01
    Duration                         us         8.38
    L1/TEX Cache Throughput           %        25.79
    L2 Cache Throughput               %        25.69
    SM Active Cycles              cycle      4381.53
    Compute (SM) Throughput           %        17.22
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.91
    Achieved Active Warps Per SM           warp        35.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.09%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23792
    Total DRAM Elapsed Cycles        cycle       310272
    Average L1 Active Cycles         cycle      4381.53
    Total L1 Elapsed Cycles          cycle       380688
    Average L2 Active Cycles         cycle      4025.29
    Total L2 Elapsed Cycles          cycle       163272
    Average SM Active Cycles         cycle      4381.53
    Total SM Elapsed Cycles          cycle       380688
    Average SMSP Active Cycles       cycle      4417.87
    Total SMSP Elapsed Cycles        cycle      1522752
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.06
    Elapsed Cycles                cycle         7310
    Memory Throughput                 %        48.49
    DRAM Throughput                   %        48.49
    Duration                         us         9.31
    L1/TEX Cache Throughput           %        23.65
    L2 Cache Throughput               %        27.25
    SM Active Cycles              cycle      4960.64
    Compute (SM) Throughput           %        15.78
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.51
    Achieved Active Warps Per SM           warp        34.80
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.49%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27805.33
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      4960.64
    Total L1 Elapsed Cycles          cycle       415328
    Average L2 Active Cycles         cycle      4515.83
    Total L2 Elapsed Cycles          cycle       182016
    Average SM Active Cycles         cycle      4960.64
    Total SM Elapsed Cycles          cycle       415328
    Average SMSP Active Cycles       cycle      4927.63
    Total SMSP Elapsed Cycles        cycle      1661312
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.03
    Elapsed Cycles                cycle         7407
    Memory Throughput                 %        47.91
    DRAM Throughput                   %        47.91
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.76
    L2 Cache Throughput               %        26.90
    SM Active Cycles              cycle      5049.07
    Compute (SM) Throughput           %        15.85
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.45
    Achieved Active Warps Per SM           warp        35.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.55%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27802.67
    Total DRAM Elapsed Cycles        cycle       348160
    Average L1 Active Cycles         cycle      5049.07
    Total L1 Elapsed Cycles          cycle       413598
    Average L2 Active Cycles         cycle      4435.12
    Total L2 Elapsed Cycles          cycle       184464
    Average SM Active Cycles         cycle      5049.07
    Total SM Elapsed Cycles          cycle       413598
    Average SMSP Active Cycles       cycle      4848.31
    Total SMSP Elapsed Cycles        cycle      1654392
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.73
    Elapsed Cycles                cycle         7192
    Memory Throughput                 %        49.13
    DRAM Throughput                   %        49.13
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        24.17
    L2 Cache Throughput               %        27.74
    SM Active Cycles              cycle      5006.22
    Compute (SM) Throughput           %        16.13
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.89
    Achieved Active Warps Per SM           warp        34.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.11%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27754.67
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      5006.22
    Total L1 Elapsed Cycles          cycle       406392
    Average L2 Active Cycles         cycle      4505.21
    Total L2 Elapsed Cycles          cycle       178968
    Average SM Active Cycles         cycle      5006.22
    Total SM Elapsed Cycles          cycle       406392
    Average SMSP Active Cycles       cycle      4921.97
    Total SMSP Elapsed Cycles        cycle      1625568
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       796.70
    Elapsed Cycles                cycle      4642929
    Memory Throughput                 %        48.99
    DRAM Throughput                   %         0.21
    Duration                         ms         5.82
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.60
    SM Active Cycles              cycle   3592081.31
    Compute (SM) Throughput           %        45.47
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76794.67
    Total DRAM Elapsed Cycles        cycle    218214400
    Average L1 Active Cycles         cycle   3592081.31
    Total L1 Elapsed Cycles          cycle    268967976
    Average L2 Active Cycles         cycle   1222305.71
    Total L2 Elapsed Cycles          cycle    115318536
    Average SM Active Cycles         cycle   3592081.31
    Total SM Elapsed Cycles          cycle    268967976
    Average SMSP Active Cycles       cycle   3591595.38
    Total SMSP Elapsed Cycles        cycle   1075871904
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.5%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.60% above the average, while the minimum instance value is 8.00% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.48%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.57% above the average, while the minimum instance value is 7.95% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.5%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.60% above the average, while the minimum instance value is 8.00% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.06
    Elapsed Cycles                cycle         6805
    Memory Throughput                 %        44.94
    DRAM Throughput                   %        44.94
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        25.53
    L2 Cache Throughput               %        24.75
    SM Active Cycles              cycle      4425.24
    Compute (SM) Throughput           %        17.26
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.85
    Achieved Active Warps Per SM           warp        35.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.15%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24005.33
    Total DRAM Elapsed Cycles        cycle       320512
    Average L1 Active Cycles         cycle      4425.24
    Total L1 Elapsed Cycles          cycle       379610
    Average L2 Active Cycles         cycle      3998.92
    Total L2 Elapsed Cycles          cycle       169320
    Average SM Active Cycles         cycle      4425.24
    Total SM Elapsed Cycles          cycle       379610
    Average SMSP Active Cycles       cycle      4367.19
    Total SMSP Elapsed Cycles        cycle      1518440
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.11
    Elapsed Cycles                cycle         7098
    Memory Throughput                 %        49.80
    DRAM Throughput                   %        49.80
    Duration                         us         9.06
    L1/TEX Cache Throughput           %        24.06
    L2 Cache Throughput               %        28.04
    SM Active Cycles              cycle      5010.79
    Compute (SM) Throughput           %        16.08
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.39
    Achieved Active Warps Per SM           warp        34.75
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.61%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27794.67
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      5010.79
    Total L1 Elapsed Cycles          cycle       407586
    Average L2 Active Cycles         cycle      4496.79
    Total L2 Elapsed Cycles          cycle       176592
    Average SM Active Cycles         cycle      5010.79
    Total SM Elapsed Cycles          cycle       407586
    Average SMSP Active Cycles       cycle      4912.56
    Total SMSP Elapsed Cycles        cycle      1630344
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       783.16
    Elapsed Cycles                cycle         7122
    Memory Throughput                 %        49.72
    DRAM Throughput                   %        49.72
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.96
    L2 Cache Throughput               %        27.94
    SM Active Cycles              cycle      5046.33
    Compute (SM) Throughput           %        16.02
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.09
    Achieved Active Warps Per SM           warp        34.60
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27749.33
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      5046.33
    Total L1 Elapsed Cycles          cycle       408964
    Average L2 Active Cycles         cycle      4461.42
    Total L2 Elapsed Cycles          cycle       177240
    Average SM Active Cycles         cycle      5046.33
    Total SM Elapsed Cycles          cycle       408964
    Average SMSP Active Cycles       cycle      4862.74
    Total SMSP Elapsed Cycles        cycle      1635856
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       782.55
    Elapsed Cycles                cycle         7214
    Memory Throughput                 %        48.99
    DRAM Throughput                   %        48.99
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.66
    L2 Cache Throughput               %        27.56
    SM Active Cycles              cycle      4900.78
    Compute (SM) Throughput           %        15.82
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.70
    Achieved Active Warps Per SM           warp        35.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.3%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27840
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4900.78
    Total L1 Elapsed Cycles          cycle       414228
    Average L2 Active Cycles         cycle      4457.04
    Total L2 Elapsed Cycles          cycle       179616
    Average SM Active Cycles         cycle      4900.78
    Total SM Elapsed Cycles          cycle       414228
    Average SMSP Active Cycles       cycle      4987.12
    Total SMSP Elapsed Cycles        cycle      1656912
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4638747
    Memory Throughput                 %        48.96
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.24
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3592358.21
    Compute (SM) Throughput           %        45.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77426.67
    Total DRAM Elapsed Cycles        cycle    218625024
    Average L1 Active Cycles         cycle   3592358.21
    Total L1 Elapsed Cycles          cycle    269127966
    Average L2 Active Cycles         cycle   1201051.46
    Total L2 Elapsed Cycles          cycle    115531080
    Average SM Active Cycles         cycle   3592358.21
    Total SM Elapsed Cycles          cycle    269127966
    Average SMSP Active Cycles       cycle   3592611.19
    Total SMSP Elapsed Cycles        cycle   1076511864
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.48%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.57% above the average, while the minimum instance value is 7.86% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.53%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.64% above the average, while the minimum instance value is 7.91% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.48%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.57% above the average, while the minimum instance value is 7.86% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.41
    Elapsed Cycles                cycle         6523
    Memory Throughput                 %        46.48
    DRAM Throughput                   %        46.48
    Duration                         us         8.32
    L1/TEX Cache Throughput           %        25.34
    L2 Cache Throughput               %        25.83
    SM Active Cycles              cycle      4459.88
    Compute (SM) Throughput           %        17.24
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.47
    Achieved Active Warps Per SM           warp        34.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.53%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23797.33
    Total DRAM Elapsed Cycles        cycle       307200
    Average L1 Active Cycles         cycle      4459.88
    Total L1 Elapsed Cycles          cycle       380132
    Average L2 Active Cycles         cycle      4004.83
    Total L2 Elapsed Cycles          cycle       162288
    Average SM Active Cycles         cycle      4459.88
    Total SM Elapsed Cycles          cycle       380132
    Average SMSP Active Cycles       cycle      4395.19
    Total SMSP Elapsed Cycles        cycle      1520528
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.02
    Elapsed Cycles                cycle         7253
    Memory Throughput                 %        48.62
    DRAM Throughput                   %        48.62
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.60
    L2 Cache Throughput               %        27.41
    SM Active Cycles              cycle      4968.34
    Compute (SM) Throughput           %        15.78
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.47
    Achieved Active Warps Per SM           warp        34.78
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.53%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27712
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle      4968.34
    Total L1 Elapsed Cycles          cycle       415428
    Average L2 Active Cycles         cycle         4490
    Total L2 Elapsed Cycles          cycle       180528
    Average SM Active Cycles         cycle      4968.34
    Total SM Elapsed Cycles          cycle       415428
    Average SMSP Active Cycles       cycle      4907.06
    Total SMSP Elapsed Cycles        cycle      1661712
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.79
    Elapsed Cycles                cycle         7235
    Memory Throughput                 %        48.77
    DRAM Throughput                   %        48.77
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        24.33
    L2 Cache Throughput               %        27.50
    SM Active Cycles              cycle      4977.81
    Compute (SM) Throughput           %        16.26
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.34
    Achieved Active Warps Per SM           warp        35.20
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.66%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27714.67
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4977.81
    Total L1 Elapsed Cycles          cycle       403078
    Average L2 Active Cycles         cycle      4397.62
    Total L2 Elapsed Cycles          cycle       180024
    Average SM Active Cycles         cycle      4977.81
    Total SM Elapsed Cycles          cycle       403078
    Average SMSP Active Cycles       cycle      4800.18
    Total SMSP Elapsed Cycles        cycle      1612312
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       785.03
    Elapsed Cycles                cycle         7193
    Memory Throughput                 %        49.00
    DRAM Throughput                   %        49.00
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.49
    L2 Cache Throughput               %        27.65
    SM Active Cycles              cycle      4969.66
    Compute (SM) Throughput           %        15.70
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.99
    Achieved Active Warps Per SM           warp        35.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.01%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27765.33
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      4969.66
    Total L1 Elapsed Cycles          cycle       417436
    Average L2 Active Cycles         cycle      4464.79
    Total L2 Elapsed Cycles          cycle       179064
    Average SM Active Cycles         cycle      4969.66
    Total SM Elapsed Cycles          cycle       417436
    Average SMSP Active Cycles       cycle      4878.50
    Total SMSP Elapsed Cycles        cycle      1669744
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4644106
    Memory Throughput                 %        48.94
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3591910.55
    Compute (SM) Throughput           %        45.43
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        76704
    Total DRAM Elapsed Cycles        cycle    218878976
    Average L1 Active Cycles         cycle   3591910.55
    Total L1 Elapsed Cycles          cycle    269243560
    Average L2 Active Cycles         cycle   1224876.75
    Total L2 Elapsed Cycles          cycle    115664424
    Average SM Active Cycles         cycle   3591910.55
    Total SM Elapsed Cycles          cycle    269243560
    Average SMSP Active Cycles       cycle   3591532.45
    Total SMSP Elapsed Cycles        cycle   1076974240
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.5%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.61% above the average, while the minimum instance value is 7.97% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.45%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.55% above the average, while the minimum instance value is 7.87% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.5%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.61% above the average, while the minimum instance value is 7.97% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.23
    Elapsed Cycles                cycle         6842
    Memory Throughput                 %        44.65
    DRAM Throughput                   %        44.65
    Duration                         us         8.74
    L1/TEX Cache Throughput           %        25.26
    L2 Cache Throughput               %        24.62
    SM Active Cycles              cycle      4472.83
    Compute (SM) Throughput           %        16.90
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.30
    Achieved Active Warps Per SM           warp        35.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.7%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24005.33
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle      4472.83
    Total L1 Elapsed Cycles          cycle       387734
    Average L2 Active Cycles         cycle         4059
    Total L2 Elapsed Cycles          cycle       170352
    Average SM Active Cycles         cycle      4472.83
    Total SM Elapsed Cycles          cycle       387734
    Average SMSP Active Cycles       cycle      4376.66
    Total SMSP Elapsed Cycles        cycle      1550936
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.69
    Elapsed Cycles                cycle         7118
    Memory Throughput                 %        49.84
    DRAM Throughput                   %        49.84
    Duration                         us         9.06
    L1/TEX Cache Throughput           %        23.84
    L2 Cache Throughput               %        27.93
    SM Active Cycles              cycle      4991.91
    Compute (SM) Throughput           %        15.94
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.81
    Achieved Active Warps Per SM           warp        34.47
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.19%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27813.33
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      4991.91
    Total L1 Elapsed Cycles          cycle       411160
    Average L2 Active Cycles         cycle      4573.83
    Total L2 Elapsed Cycles          cycle       177240
    Average SM Active Cycles         cycle      4991.91
    Total SM Elapsed Cycles          cycle       411160
    Average SMSP Active Cycles       cycle      4967.81
    Total SMSP Elapsed Cycles        cycle      1644640
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.39
    Elapsed Cycles                cycle         7249
    Memory Throughput                 %        48.89
    DRAM Throughput                   %        48.89
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.66
    L2 Cache Throughput               %        27.44
    SM Active Cycles              cycle      5046.60
    Compute (SM) Throughput           %        15.81
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.17
    Achieved Active Warps Per SM           warp        34.64
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.83%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27786.67
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5046.60
    Total L1 Elapsed Cycles          cycle       414478
    Average L2 Active Cycles         cycle      4449.96
    Total L2 Elapsed Cycles          cycle       180408
    Average SM Active Cycles         cycle      5046.60
    Total SM Elapsed Cycles          cycle       414478
    Average SMSP Active Cycles       cycle      4883.59
    Total SMSP Elapsed Cycles        cycle      1657912
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.84
    Elapsed Cycles                cycle         7080
    Memory Throughput                 %        50.06
    DRAM Throughput                   %        50.06
    Duration                         us         9.02
    L1/TEX Cache Throughput           %        23.88
    L2 Cache Throughput               %        28.14
    SM Active Cycles              cycle      4872.57
    Compute (SM) Throughput           %        15.97
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.60
    Achieved Active Warps Per SM           warp        35.33
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.4%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27850.67
    Total DRAM Elapsed Cycles        cycle       333824
    Average L1 Active Cycles         cycle      4872.57
    Total L1 Elapsed Cycles          cycle       410462
    Average L2 Active Cycles         cycle      4553.21
    Total L2 Elapsed Cycles          cycle       176088
    Average SM Active Cycles         cycle      4872.57
    Total SM Elapsed Cycles          cycle       410462
    Average SMSP Active Cycles       cycle      5035.70
    Total SMSP Elapsed Cycles        cycle      1641848
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.87
    Elapsed Cycles                cycle      4643093
    Memory Throughput                 %        48.95
    DRAM Throughput                   %         0.21
    Duration                         ms         5.83
    L1/TEX Cache Throughput           %        63.24
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3592694.16
    Compute (SM) Throughput           %        45.43
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76501.33
    Total DRAM Elapsed Cycles        cycle    218506240
    Average L1 Active Cycles         cycle   3592694.16
    Total L1 Elapsed Cycles          cycle    269225120
    Average L2 Active Cycles         cycle   1208235.58
    Total L2 Elapsed Cycles          cycle    115468368
    Average SM Active Cycles         cycle   3592694.16
    Total SM Elapsed Cycles          cycle    269225120
    Average SMSP Active Cycles       cycle   3592116.12
    Total SMSP Elapsed Cycles        cycle   1076900480
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.52%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.64% above the average, while the minimum instance value is 7.95% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.5%                                                                                           
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.61% above the average, while the minimum instance value is 7.92% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.52%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.64% above the average, while the minimum instance value is 7.95% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       783.47
    Elapsed Cycles                cycle         6576
    Memory Throughput                 %        46.22
    DRAM Throughput                   %        46.22
    Duration                         us         8.38
    L1/TEX Cache Throughput           %        25.35
    L2 Cache Throughput               %        25.62
    SM Active Cycles              cycle      4457.83
    Compute (SM) Throughput           %        17.22
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.24
    Achieved Active Warps Per SM           warp        34.68
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.76%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23744
    Total DRAM Elapsed Cycles        cycle       308224
    Average L1 Active Cycles         cycle      4457.83
    Total L1 Elapsed Cycles          cycle       380672
    Average L2 Active Cycles         cycle      3976.62
    Total L2 Elapsed Cycles          cycle       163800
    Average SM Active Cycles         cycle      4457.83
    Total SM Elapsed Cycles          cycle       380672
    Average SMSP Active Cycles       cycle      4338.97
    Total SMSP Elapsed Cycles        cycle      1522688
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.68
    Elapsed Cycles                cycle         7190
    Memory Throughput                 %        49.17
    DRAM Throughput                   %        49.17
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        24.06
    L2 Cache Throughput               %        27.64
    SM Active Cycles              cycle      4912.83
    Compute (SM) Throughput           %        16.08
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.48
    Achieved Active Warps Per SM           warp        35.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.52%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27776
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      4912.83
    Total L1 Elapsed Cycles          cycle       407648
    Average L2 Active Cycles         cycle      4467.50
    Total L2 Elapsed Cycles          cycle       179016
    Average SM Active Cycles         cycle      4912.83
    Total SM Elapsed Cycles          cycle       407648
    Average SMSP Active Cycles       cycle      4843.16
    Total SMSP Elapsed Cycles        cycle      1630592
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.39
    Elapsed Cycles                cycle         7373
    Memory Throughput                 %        47.98
    DRAM Throughput                   %        47.98
    Duration                         us         9.38
    L1/TEX Cache Throughput           %        23.72
    L2 Cache Throughput               %        26.96
    SM Active Cycles              cycle      5034.66
    Compute (SM) Throughput           %        15.85
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.62
    Achieved Active Warps Per SM           warp        34.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.38%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27760
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      5034.66
    Total L1 Elapsed Cycles          cycle       413452
    Average L2 Active Cycles         cycle      4543.83
    Total L2 Elapsed Cycles          cycle       183624
    Average SM Active Cycles         cycle      5034.66
    Total SM Elapsed Cycles          cycle       413452
    Average SMSP Active Cycles       cycle      4957.43
    Total SMSP Elapsed Cycles        cycle      1653808
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.151%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 7.41% above the average, while the minimum instance value is 7.67% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       785.31
    Elapsed Cycles                cycle         7322
    Memory Throughput                 %        48.08
    DRAM Throughput                   %        48.08
    Duration                         us         9.31
    L1/TEX Cache Throughput           %        23.92
    L2 Cache Throughput               %        27.17
    SM Active Cycles              cycle      4991.88
    Compute (SM) Throughput           %        15.99
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.85
    Achieved Active Warps Per SM           warp        34.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.15%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27736
    Total DRAM Elapsed Cycles        cycle       346112
    Average L1 Active Cycles         cycle      4991.88
    Total L1 Elapsed Cycles          cycle       409952
    Average L2 Active Cycles         cycle      4408.83
    Total L2 Elapsed Cycles          cycle       182304
    Average SM Active Cycles         cycle      4991.88
    Total SM Elapsed Cycles          cycle       409952
    Average SMSP Active Cycles       cycle      4896.31
    Total SMSP Elapsed Cycles        cycle      1639808
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4642159
    Memory Throughput                 %        48.95
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.26
    L2 Cache Throughput               %         3.58
    SM Active Cycles              cycle   3591693.53
    Compute (SM) Throughput           %        45.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76986.67
    Total DRAM Elapsed Cycles        cycle    218784768
    Average L1 Active Cycles         cycle   3591693.53
    Total L1 Elapsed Cycles          cycle    269198054
    Average L2 Active Cycles         cycle   1217200.33
    Total L2 Elapsed Cycles          cycle    115615944
    Average SM Active Cycles         cycle   3591693.53
    Total SM Elapsed Cycles          cycle    269198054
    Average SMSP Active Cycles       cycle   3591818.14
    Total SMSP Elapsed Cycles        cycle   1076792216
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.45%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.55% above the average, while the minimum instance value is 7.86% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.48%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.59% above the average, while the minimum instance value is 7.91% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.45%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.55% above the average, while the minimum instance value is 7.86% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       781.34
    Elapsed Cycles                cycle         6784
    Memory Throughput                 %        45.19
    DRAM Throughput                   %        45.19
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        24.62
    L2 Cache Throughput               %        24.80
    SM Active Cycles              cycle      4589.33
    Compute (SM) Throughput           %        17.38
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.89
    Achieved Active Warps Per SM           warp        34.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.11%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24061.33
    Total DRAM Elapsed Cycles        cycle       319488
    Average L1 Active Cycles         cycle      4589.33
    Total L1 Elapsed Cycles          cycle       377032
    Average L2 Active Cycles         cycle      4176.21
    Total L2 Elapsed Cycles          cycle       168984
    Average SM Active Cycles         cycle      4589.33
    Total SM Elapsed Cycles          cycle       377032
    Average SMSP Active Cycles       cycle      4478.52
    Total SMSP Elapsed Cycles        cycle      1508128
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.03
    Elapsed Cycles                cycle         7125
    Memory Throughput                 %        49.55
    DRAM Throughput                   %        49.55
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.42
    L2 Cache Throughput               %        27.91
    SM Active Cycles              cycle      4992.72
    Compute (SM) Throughput           %        15.67
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.47
    Achieved Active Warps Per SM           warp        34.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.53%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27821.33
    Total DRAM Elapsed Cycles        cycle       336896
    Average L1 Active Cycles         cycle      4992.72
    Total L1 Elapsed Cycles          cycle       418338
    Average L2 Active Cycles         cycle      4530.21
    Total L2 Elapsed Cycles          cycle       177360
    Average SM Active Cycles         cycle      4992.72
    Total SM Elapsed Cycles          cycle       418338
    Average SMSP Active Cycles       cycle      4933.18
    Total SMSP Elapsed Cycles        cycle      1673352
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.61
    Elapsed Cycles                cycle         7249
    Memory Throughput                 %        48.91
    DRAM Throughput                   %        48.91
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.78
    L2 Cache Throughput               %        27.42
    SM Active Cycles              cycle      5002.93
    Compute (SM) Throughput           %        15.88
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.37
    Achieved Active Warps Per SM           warp        35.22
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.63%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27797.33
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5002.93
    Total L1 Elapsed Cycles          cycle       412570
    Average L2 Active Cycles         cycle      4491.58
    Total L2 Elapsed Cycles          cycle       180480
    Average SM Active Cycles         cycle      5002.93
    Total SM Elapsed Cycles          cycle       412570
    Average SMSP Active Cycles       cycle      4910.74
    Total SMSP Elapsed Cycles        cycle      1650280
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.00
    Elapsed Cycles                cycle         7248
    Memory Throughput                 %        48.75
    DRAM Throughput                   %        48.75
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.89
    L2 Cache Throughput               %        27.46
    SM Active Cycles              cycle      4989.02
    Compute (SM) Throughput           %        15.97
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.40
    Achieved Active Warps Per SM           warp        35.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.6%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27786.67
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle      4989.02
    Total L1 Elapsed Cycles          cycle       410250
    Average L2 Active Cycles         cycle      4568.83
    Total L2 Elapsed Cycles          cycle       180480
    Average SM Active Cycles         cycle      4989.02
    Total SM Elapsed Cycles          cycle       410250
    Average SMSP Active Cycles       cycle      5005.17
    Total SMSP Elapsed Cycles        cycle      1641000
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4642953
    Memory Throughput                 %        48.94
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.26
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3591462.16
    Compute (SM) Throughput           %        45.43
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        78152
    Total DRAM Elapsed Cycles        cycle    218823680
    Average L1 Active Cycles         cycle   3591462.16
    Total L1 Elapsed Cycles          cycle    269254508
    Average L2 Active Cycles         cycle   1204954.29
    Total L2 Elapsed Cycles          cycle    115635936
    Average SM Active Cycles         cycle   3591462.16
    Total SM Elapsed Cycles          cycle    269254508
    Average SMSP Active Cycles       cycle   3591984.06
    Total SMSP Elapsed Cycles        cycle   1077018032
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.52%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.65% above the average, while the minimum instance value is 7.99% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.44%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.54% above the average, while the minimum instance value is 7.92% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.52%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.65% above the average, while the minimum instance value is 7.99% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.17
    Elapsed Cycles                cycle         6525
    Memory Throughput                 %        46.40
    DRAM Throughput                   %        46.40
    Duration                         us         8.32
    L1/TEX Cache Throughput           %        26.11
    L2 Cache Throughput               %        25.82
    SM Active Cycles              cycle      4327.98
    Compute (SM) Throughput           %        17.11
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.63
    Achieved Active Warps Per SM           warp        35.34
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.37%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23757.33
    Total DRAM Elapsed Cycles        cycle       307200
    Average L1 Active Cycles         cycle      4327.98
    Total L1 Elapsed Cycles          cycle       382940
    Average L2 Active Cycles         cycle      4025.58
    Total L2 Elapsed Cycles          cycle       162480
    Average SM Active Cycles         cycle      4327.98
    Total SM Elapsed Cycles          cycle       382940
    Average SMSP Active Cycles       cycle      4408.20
    Total SMSP Elapsed Cycles        cycle      1531760
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.94
    Elapsed Cycles                cycle         7287
    Memory Throughput                 %        48.51
    DRAM Throughput                   %        48.51
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.65
    L2 Cache Throughput               %        27.30
    SM Active Cycles              cycle      4937.90
    Compute (SM) Throughput           %        15.81
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.42
    Achieved Active Warps Per SM           warp        35.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.58%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27816
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      4937.90
    Total L1 Elapsed Cycles          cycle       414444
    Average L2 Active Cycles         cycle      4517.12
    Total L2 Elapsed Cycles          cycle       181464
    Average SM Active Cycles         cycle      4937.90
    Total SM Elapsed Cycles          cycle       414444
    Average SMSP Active Cycles       cycle      4943.58
    Total SMSP Elapsed Cycles        cycle      1657776
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.45
    Elapsed Cycles                cycle         7370
    Memory Throughput                 %        47.91
    DRAM Throughput                   %        47.91
    Duration                         us         9.38
    L1/TEX Cache Throughput           %        23.86
    L2 Cache Throughput               %        26.98
    SM Active Cycles              cycle      4920.07
    Compute (SM) Throughput           %        15.94
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.82
    Achieved Active Warps Per SM           warp        35.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.18%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27720
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      4920.07
    Total L1 Elapsed Cycles          cycle       411032
    Average L2 Active Cycles         cycle      4560.92
    Total L2 Elapsed Cycles          cycle       183408
    Average SM Active Cycles         cycle      4920.07
    Total SM Elapsed Cycles          cycle       411032
    Average SMSP Active Cycles       cycle      4976.11
    Total SMSP Elapsed Cycles        cycle      1644128
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.77
    Elapsed Cycles                cycle         7239
    Memory Throughput                 %        48.86
    DRAM Throughput                   %        48.86
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.62
    L2 Cache Throughput               %        27.46
    SM Active Cycles              cycle      5012.19
    Compute (SM) Throughput           %        15.80
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.90
    Achieved Active Warps Per SM           warp        34.51
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.1%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27768
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5012.19
    Total L1 Elapsed Cycles          cycle       414810
    Average L2 Active Cycles         cycle      4479.88
    Total L2 Elapsed Cycles          cycle       180312
    Average SM Active Cycles         cycle      5012.19
    Total SM Elapsed Cycles          cycle       414810
    Average SMSP Active Cycles       cycle      4896.38
    Total SMSP Elapsed Cycles        cycle      1659240
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4644031
    Memory Throughput                 %        48.97
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.58
    SM Active Cycles              cycle   3592146.03
    Compute (SM) Throughput           %        45.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77370.67
    Total DRAM Elapsed Cycles        cycle    218873856
    Average L1 Active Cycles         cycle   3592146.03
    Total L1 Elapsed Cycles          cycle    269111568
    Average L2 Active Cycles         cycle   1219439.46
    Total L2 Elapsed Cycles          cycle    115662600
    Average SM Active Cycles         cycle   3592146.03
    Total SM Elapsed Cycles          cycle    269111568
    Average SMSP Active Cycles       cycle   3592177.61
    Total SMSP Elapsed Cycles        cycle   1076446272
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.47%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.56% above the average, while the minimum instance value is 7.85% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.48%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.58% above the average, while the minimum instance value is 7.94% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.47%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.56% above the average, while the minimum instance value is 7.85% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.87
    Elapsed Cycles                cycle         6773
    Memory Throughput                 %        45.08
    DRAM Throughput                   %        45.08
    Duration                         us         8.64
    L1/TEX Cache Throughput           %        25.15
    L2 Cache Throughput               %        24.86
    SM Active Cycles              cycle      4492.78
    Compute (SM) Throughput           %        17.04
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.09
    Achieved Active Warps Per SM           warp        34.60
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24005.33
    Total DRAM Elapsed Cycles        cycle       319488
    Average L1 Active Cycles         cycle      4492.78
    Total L1 Elapsed Cycles          cycle       384536
    Average L2 Active Cycles         cycle      4085.67
    Total L2 Elapsed Cycles          cycle       168624
    Average SM Active Cycles         cycle      4492.78
    Total SM Elapsed Cycles          cycle       384536
    Average SMSP Active Cycles       cycle      4469.16
    Total SMSP Elapsed Cycles        cycle      1538144
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.83
    Elapsed Cycles                cycle         7278
    Memory Throughput                 %        48.91
    DRAM Throughput                   %        48.91
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.67
    L2 Cache Throughput               %        27.35
    SM Active Cycles              cycle      4992.28
    Compute (SM) Throughput           %        15.83
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.80
    Achieved Active Warps Per SM           warp        35.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.2%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27880
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle      4992.28
    Total L1 Elapsed Cycles          cycle       414090
    Average L2 Active Cycles         cycle      4535.58
    Total L2 Elapsed Cycles          cycle       181176
    Average SM Active Cycles         cycle      4992.28
    Total SM Elapsed Cycles          cycle       414090
    Average SMSP Active Cycles       cycle      4957.33
    Total SMSP Elapsed Cycles        cycle      1656360
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       786.54
    Elapsed Cycles                cycle         7211
    Memory Throughput                 %        49.45
    DRAM Throughput                   %        49.45
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        24.32
    L2 Cache Throughput               %        27.76
    SM Active Cycles              cycle      5041.67
    Compute (SM) Throughput           %        16.25
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.76
    Achieved Active Warps Per SM           warp        35.40
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.24%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27850.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5041.67
    Total L1 Elapsed Cycles          cycle       403182
    Average L2 Active Cycles         cycle      4548.71
    Total L2 Elapsed Cycles          cycle       178320
    Average SM Active Cycles         cycle      5041.67
    Total SM Elapsed Cycles          cycle       403182
    Average SMSP Active Cycles       cycle      4970.93
    Total SMSP Elapsed Cycles        cycle      1612728
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.44
    Elapsed Cycles                cycle         7238
    Memory Throughput                 %        48.87
    DRAM Throughput                   %        48.87
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.88
    L2 Cache Throughput               %        27.50
    SM Active Cycles              cycle      4995.93
    Compute (SM) Throughput           %        15.96
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.38
    Achieved Active Warps Per SM           warp        35.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.62%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27773.33
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4995.93
    Total L1 Elapsed Cycles          cycle       410604
    Average L2 Active Cycles         cycle      4494.96
    Total L2 Elapsed Cycles          cycle       180192
    Average SM Active Cycles         cycle      4995.93
    Total SM Elapsed Cycles          cycle       410604
    Average SMSP Active Cycles       cycle      4894.57
    Total SMSP Elapsed Cycles        cycle      1642416
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4641135
    Memory Throughput                 %        48.97
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.26
    L2 Cache Throughput               %         3.58
    SM Active Cycles              cycle   3591787.64
    Compute (SM) Throughput           %        45.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        76664
    Total DRAM Elapsed Cycles        cycle    218736640
    Average L1 Active Cycles         cycle   3591787.64
    Total L1 Elapsed Cycles          cycle    269116562
    Average L2 Active Cycles         cycle   1209032.42
    Total L2 Elapsed Cycles          cycle    115590456
    Average SM Active Cycles         cycle   3591787.64
    Total SM Elapsed Cycles          cycle    269116562
    Average SMSP Active Cycles       cycle   3591892.03
    Total SMSP Elapsed Cycles        cycle   1076466248
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.54%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.66% above the average, while the minimum instance value is 7.94% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.51%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.62% above the average, while the minimum instance value is 8.03% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.54%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.66% above the average, while the minimum instance value is 7.94% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.38
    Elapsed Cycles                cycle         6471
    Memory Throughput                 %        46.63
    DRAM Throughput                   %        46.63
    Duration                         us         8.26
    L1/TEX Cache Throughput           %        25.48
    L2 Cache Throughput               %        25.99
    SM Active Cycles              cycle      4433.86
    Compute (SM) Throughput           %        17.15
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.63
    Achieved Active Warps Per SM           warp        34.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.37%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23797.33
    Total DRAM Elapsed Cycles        cycle       306176
    Average L1 Active Cycles         cycle      4433.86
    Total L1 Elapsed Cycles          cycle       382168
    Average L2 Active Cycles         cycle      4149.17
    Total L2 Elapsed Cycles          cycle       161112
    Average SM Active Cycles         cycle      4433.86
    Total SM Elapsed Cycles          cycle       382168
    Average SMSP Active Cycles       cycle      4488.75
    Total SMSP Elapsed Cycles        cycle      1528672
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.22
    Elapsed Cycles                cycle         7319
    Memory Throughput                 %        48.55
    DRAM Throughput                   %        48.55
    Duration                         us         9.31
    L1/TEX Cache Throughput           %        23.82
    L2 Cache Throughput               %        27.20
    SM Active Cycles              cycle      5050.43
    Compute (SM) Throughput           %        15.91
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.66
    Achieved Active Warps Per SM           warp        34.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.34%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27842.67
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      5050.43
    Total L1 Elapsed Cycles          cycle       411792
    Average L2 Active Cycles         cycle      4591.83
    Total L2 Elapsed Cycles          cycle       181992
    Average SM Active Cycles         cycle      5050.43
    Total SM Elapsed Cycles          cycle       411792
    Average SMSP Active Cycles       cycle      4997.66
    Total SMSP Elapsed Cycles        cycle      1647168
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       785.00
    Elapsed Cycles                cycle         7215
    Memory Throughput                 %        49.22
    DRAM Throughput                   %        49.22
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.68
    L2 Cache Throughput               %        27.56
    SM Active Cycles              cycle      4916.03
    Compute (SM) Throughput           %        15.83
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.78
    Achieved Active Warps Per SM           warp        35.42
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.22%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27805.33
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      4916.03
    Total L1 Elapsed Cycles          cycle       413982
    Average L2 Active Cycles         cycle      4488.50
    Total L2 Elapsed Cycles          cycle       179616
    Average SM Active Cycles         cycle      4916.03
    Total SM Elapsed Cycles          cycle       413982
    Average SMSP Active Cycles       cycle      4904.58
    Total SMSP Elapsed Cycles        cycle      1655928
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.71
    Elapsed Cycles                cycle         7198
    Memory Throughput                 %        49.10
    DRAM Throughput                   %        49.10
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        24.00
    L2 Cache Throughput               %        27.70
    SM Active Cycles              cycle      4950.79
    Compute (SM) Throughput           %        16.04
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.67
    Achieved Active Warps Per SM           warp        35.36
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27738.67
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      4950.79
    Total L1 Elapsed Cycles          cycle       408464
    Average L2 Active Cycles         cycle      4487.21
    Total L2 Elapsed Cycles          cycle       178656
    Average SM Active Cycles         cycle      4950.79
    Total SM Elapsed Cycles          cycle       408464
    Average SMSP Active Cycles       cycle      4905.10
    Total SMSP Elapsed Cycles        cycle      1633856
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4642912
    Memory Throughput                 %        48.93
    DRAM Throughput                   %         0.22
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.24
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3592440.24
    Compute (SM) Throughput           %        45.42
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     79210.67
    Total DRAM Elapsed Cycles        cycle    218819584
    Average L1 Active Cycles         cycle   3592440.24
    Total L1 Elapsed Cycles          cycle    269286910
    Average L2 Active Cycles         cycle   1217745.08
    Total L2 Elapsed Cycles          cycle    115634808
    Average SM Active Cycles         cycle   3592440.24
    Total SM Elapsed Cycles          cycle    269286910
    Average SMSP Active Cycles       cycle   3592251.62
    Total SMSP Elapsed Cycles        cycle   1077147640
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.45%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.55% above the average, while the minimum instance value is 7.94% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.47%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.58% above the average, while the minimum instance value is 7.88% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.45%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.55% above the average, while the minimum instance value is 7.94% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       782.68
    Elapsed Cycles                cycle         6790
    Memory Throughput                 %        45.19
    DRAM Throughput                   %        45.19
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        25.43
    L2 Cache Throughput               %        24.77
    SM Active Cycles              cycle      4442.62
    Compute (SM) Throughput           %        17.11
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.11
    Achieved Active Warps Per SM           warp        35.09
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.89%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24061.33
    Total DRAM Elapsed Cycles        cycle       319488
    Average L1 Active Cycles         cycle      4442.62
    Total L1 Elapsed Cycles          cycle       382950
    Average L2 Active Cycles         cycle      3973.96
    Total L2 Elapsed Cycles          cycle       169152
    Average SM Active Cycles         cycle      4442.62
    Total SM Elapsed Cycles          cycle       382950
    Average SMSP Active Cycles       cycle      4387.34
    Total SMSP Elapsed Cycles        cycle      1531800
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       785.71
    Elapsed Cycles                cycle         7222
    Memory Throughput                 %        48.99
    DRAM Throughput                   %        48.99
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.71
    L2 Cache Throughput               %        27.56
    SM Active Cycles              cycle      5017.47
    Compute (SM) Throughput           %        15.84
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.41
    Achieved Active Warps Per SM           warp        34.76
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.59%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27842.67
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5017.47
    Total L1 Elapsed Cycles          cycle       413718
    Average L2 Active Cycles         cycle      4424.88
    Total L2 Elapsed Cycles          cycle       179784
    Average SM Active Cycles         cycle      5017.47
    Total SM Elapsed Cycles          cycle       413718
    Average SMSP Active Cycles       cycle      4833.35
    Total SMSP Elapsed Cycles        cycle      1654872
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.127%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 7.57% above the average, while the minimum instance value is 12.81% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.53
    Elapsed Cycles                cycle         7154
    Memory Throughput                 %        49.41
    DRAM Throughput                   %        49.41
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.78
    L2 Cache Throughput               %        27.81
    SM Active Cycles              cycle      5027.22
    Compute (SM) Throughput           %        15.90
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.43
    Achieved Active Warps Per SM           warp        34.77
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.57%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27741.33
    Total DRAM Elapsed Cycles        cycle       336896
    Average L1 Active Cycles         cycle      5027.22
    Total L1 Elapsed Cycles          cycle       412132
    Average L2 Active Cycles         cycle      4525.50
    Total L2 Elapsed Cycles          cycle       178008
    Average SM Active Cycles         cycle      5027.22
    Total SM Elapsed Cycles          cycle       412132
    Average SMSP Active Cycles       cycle      4970.24
    Total SMSP Elapsed Cycles        cycle      1648528
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       786.18
    Elapsed Cycles                cycle         7250
    Memory Throughput                 %        48.73
    DRAM Throughput                   %        48.73
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.41
    L2 Cache Throughput               %        27.41
    SM Active Cycles              cycle      4964.67
    Compute (SM) Throughput           %        15.64
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.45
    Achieved Active Warps Per SM           warp        34.78
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.55%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27776
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle      4964.67
    Total L1 Elapsed Cycles          cycle       419034
    Average L2 Active Cycles         cycle      4474.54
    Total L2 Elapsed Cycles          cycle       180672
    Average SM Active Cycles         cycle      4964.67
    Total SM Elapsed Cycles          cycle       419034
    Average SMSP Active Cycles       cycle      4925.04
    Total SMSP Elapsed Cycles        cycle      1676136
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4643641
    Memory Throughput                 %        48.89
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.58
    SM Active Cycles              cycle   3591979.52
    Compute (SM) Throughput           %        45.38
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        77408
    Total DRAM Elapsed Cycles        cycle    218855424
    Average L1 Active Cycles         cycle   3591979.52
    Total L1 Elapsed Cycles          cycle    269540332
    Average L2 Active Cycles         cycle   1206550.71
    Total L2 Elapsed Cycles          cycle    115652808
    Average SM Active Cycles         cycle   3591979.52
    Total SM Elapsed Cycles          cycle    269540332
    Average SMSP Active Cycles       cycle   3591962.39
    Total SMSP Elapsed Cycles        cycle   1078161328
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.48%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.61% above the average, while the minimum instance value is 7.97% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.46%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.59% above the average, while the minimum instance value is 7.87% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.48%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.61% above the average, while the minimum instance value is 7.97% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       789.76
    Elapsed Cycles                cycle         6661
    Memory Throughput                 %        45.86
    DRAM Throughput                   %        45.86
    Duration                         us         8.42
    L1/TEX Cache Throughput           %        25.54
    L2 Cache Throughput               %        25.50
    SM Active Cycles              cycle      4423.76
    Compute (SM) Throughput           %        17.05
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.18
    Achieved Active Warps Per SM           warp        35.13
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.82%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23794.67
    Total DRAM Elapsed Cycles        cycle       311296
    Average L1 Active Cycles         cycle      4423.76
    Total L1 Elapsed Cycles          cycle       384384
    Average L2 Active Cycles         cycle      3957.29
    Total L2 Elapsed Cycles          cycle       164304
    Average SM Active Cycles         cycle      4423.76
    Total SM Elapsed Cycles          cycle       384384
    Average SMSP Active Cycles       cycle      4327.78
    Total SMSP Elapsed Cycles        cycle      1537536
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       788.19
    Elapsed Cycles                cycle         7324
    Memory Throughput                 %        48.73
    DRAM Throughput                   %        48.73
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.51
    L2 Cache Throughput               %        27.29
    SM Active Cycles              cycle      5026.41
    Compute (SM) Throughput           %        15.72
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.44
    Achieved Active Warps Per SM           warp        34.77
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.56%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27776
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle      5026.41
    Total L1 Elapsed Cycles          cycle       416812
    Average L2 Active Cycles         cycle      4595.25
    Total L2 Elapsed Cycles          cycle       181464
    Average SM Active Cycles         cycle      5026.41
    Total SM Elapsed Cycles          cycle       416812
    Average SMSP Active Cycles       cycle      4998.59
    Total SMSP Elapsed Cycles        cycle      1667248
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.56
    Elapsed Cycles                cycle         7358
    Memory Throughput                 %        48.17
    DRAM Throughput                   %        48.17
    Duration                         us         9.38
    L1/TEX Cache Throughput           %        24.15
    L2 Cache Throughput               %        27.05
    SM Active Cycles              cycle      4970.74
    Compute (SM) Throughput           %        16.14
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 24.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.74
    Achieved Active Warps Per SM           warp        36.36
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.26%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27869.33
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      4970.74
    Total L1 Elapsed Cycles          cycle       406130
    Average L2 Active Cycles         cycle      4428.96
    Total L2 Elapsed Cycles          cycle       183168
    Average SM Active Cycles         cycle      4970.74
    Total SM Elapsed Cycles          cycle       406130
    Average SMSP Active Cycles       cycle      4855.24
    Total SMSP Elapsed Cycles        cycle      1624520
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.18
    Elapsed Cycles                cycle         7243
    Memory Throughput                 %        48.84
    DRAM Throughput                   %        48.84
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.81
    L2 Cache Throughput               %        27.42
    SM Active Cycles              cycle      4974.09
    Compute (SM) Throughput           %        15.90
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.12
    Achieved Active Warps Per SM           warp        35.10
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.88%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27754.67
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4974.09
    Total L1 Elapsed Cycles          cycle       412054
    Average L2 Active Cycles         cycle      4546.54
    Total L2 Elapsed Cycles          cycle       180504
    Average SM Active Cycles         cycle      4974.09
    Total SM Elapsed Cycles          cycle       412054
    Average SMSP Active Cycles       cycle      4934.33
    Total SMSP Elapsed Cycles        cycle      1648216
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4643110
    Memory Throughput                 %        48.89
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.24
    L2 Cache Throughput               %         3.58
    SM Active Cycles              cycle   3592682.43
    Compute (SM) Throughput           %        45.38
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.59
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76714.67
    Total DRAM Elapsed Cycles        cycle    218830848
    Average L1 Active Cycles         cycle   3592682.43
    Total L1 Elapsed Cycles          cycle    269524632
    Average L2 Active Cycles         cycle   1206054.75
    Total L2 Elapsed Cycles          cycle    115639704
    Average SM Active Cycles         cycle   3592682.43
    Total SM Elapsed Cycles          cycle    269524632
    Average SMSP Active Cycles       cycle   3592295.25
    Total SMSP Elapsed Cycles        cycle   1078098528
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.44%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.55% above the average, while the minimum instance value is 7.92% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.47%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.60% above the average, while the minimum instance value is 7.85% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.44%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.55% above the average, while the minimum instance value is 7.92% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.01
    Elapsed Cycles                cycle         6657
    Memory Throughput                 %        45.99
    DRAM Throughput                   %        45.99
    Duration                         us         8.48
    L1/TEX Cache Throughput           %        25.26
    L2 Cache Throughput               %        25.33
    SM Active Cycles              cycle      4472.79
    Compute (SM) Throughput           %        17.15
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.76
    Achieved Active Warps Per SM           warp        34.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.24%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24016
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4472.79
    Total L1 Elapsed Cycles          cycle       382132
    Average L2 Active Cycles         cycle      4031.92
    Total L2 Elapsed Cycles          cycle       165504
    Average SM Active Cycles         cycle      4472.79
    Total SM Elapsed Cycles          cycle       382132
    Average SMSP Active Cycles       cycle      4380.52
    Total SMSP Elapsed Cycles        cycle      1528528
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.90
    Elapsed Cycles                cycle         7276
    Memory Throughput                 %        48.59
    DRAM Throughput                   %        48.59
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.60
    L2 Cache Throughput               %        27.33
    SM Active Cycles              cycle      4913.16
    Compute (SM) Throughput           %        15.78
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.90
    Achieved Active Warps Per SM           warp        35.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.1%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27861.33
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      4913.16
    Total L1 Elapsed Cycles          cycle       415348
    Average L2 Active Cycles         cycle      4511.46
    Total L2 Elapsed Cycles          cycle       181200
    Average SM Active Cycles         cycle      4913.16
    Total SM Elapsed Cycles          cycle       415348
    Average SMSP Active Cycles       cycle      4951.78
    Total SMSP Elapsed Cycles        cycle      1661392
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       785.83
    Elapsed Cycles                cycle         7145
    Memory Throughput                 %        49.41
    DRAM Throughput                   %        49.41
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.68
    L2 Cache Throughput               %        27.84
    SM Active Cycles              cycle      5049.47
    Compute (SM) Throughput           %        15.82
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.79
    Achieved Active Warps Per SM           warp        34.46
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.21%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27741.33
    Total DRAM Elapsed Cycles        cycle       336896
    Average L1 Active Cycles         cycle      5049.47
    Total L1 Elapsed Cycles          cycle       414182
    Average L2 Active Cycles         cycle      4514.08
    Total L2 Elapsed Cycles          cycle       177840
    Average SM Active Cycles         cycle      5049.47
    Total SM Elapsed Cycles          cycle       414182
    Average SMSP Active Cycles       cycle      4953.50
    Total SMSP Elapsed Cycles        cycle      1656728
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.83
    Elapsed Cycles                cycle         7105
    Memory Throughput                 %        49.74
    DRAM Throughput                   %        49.74
    Duration                         us         9.06
    L1/TEX Cache Throughput           %        23.39
    L2 Cache Throughput               %        27.96
    SM Active Cycles              cycle      5002.88
    Compute (SM) Throughput           %        15.63
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.80
    Achieved Active Warps Per SM           warp        34.46
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.2%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27760
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      5002.88
    Total L1 Elapsed Cycles          cycle       419206
    Average L2 Active Cycles         cycle      4581.79
    Total L2 Elapsed Cycles          cycle       177048
    Average SM Active Cycles         cycle      5002.88
    Total SM Elapsed Cycles          cycle       419206
    Average SMSP Active Cycles       cycle      5022.85
    Total SMSP Elapsed Cycles        cycle      1676824
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4643952
    Memory Throughput                 %        48.95
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3592120.26
    Compute (SM) Throughput           %        45.43
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76954.67
    Total DRAM Elapsed Cycles        cycle    218868736
    Average L1 Active Cycles         cycle   3592120.26
    Total L1 Elapsed Cycles          cycle    269208158
    Average L2 Active Cycles         cycle   1230560.67
    Total L2 Elapsed Cycles          cycle    115660512
    Average SM Active Cycles         cycle   3592120.26
    Total SM Elapsed Cycles          cycle    269208158
    Average SMSP Active Cycles       cycle   3591361.36
    Total SMSP Elapsed Cycles        cycle   1076832632
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.45%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.55% above the average, while the minimum instance value is 7.83% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.54%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.67% above the average, while the minimum instance value is 7.95% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.45%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.55% above the average, while the minimum instance value is 7.83% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.51
    Elapsed Cycles                cycle         6510
    Memory Throughput                 %        46.51
    DRAM Throughput                   %        46.51
    Duration                         us         8.29
    L1/TEX Cache Throughput           %        25.95
    L2 Cache Throughput               %        25.86
    SM Active Cycles              cycle      4353.74
    Compute (SM) Throughput           %        17.16
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.80
    Achieved Active Warps Per SM           warp        34.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.2%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23813.33
    Total DRAM Elapsed Cycles        cycle       307200
    Average L1 Active Cycles         cycle      4353.74
    Total L1 Elapsed Cycles          cycle       381818
    Average L2 Active Cycles         cycle      3993.88
    Total L2 Elapsed Cycles          cycle       162216
    Average SM Active Cycles         cycle      4353.74
    Total SM Elapsed Cycles          cycle       381818
    Average SMSP Active Cycles       cycle      4354.54
    Total SMSP Elapsed Cycles        cycle      1527272
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       785.58
    Elapsed Cycles                cycle         7222
    Memory Throughput                 %        48.90
    DRAM Throughput                   %        48.90
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.89
    L2 Cache Throughput               %        27.52
    SM Active Cycles              cycle      4948.02
    Compute (SM) Throughput           %        15.97
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.86
    Achieved Active Warps Per SM           warp        35.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.14%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27792
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4948.02
    Total L1 Elapsed Cycles          cycle       410458
    Average L2 Active Cycles         cycle      4535.04
    Total L2 Elapsed Cycles          cycle       179904
    Average SM Active Cycles         cycle      4948.02
    Total SM Elapsed Cycles          cycle       410458
    Average SMSP Active Cycles       cycle      4934.41
    Total SMSP Elapsed Cycles        cycle      1641832
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.75
    Elapsed Cycles                cycle         7186
    Memory Throughput                 %        49.40
    DRAM Throughput                   %        49.40
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.93
    L2 Cache Throughput               %        27.68
    SM Active Cycles              cycle      4990.45
    Compute (SM) Throughput           %        15.99
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.65
    Achieved Active Warps Per SM           warp        34.87
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27824
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4990.45
    Total L1 Elapsed Cycles          cycle       409856
    Average L2 Active Cycles         cycle         4475
    Total L2 Elapsed Cycles          cycle       178920
    Average SM Active Cycles         cycle      4990.45
    Total SM Elapsed Cycles          cycle       409856
    Average SMSP Active Cycles       cycle      4899.24
    Total SMSP Elapsed Cycles        cycle      1639424
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.87
    Elapsed Cycles                cycle         7335
    Memory Throughput                 %        48.39
    DRAM Throughput                   %        48.39
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        23.66
    L2 Cache Throughput               %        27.12
    SM Active Cycles              cycle      4973.98
    Compute (SM) Throughput           %        15.81
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.10
    Achieved Active Warps Per SM           warp        35.57
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.9%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27832
    Total DRAM Elapsed Cycles        cycle       345088
    Average L1 Active Cycles         cycle      4973.98
    Total L1 Elapsed Cycles          cycle       414396
    Average L2 Active Cycles         cycle      4505.04
    Total L2 Elapsed Cycles          cycle       182640
    Average SM Active Cycles         cycle      4973.98
    Total SM Elapsed Cycles          cycle       414396
    Average SMSP Active Cycles       cycle      4919.41
    Total SMSP Elapsed Cycles        cycle      1657584
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4640259
    Memory Throughput                 %        48.95
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3591954.22
    Compute (SM) Throughput           %        45.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        76576
    Total DRAM Elapsed Cycles        cycle    218695680
    Average L1 Active Cycles         cycle   3591954.22
    Total L1 Elapsed Cycles          cycle    269185526
    Average L2 Active Cycles         cycle   1208124.17
    Total L2 Elapsed Cycles          cycle    115568688
    Average SM Active Cycles         cycle   3591954.22
    Total SM Elapsed Cycles          cycle    269185526
    Average SMSP Active Cycles       cycle   3591618.93
    Total SMSP Elapsed Cycles        cycle   1076742104
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.48%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.59% above the average, while the minimum instance value is 7.91% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.46%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.57% above the average, while the minimum instance value is 7.93% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.48%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.59% above the average, while the minimum instance value is 7.91% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.50
    Elapsed Cycles                cycle         6615
    Memory Throughput                 %        46.25
    DRAM Throughput                   %        46.25
    Duration                         us         8.42
    L1/TEX Cache Throughput           %        25.01
    L2 Cache Throughput               %        25.46
    SM Active Cycles              cycle      4518.55
    Compute (SM) Throughput           %        17.23
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.27
    Achieved Active Warps Per SM           warp        33.73
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.73%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23994.67
    Total DRAM Elapsed Cycles        cycle       311296
    Average L1 Active Cycles         cycle      4518.55
    Total L1 Elapsed Cycles          cycle       380412
    Average L2 Active Cycles         cycle      4099.25
    Total L2 Elapsed Cycles          cycle       164736
    Average SM Active Cycles         cycle      4518.55
    Total SM Elapsed Cycles          cycle       380412
    Average SMSP Active Cycles       cycle      4449.75
    Total SMSP Elapsed Cycles        cycle      1521648
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.75
    Elapsed Cycles                cycle         7332
    Memory Throughput                 %        48.11
    DRAM Throughput                   %        48.11
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        23.26
    L2 Cache Throughput               %        27.13
    SM Active Cycles              cycle      5033.69
    Compute (SM) Throughput           %        15.55
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.69
    Achieved Active Warps Per SM           warp        34.89
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.31%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27754.67
    Total DRAM Elapsed Cycles        cycle       346112
    Average L1 Active Cycles         cycle      5033.69
    Total L1 Elapsed Cycles          cycle       421532
    Average L2 Active Cycles         cycle      4398.38
    Total L2 Elapsed Cycles          cycle       182472
    Average SM Active Cycles         cycle      5033.69
    Total SM Elapsed Cycles          cycle       421532
    Average SMSP Active Cycles       cycle      4845.01
    Total SMSP Elapsed Cycles        cycle      1686128
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.12
    Elapsed Cycles                cycle         7209
    Memory Throughput                 %        49.08
    DRAM Throughput                   %        49.08
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        24.07
    L2 Cache Throughput               %        27.57
    SM Active Cycles              cycle      5029.24
    Compute (SM) Throughput           %        16.10
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.96
    Achieved Active Warps Per SM           warp        35.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27728
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      5029.24
    Total L1 Elapsed Cycles          cycle       407130
    Average L2 Active Cycles         cycle      4546.62
    Total L2 Elapsed Cycles          cycle       179544
    Average SM Active Cycles         cycle      5029.24
    Total SM Elapsed Cycles          cycle       407130
    Average SMSP Active Cycles       cycle      4966.10
    Total SMSP Elapsed Cycles        cycle      1628520
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.65
    Elapsed Cycles                cycle         7371
    Memory Throughput                 %        47.94
    DRAM Throughput                   %        47.94
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        23.88
    L2 Cache Throughput               %        26.98
    SM Active Cycles              cycle      4953.69
    Compute (SM) Throughput           %        15.96
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.57
    Achieved Active Warps Per SM           warp        35.31
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.43%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27736
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      4953.69
    Total L1 Elapsed Cycles          cycle       410686
    Average L2 Active Cycles         cycle      4539.62
    Total L2 Elapsed Cycles          cycle       183504
    Average SM Active Cycles         cycle      4953.69
    Total SM Elapsed Cycles          cycle       410686
    Average SMSP Active Cycles       cycle      4964.92
    Total SMSP Elapsed Cycles        cycle      1642744
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4640584
    Memory Throughput                 %        48.92
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3591892.41
    Compute (SM) Throughput           %        45.41
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76965.33
    Total DRAM Elapsed Cycles        cycle    218711040
    Average L1 Active Cycles         cycle   3591892.41
    Total L1 Elapsed Cycles          cycle    269347002
    Average L2 Active Cycles         cycle   1225026.62
    Total L2 Elapsed Cycles          cycle    115576440
    Average SM Active Cycles         cycle   3591892.41
    Total SM Elapsed Cycles          cycle    269347002
    Average SMSP Active Cycles       cycle   3592064.26
    Total SMSP Elapsed Cycles        cycle   1077388008
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.44%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.55% above the average, while the minimum instance value is 7.87% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.46%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.57% above the average, while the minimum instance value is 7.98% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.44%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.55% above the average, while the minimum instance value is 7.87% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       782.71
    Elapsed Cycles                cycle         6571
    Memory Throughput                 %        46.34
    DRAM Throughput                   %        46.34
    Duration                         us         8.38
    L1/TEX Cache Throughput           %        25.50
    L2 Cache Throughput               %        25.63
    SM Active Cycles              cycle      4431.21
    Compute (SM) Throughput           %        17.54
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.93
    Achieved Active Warps Per SM           warp        34.53
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.07%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23805.33
    Total DRAM Elapsed Cycles        cycle       308224
    Average L1 Active Cycles         cycle      4431.21
    Total L1 Elapsed Cycles          cycle       373644
    Average L2 Active Cycles         cycle      3908.83
    Total L2 Elapsed Cycles          cycle       163536
    Average SM Active Cycles         cycle      4431.21
    Total SM Elapsed Cycles          cycle       373644
    Average SMSP Active Cycles       cycle      4298.68
    Total SMSP Elapsed Cycles        cycle      1494576
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       786.44
    Elapsed Cycles                cycle         7154
    Memory Throughput                 %        49.53
    DRAM Throughput                   %        49.53
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.71
    L2 Cache Throughput               %        27.83
    SM Active Cycles              cycle      4963.84
    Compute (SM) Throughput           %        15.85
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.88
    Achieved Active Warps Per SM           warp        34.50
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.12%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27808
    Total DRAM Elapsed Cycles        cycle       336896
    Average L1 Active Cycles         cycle      4963.84
    Total L1 Elapsed Cycles          cycle       413548
    Average L2 Active Cycles         cycle      4405.04
    Total L2 Elapsed Cycles          cycle       177888
    Average SM Active Cycles         cycle      4963.84
    Total SM Elapsed Cycles          cycle       413548
    Average SMSP Active Cycles       cycle      4802.12
    Total SMSP Elapsed Cycles        cycle      1654192
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       797.15
    Elapsed Cycles                cycle         7302
    Memory Throughput                 %        49.29
    DRAM Throughput                   %        49.29
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.71
    L2 Cache Throughput               %        27.76
    SM Active Cycles              cycle      5015.53
    Compute (SM) Throughput           %        15.85
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.27
    Achieved Active Warps Per SM           warp        35.17
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.73%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27757.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5015.53
    Total L1 Elapsed Cycles          cycle       413482
    Average L2 Active Cycles         cycle      4498.25
    Total L2 Elapsed Cycles          cycle       178368
    Average SM Active Cycles         cycle      5015.53
    Total SM Elapsed Cycles          cycle       413482
    Average SMSP Active Cycles       cycle      4921.19
    Total SMSP Elapsed Cycles        cycle      1653928
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.06
    Elapsed Cycles                cycle         7204
    Memory Throughput                 %        49.02
    DRAM Throughput                   %        49.02
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.67
    L2 Cache Throughput               %        27.60
    SM Active Cycles              cycle      4928.71
    Compute (SM) Throughput           %        15.83
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.90
    Achieved Active Warps Per SM           warp        35.47
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.1%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27776
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      4928.71
    Total L1 Elapsed Cycles          cycle       414098
    Average L2 Active Cycles         cycle      4489.54
    Total L2 Elapsed Cycles          cycle       179304
    Average SM Active Cycles         cycle      4928.71
    Total SM Elapsed Cycles          cycle       414098
    Average SMSP Active Cycles       cycle      4909.89
    Total SMSP Elapsed Cycles        cycle      1656392
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4639792
    Memory Throughput                 %        49.00
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.24
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3592415.78
    Compute (SM) Throughput           %        45.48
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77154.67
    Total DRAM Elapsed Cycles        cycle    218674176
    Average L1 Active Cycles         cycle   3592415.78
    Total L1 Elapsed Cycles          cycle    268947906
    Average L2 Active Cycles         cycle   1206303.12
    Total L2 Elapsed Cycles          cycle    115556832
    Average SM Active Cycles         cycle   3592415.78
    Total SM Elapsed Cycles          cycle    268947906
    Average SMSP Active Cycles       cycle   3592052.50
    Total SMSP Elapsed Cycles        cycle   1075791624
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.47%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.55% above the average, while the minimum instance value is 7.85% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.5%                                                                                           
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.59% above the average, while the minimum instance value is 7.91% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.47%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.55% above the average, while the minimum instance value is 7.85% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       781.85
    Elapsed Cycles                cycle         6765
    Memory Throughput                 %        45.21
    DRAM Throughput                   %        45.21
    Duration                         us         8.64
    L1/TEX Cache Throughput           %        24.93
    L2 Cache Throughput               %        24.87
    SM Active Cycles              cycle      4532.78
    Compute (SM) Throughput           %        16.92
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.57
    Achieved Active Warps Per SM           warp        33.87
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.43%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23997.33
    Total DRAM Elapsed Cycles        cycle       318464
    Average L1 Active Cycles         cycle      4532.78
    Total L1 Elapsed Cycles          cycle       387352
    Average L2 Active Cycles         cycle      4142.38
    Total L2 Elapsed Cycles          cycle       168408
    Average SM Active Cycles         cycle      4532.78
    Total SM Elapsed Cycles          cycle       387352
    Average SMSP Active Cycles       cycle      4477.88
    Total SMSP Elapsed Cycles        cycle      1549408
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       784.22
    Elapsed Cycles                cycle         7185
    Memory Throughput                 %        48.92
    DRAM Throughput                   %        48.92
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.98
    L2 Cache Throughput               %        27.67
    SM Active Cycles              cycle      4963.62
    Compute (SM) Throughput           %        16.03
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.58
    Achieved Active Warps Per SM           warp        35.32
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.42%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27717.33
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      4963.62
    Total L1 Elapsed Cycles          cycle       408706
    Average L2 Active Cycles         cycle      4395.96
    Total L2 Elapsed Cycles          cycle       178872
    Average SM Active Cycles         cycle      4963.62
    Total SM Elapsed Cycles          cycle       408706
    Average SMSP Active Cycles       cycle      4828.32
    Total SMSP Elapsed Cycles        cycle      1634824
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       802.17
    Elapsed Cycles                cycle         7115
    Memory Throughput                 %        50.95
    DRAM Throughput                   %        50.95
    Duration                         us         8.83
    L1/TEX Cache Throughput           %        24.03
    L2 Cache Throughput               %        28.63
    SM Active Cycles              cycle      4998.09
    Compute (SM) Throughput           %        16.06
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.52
    Achieved Active Warps Per SM           warp        34.81
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.48%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27738.67
    Total DRAM Elapsed Cycles        cycle       326656
    Average L1 Active Cycles         cycle      4998.09
    Total L1 Elapsed Cycles          cycle       408128
    Average L2 Active Cycles         cycle      4462.29
    Total L2 Elapsed Cycles          cycle       173064
    Average SM Active Cycles         cycle      4998.09
    Total SM Elapsed Cycles          cycle       408128
    Average SMSP Active Cycles       cycle      4925.39
    Total SMSP Elapsed Cycles        cycle      1632512
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.35
    Elapsed Cycles                cycle         7172
    Memory Throughput                 %        49.28
    DRAM Throughput                   %        49.28
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.76
    L2 Cache Throughput               %        27.79
    SM Active Cycles              cycle      4976.29
    Compute (SM) Throughput           %        15.89
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.42
    Achieved Active Warps Per SM           warp        35.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.58%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27754.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4976.29
    Total L1 Elapsed Cycles          cycle       412482
    Average L2 Active Cycles         cycle      4487.83
    Total L2 Elapsed Cycles          cycle       178344
    Average SM Active Cycles         cycle      4976.29
    Total SM Elapsed Cycles          cycle       412482
    Average SMSP Active Cycles       cycle      4921.95
    Total SMSP Elapsed Cycles        cycle      1649928
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       801.96
    Elapsed Cycles                cycle      4658788
    Memory Throughput                 %        48.92
    DRAM Throughput                   %         0.21
    Duration                         ms         5.80
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.61
    SM Active Cycles              cycle   3592214.90
    Compute (SM) Throughput           %        45.41
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76522.67
    Total DRAM Elapsed Cycles        cycle    217190400
    Average L1 Active Cycles         cycle   3592214.90
    Total L1 Elapsed Cycles          cycle    269365468
    Average L2 Active Cycles         cycle   1206414.38
    Total L2 Elapsed Cycles          cycle    114796584
    Average SM Active Cycles         cycle   3592214.90
    Total SM Elapsed Cycles          cycle    269365468
    Average SMSP Active Cycles       cycle   3592205.83
    Total SMSP Elapsed Cycles        cycle   1077461872
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.44%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.55% above the average, while the minimum instance value is 7.96% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.54%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.68% above the average, while the minimum instance value is 7.99% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.44%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.55% above the average, while the minimum instance value is 7.96% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       781.25
    Elapsed Cycles                cycle         6654
    Memory Throughput                 %        45.56
    DRAM Throughput                   %        45.56
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        25.73
    L2 Cache Throughput               %        25.32
    SM Active Cycles              cycle      4390.78
    Compute (SM) Throughput           %        17.21
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.55
    Achieved Active Warps Per SM           warp        35.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.45%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23794.67
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4390.78
    Total L1 Elapsed Cycles          cycle       380832
    Average L2 Active Cycles         cycle      3972.67
    Total L2 Elapsed Cycles          cycle       165504
    Average SM Active Cycles         cycle      4390.78
    Total SM Elapsed Cycles          cycle       380832
    Average SMSP Active Cycles       cycle      4351.05
    Total SMSP Elapsed Cycles        cycle      1523328
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.92
    Elapsed Cycles                cycle         7347
    Memory Throughput                 %        48.39
    DRAM Throughput                   %        48.39
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        23.55
    L2 Cache Throughput               %        27.07
    SM Active Cycles              cycle      4968.52
    Compute (SM) Throughput           %        15.74
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.66
    Achieved Active Warps Per SM           warp        35.36
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.34%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27832
    Total DRAM Elapsed Cycles        cycle       345088
    Average L1 Active Cycles         cycle      4968.52
    Total L1 Elapsed Cycles          cycle       416470
    Average L2 Active Cycles         cycle      4426.38
    Total L2 Elapsed Cycles          cycle       182784
    Average SM Active Cycles         cycle      4968.52
    Total SM Elapsed Cycles          cycle       416470
    Average SMSP Active Cycles       cycle      4816.35
    Total SMSP Elapsed Cycles        cycle      1665880
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.80
    Elapsed Cycles                cycle         7201
    Memory Throughput                 %        49.20
    DRAM Throughput                   %        49.20
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.59
    L2 Cache Throughput               %        27.62
    SM Active Cycles              cycle      4997.72
    Compute (SM) Throughput           %        15.77
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.65
    Achieved Active Warps Per SM           warp        34.39
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27794.67
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      4997.72
    Total L1 Elapsed Cycles          cycle       415546
    Average L2 Active Cycles         cycle      4545.54
    Total L2 Elapsed Cycles          cycle       179184
    Average SM Active Cycles         cycle      4997.72
    Total SM Elapsed Cycles          cycle       415546
    Average SMSP Active Cycles       cycle      4945.63
    Total SMSP Elapsed Cycles        cycle      1662184
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.47
    Elapsed Cycles                cycle         7258
    Memory Throughput                 %        48.93
    DRAM Throughput                   %        48.93
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.75
    L2 Cache Throughput               %        27.42
    SM Active Cycles              cycle      5050.86
    Compute (SM) Throughput           %        15.88
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.13
    Achieved Active Warps Per SM           warp        35.10
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.87%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27808
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5050.86
    Total L1 Elapsed Cycles          cycle       412794
    Average L2 Active Cycles         cycle      4566.67
    Total L2 Elapsed Cycles          cycle       180576
    Average SM Active Cycles         cycle      5050.86
    Total SM Elapsed Cycles          cycle       412794
    Average SMSP Active Cycles       cycle      4922.42
    Total SMSP Elapsed Cycles        cycle      1651176
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4639717
    Memory Throughput                 %        48.96
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3591859.24
    Compute (SM) Throughput           %        45.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77365.33
    Total DRAM Elapsed Cycles        cycle    218671104
    Average L1 Active Cycles         cycle   3591859.24
    Total L1 Elapsed Cycles          cycle    269125222
    Average L2 Active Cycles         cycle   1235728.25
    Total L2 Elapsed Cycles          cycle    115555176
    Average SM Active Cycles         cycle   3591859.24
    Total SM Elapsed Cycles          cycle    269125222
    Average SMSP Active Cycles       cycle   3591569.79
    Total SMSP Elapsed Cycles        cycle   1076500888
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.47%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.57% above the average, while the minimum instance value is 7.88% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.5%                                                                                           
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.61% above the average, while the minimum instance value is 7.93% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.47%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.57% above the average, while the minimum instance value is 7.88% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.82
    Elapsed Cycles                cycle         6771
    Memory Throughput                 %        45.15
    DRAM Throughput                   %        45.15
    Duration                         us         8.64
    L1/TEX Cache Throughput           %        24.84
    L2 Cache Throughput               %        24.86
    SM Active Cycles              cycle      4549.09
    Compute (SM) Throughput           %        17.34
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.31
    Achieved Active Warps Per SM           warp        34.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.69%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24042.67
    Total DRAM Elapsed Cycles        cycle       319488
    Average L1 Active Cycles         cycle      4549.09
    Total L1 Elapsed Cycles          cycle       378052
    Average L2 Active Cycles         cycle      4135.75
    Total L2 Elapsed Cycles          cycle       168600
    Average SM Active Cycles         cycle      4549.09
    Total SM Elapsed Cycles          cycle       378052
    Average SMSP Active Cycles       cycle      4456.40
    Total SMSP Elapsed Cycles        cycle      1512208
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.50
    Elapsed Cycles                cycle         7378
    Memory Throughput                 %        47.78
    DRAM Throughput                   %        47.78
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        23.72
    L2 Cache Throughput               %        26.97
    SM Active Cycles              cycle      4953.79
    Compute (SM) Throughput           %        15.86
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.87
    Achieved Active Warps Per SM           warp        35.46
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.13%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27765.33
    Total DRAM Elapsed Cycles        cycle       348672
    Average L1 Active Cycles         cycle      4953.79
    Total L1 Elapsed Cycles          cycle       413202
    Average L2 Active Cycles         cycle      4545.62
    Total L2 Elapsed Cycles          cycle       183672
    Average SM Active Cycles         cycle      4953.79
    Total SM Elapsed Cycles          cycle       413202
    Average SMSP Active Cycles       cycle      4970.11
    Total SMSP Elapsed Cycles        cycle      1652808
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.33
    Elapsed Cycles                cycle         7094
    Memory Throughput                 %        49.74
    DRAM Throughput                   %        49.74
    Duration                         us         9.06
    L1/TEX Cache Throughput           %        23.53
    L2 Cache Throughput               %        28.01
    SM Active Cycles              cycle      4979.29
    Compute (SM) Throughput           %        15.73
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.61
    Achieved Active Warps Per SM           warp        34.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.39%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27760
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      4979.29
    Total L1 Elapsed Cycles          cycle       416642
    Average L2 Active Cycles         cycle      4507.83
    Total L2 Elapsed Cycles          cycle       176712
    Average SM Active Cycles         cycle      4979.29
    Total SM Elapsed Cycles          cycle       416642
    Average SMSP Active Cycles       cycle      4949.82
    Total SMSP Elapsed Cycles        cycle      1666568
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.70
    Elapsed Cycles                cycle         7178
    Memory Throughput                 %        49.44
    DRAM Throughput                   %        49.44
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.90
    L2 Cache Throughput               %        27.70
    SM Active Cycles              cycle      4915.95
    Compute (SM) Throughput           %        15.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.90
    Achieved Active Warps Per SM           warp        34.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.1%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27845.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4915.95
    Total L1 Elapsed Cycles          cycle       410130
    Average L2 Active Cycles         cycle      4583.17
    Total L2 Elapsed Cycles          cycle       178728
    Average SM Active Cycles         cycle      4915.95
    Total SM Elapsed Cycles          cycle       410130
    Average SMSP Active Cycles       cycle      4993.97
    Total SMSP Elapsed Cycles        cycle      1640520
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4644300
    Memory Throughput                 %        48.92
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.58
    SM Active Cycles              cycle   3592152.71
    Compute (SM) Throughput           %        45.41
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77426.67
    Total DRAM Elapsed Cycles        cycle    218886144
    Average L1 Active Cycles         cycle   3592152.71
    Total L1 Elapsed Cycles          cycle    269347490
    Average L2 Active Cycles         cycle   1231448.29
    Total L2 Elapsed Cycles          cycle    115669416
    Average SM Active Cycles         cycle   3592152.71
    Total SM Elapsed Cycles          cycle    269347490
    Average SMSP Active Cycles       cycle   3591798.69
    Total SMSP Elapsed Cycles        cycle   1077389960
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.46%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.57% above the average, while the minimum instance value is 7.92% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.43%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.54% above the average, while the minimum instance value is 7.88% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.46%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.57% above the average, while the minimum instance value is 7.92% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       784.12
    Elapsed Cycles                cycle         6501
    Memory Throughput                 %        46.77
    DRAM Throughput                   %        46.77
    Duration                         us         8.29
    L1/TEX Cache Throughput           %        24.95
    L2 Cache Throughput               %        25.92
    SM Active Cycles              cycle      4528.79
    Compute (SM) Throughput           %        17.49
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.13
    Achieved Active Warps Per SM           warp        33.66
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.87%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23786.67
    Total DRAM Elapsed Cycles        cycle       305152
    Average L1 Active Cycles         cycle      4528.79
    Total L1 Elapsed Cycles          cycle       374604
    Average L2 Active Cycles         cycle      3959.21
    Total L2 Elapsed Cycles          cycle       161856
    Average SM Active Cycles         cycle      4528.79
    Total SM Elapsed Cycles          cycle       374604
    Average SMSP Active Cycles       cycle      4325.69
    Total SMSP Elapsed Cycles        cycle      1498416
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       785.06
    Elapsed Cycles                cycle         7418
    Memory Throughput                 %        47.50
    DRAM Throughput                   %        47.50
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.67
    L2 Cache Throughput               %        26.78
    SM Active Cycles              cycle      5031.48
    Compute (SM) Throughput           %        15.81
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.73
    Achieved Active Warps Per SM           warp        34.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.27%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27725.33
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5031.48
    Total L1 Elapsed Cycles          cycle       414538
    Average L2 Active Cycles         cycle      4497.92
    Total L2 Elapsed Cycles          cycle       184824
    Average SM Active Cycles         cycle      5031.48
    Total SM Elapsed Cycles          cycle       414538
    Average SMSP Active Cycles       cycle      4879.48
    Total SMSP Elapsed Cycles        cycle      1658152
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       782.95
    Elapsed Cycles                cycle         7270
    Memory Throughput                 %        48.79
    DRAM Throughput                   %        48.79
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.80
    L2 Cache Throughput               %        27.30
    SM Active Cycles              cycle      4902.81
    Compute (SM) Throughput           %        15.91
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.73
    Achieved Active Warps Per SM           warp        35.39
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.27%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27810.67
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle      4902.81
    Total L1 Elapsed Cycles          cycle       412036
    Average L2 Active Cycles         cycle      4484.21
    Total L2 Elapsed Cycles          cycle       181176
    Average SM Active Cycles         cycle      4902.81
    Total SM Elapsed Cycles          cycle       412036
    Average SMSP Active Cycles       cycle      4881.63
    Total SMSP Elapsed Cycles        cycle      1648144
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.00
    Elapsed Cycles                cycle         7251
    Memory Throughput                 %        48.99
    DRAM Throughput                   %        48.99
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.96
    L2 Cache Throughput               %        27.42
    SM Active Cycles              cycle      4988.36
    Compute (SM) Throughput           %        16.02
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.38
    Achieved Active Warps Per SM           warp        35.22
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.62%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27842.67
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4988.36
    Total L1 Elapsed Cycles          cycle       409028
    Average L2 Active Cycles         cycle      4483.79
    Total L2 Elapsed Cycles          cycle       180480
    Average SM Active Cycles         cycle      4988.36
    Total SM Elapsed Cycles          cycle       409028
    Average SMSP Active Cycles       cycle      4888.83
    Total SMSP Elapsed Cycles        cycle      1636112
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4645404
    Memory Throughput                 %        48.94
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.58
    SM Active Cycles              cycle   3591894.64
    Compute (SM) Throughput           %        45.43
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76573.33
    Total DRAM Elapsed Cycles        cycle    218938368
    Average L1 Active Cycles         cycle   3591894.64
    Total L1 Elapsed Cycles          cycle    269256822
    Average L2 Active Cycles         cycle   1211485.62
    Total L2 Elapsed Cycles          cycle    115696800
    Average SM Active Cycles         cycle   3591894.64
    Total SM Elapsed Cycles          cycle    269256822
    Average SMSP Active Cycles       cycle   3592073.06
    Total SMSP Elapsed Cycles        cycle   1077027288
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.47%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.58% above the average, while the minimum instance value is 7.86% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.47%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.58% above the average, while the minimum instance value is 7.90% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.47%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.58% above the average, while the minimum instance value is 7.86% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.53
    Elapsed Cycles                cycle         6751
    Memory Throughput                 %        45.46
    DRAM Throughput                   %        45.46
    Duration                         us         8.61
    L1/TEX Cache Throughput           %        25.54
    L2 Cache Throughput               %        24.92
    SM Active Cycles              cycle      4424.33
    Compute (SM) Throughput           %        16.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.27
    Achieved Active Warps Per SM           warp        34.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.73%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24050.67
    Total DRAM Elapsed Cycles        cycle       317440
    Average L1 Active Cycles         cycle      4424.33
    Total L1 Elapsed Cycles          cycle       385966
    Average L2 Active Cycles         cycle      4073.12
    Total L2 Elapsed Cycles          cycle       168192
    Average SM Active Cycles         cycle      4424.33
    Total SM Elapsed Cycles          cycle       385966
    Average SMSP Active Cycles       cycle      4426.52
    Total SMSP Elapsed Cycles        cycle      1543864
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       785.52
    Elapsed Cycles                cycle         7222
    Memory Throughput                 %        49.21
    DRAM Throughput                   %        49.21
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        24.09
    L2 Cache Throughput               %        27.53
    SM Active Cycles              cycle      4978.19
    Compute (SM) Throughput           %        16.10
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.54
    Achieved Active Warps Per SM           warp        34.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.46%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27965.33
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4978.19
    Total L1 Elapsed Cycles          cycle       406956
    Average L2 Active Cycles         cycle      4543.58
    Total L2 Elapsed Cycles          cycle       179856
    Average SM Active Cycles         cycle      4978.19
    Total SM Elapsed Cycles          cycle       406956
    Average SMSP Active Cycles       cycle      4975.31
    Total SMSP Elapsed Cycles        cycle      1627824
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       787.65
    Elapsed Cycles                cycle         7287
    Memory Throughput                 %        48.68
    DRAM Throughput                   %        48.68
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.84
    L2 Cache Throughput               %        27.28
    SM Active Cycles              cycle      4999.36
    Compute (SM) Throughput           %        15.94
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.45
    Achieved Active Warps Per SM           warp        34.78
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.55%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27746.67
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle      4999.36
    Total L1 Elapsed Cycles          cycle       411122
    Average L2 Active Cycles         cycle      4547.62
    Total L2 Elapsed Cycles          cycle       181512
    Average SM Active Cycles         cycle      4999.36
    Total SM Elapsed Cycles          cycle       411122
    Average SMSP Active Cycles       cycle      4956.14
    Total SMSP Elapsed Cycles        cycle      1644488
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       789.84
    Elapsed Cycles                cycle         7327
    Memory Throughput                 %        48.80
    DRAM Throughput                   %        48.80
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.46
    L2 Cache Throughput               %        27.31
    SM Active Cycles              cycle      5027.53
    Compute (SM) Throughput           %        15.68
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.78
    Achieved Active Warps Per SM           warp        35.41
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.22%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27818.67
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle      5027.53
    Total L1 Elapsed Cycles          cycle       417894
    Average L2 Active Cycles         cycle      4563.58
    Total L2 Elapsed Cycles          cycle       181128
    Average SM Active Cycles         cycle      5027.53
    Total SM Elapsed Cycles          cycle       417894
    Average SMSP Active Cycles       cycle      4989.17
    Total SMSP Elapsed Cycles        cycle      1671576
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       805.91
    Elapsed Cycles                cycle      4670811
    Memory Throughput                 %        48.96
    DRAM Throughput                   %         0.21
    Duration                         ms         5.78
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.62
    SM Active Cycles              cycle   3591916.52
    Compute (SM) Throughput           %        45.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76693.33
    Total DRAM Elapsed Cycles        cycle    216505344
    Average L1 Active Cycles         cycle   3591916.52
    Total L1 Elapsed Cycles          cycle    269171250
    Average L2 Active Cycles         cycle   1214082.42
    Total L2 Elapsed Cycles          cycle    114522360
    Average SM Active Cycles         cycle   3591916.52
    Total SM Elapsed Cycles          cycle    269171250
    Average SMSP Active Cycles       cycle   3592144.72
    Total SMSP Elapsed Cycles        cycle   1076685000
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.47%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.57% above the average, while the minimum instance value is 7.94% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.56%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.69% above the average, while the minimum instance value is 7.93% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.47%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.57% above the average, while the minimum instance value is 7.94% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.62
    Elapsed Cycles                cycle         6640
    Memory Throughput                 %        45.56
    DRAM Throughput                   %        45.56
    Duration                         us         8.48
    L1/TEX Cache Throughput           %        25.33
    L2 Cache Throughput               %        25.36
    SM Active Cycles              cycle      4461.33
    Compute (SM) Throughput           %        17.22
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.66
    Achieved Active Warps Per SM           warp        34.40
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.34%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23792
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4461.33
    Total L1 Elapsed Cycles          cycle       380680
    Average L2 Active Cycles         cycle      4012.29
    Total L2 Elapsed Cycles          cycle       165312
    Average SM Active Cycles         cycle      4461.33
    Total SM Elapsed Cycles          cycle       380680
    Average SMSP Active Cycles       cycle      4418.71
    Total SMSP Elapsed Cycles        cycle      1522720
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.10
    Elapsed Cycles                cycle         7274
    Memory Throughput                 %        48.54
    DRAM Throughput                   %        48.54
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.53
    L2 Cache Throughput               %        27.31
    SM Active Cycles              cycle      5029.33
    Compute (SM) Throughput           %        15.74
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.01
    Achieved Active Warps Per SM           warp        35.05
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27837.33
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      5029.33
    Total L1 Elapsed Cycles          cycle       416480
    Average L2 Active Cycles         cycle      4495.88
    Total L2 Elapsed Cycles          cycle       181224
    Average SM Active Cycles         cycle      5029.33
    Total SM Elapsed Cycles          cycle       416480
    Average SMSP Active Cycles       cycle      4889.51
    Total SMSP Elapsed Cycles        cycle      1665920
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.22
    Elapsed Cycles                cycle         7289
    Memory Throughput                 %        48.34
    DRAM Throughput                   %        48.34
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.45
    L2 Cache Throughput               %        27.28
    SM Active Cycles              cycle      4948.21
    Compute (SM) Throughput           %        15.67
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.28
    Achieved Active Warps Per SM           warp        35.17
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.72%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27720
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      4948.21
    Total L1 Elapsed Cycles          cycle       418196
    Average L2 Active Cycles         cycle      4414.29
    Total L2 Elapsed Cycles          cycle       181464
    Average SM Active Cycles         cycle      4948.21
    Total SM Elapsed Cycles          cycle       418196
    Average SMSP Active Cycles       cycle      4818.87
    Total SMSP Elapsed Cycles        cycle      1672784
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.91
    Elapsed Cycles                cycle         7384
    Memory Throughput                 %        48.14
    DRAM Throughput                   %        48.14
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        23.59
    L2 Cache Throughput               %        26.94
    SM Active Cycles              cycle      4973.50
    Compute (SM) Throughput           %        15.77
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.07
    Achieved Active Warps Per SM           warp        35.55
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27850.67
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      4973.50
    Total L1 Elapsed Cycles          cycle       415498
    Average L2 Active Cycles         cycle      4508.79
    Total L2 Elapsed Cycles          cycle       183768
    Average SM Active Cycles         cycle      4973.50
    Total SM Elapsed Cycles          cycle       415498
    Average SMSP Active Cycles       cycle         4955
    Total SMSP Elapsed Cycles        cycle      1661992
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4639133
    Memory Throughput                 %        48.92
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.24
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3592744.34
    Compute (SM) Throughput           %        45.41
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.59
    Achieved Active Warps Per SM           warp         8.44
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76661.33
    Total DRAM Elapsed Cycles        cycle    218643456
    Average L1 Active Cycles         cycle   3592744.34
    Total L1 Elapsed Cycles          cycle    269340288
    Average L2 Active Cycles         cycle      1198703
    Total L2 Elapsed Cycles          cycle    115540632
    Average SM Active Cycles         cycle   3592744.34
    Total SM Elapsed Cycles          cycle    269340288
    Average SMSP Active Cycles       cycle   3592060.35
    Total SMSP Elapsed Cycles        cycle   1077361152
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.47%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.58% above the average, while the minimum instance value is 8.00% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.45%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.56% above the average, while the minimum instance value is 7.93% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.47%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.58% above the average, while the minimum instance value is 8.00% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.16
    Elapsed Cycles                cycle         6649
    Memory Throughput                 %        45.93
    DRAM Throughput                   %        45.93
    Duration                         us         8.48
    L1/TEX Cache Throughput           %        25.15
    L2 Cache Throughput               %        25.32
    SM Active Cycles              cycle      4492.57
    Compute (SM) Throughput           %        16.81
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.45
    Achieved Active Warps Per SM           warp        33.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.55%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23984
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4492.57
    Total L1 Elapsed Cycles          cycle       389902
    Average L2 Active Cycles         cycle      4128.67
    Total L2 Elapsed Cycles          cycle       165624
    Average SM Active Cycles         cycle      4492.57
    Total SM Elapsed Cycles          cycle       389902
    Average SMSP Active Cycles       cycle      4472.43
    Total SMSP Elapsed Cycles        cycle      1559608
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.45
    Elapsed Cycles                cycle         7346
    Memory Throughput                 %        48.47
    DRAM Throughput                   %        48.47
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        23.28
    L2 Cache Throughput               %        27.05
    SM Active Cycles              cycle      4995.57
    Compute (SM) Throughput           %        15.57
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.39
    Achieved Active Warps Per SM           warp        35.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.61%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27874.67
    Total DRAM Elapsed Cycles        cycle       345088
    Average L1 Active Cycles         cycle      4995.57
    Total L1 Elapsed Cycles          cycle       420940
    Average L2 Active Cycles         cycle      4542.96
    Total L2 Elapsed Cycles          cycle       182976
    Average SM Active Cycles         cycle      4995.57
    Total SM Elapsed Cycles          cycle       420940
    Average SMSP Active Cycles       cycle      4981.03
    Total SMSP Elapsed Cycles        cycle      1683760
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.83
    Elapsed Cycles                cycle         7180
    Memory Throughput                 %        49.31
    DRAM Throughput                   %        49.31
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.77
    L2 Cache Throughput               %        27.68
    SM Active Cycles              cycle      4997.90
    Compute (SM) Throughput           %        15.89
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.79
    Achieved Active Warps Per SM           warp        34.94
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.21%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27770.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4997.90
    Total L1 Elapsed Cycles          cycle       412370
    Average L2 Active Cycles         cycle      4589.21
    Total L2 Elapsed Cycles          cycle       178896
    Average SM Active Cycles         cycle      4997.90
    Total SM Elapsed Cycles          cycle       412370
    Average SMSP Active Cycles       cycle      4990.50
    Total SMSP Elapsed Cycles        cycle      1649480
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       781.38
    Elapsed Cycles                cycle         7182
    Memory Throughput                 %        49.20
    DRAM Throughput                   %        49.20
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.69
    L2 Cache Throughput               %        27.66
    SM Active Cycles              cycle      4975.79
    Compute (SM) Throughput           %        15.83
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.37
    Achieved Active Warps Per SM           warp        35.22
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.63%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27794.67
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      4975.79
    Total L1 Elapsed Cycles          cycle       414054
    Average L2 Active Cycles         cycle      4507.54
    Total L2 Elapsed Cycles          cycle       178944
    Average SM Active Cycles         cycle      4975.79
    Total SM Elapsed Cycles          cycle       414054
    Average SMSP Active Cycles       cycle      4931.22
    Total SMSP Elapsed Cycles        cycle      1656216
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4638636
    Memory Throughput                 %        48.94
    DRAM Throughput                   %         0.21
    Duration                         ms         5.83
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3591878.09
    Compute (SM) Throughput           %        45.43
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        78264
    Total DRAM Elapsed Cycles        cycle    218618880
    Average L1 Active Cycles         cycle   3591878.09
    Total L1 Elapsed Cycles          cycle    269256638
    Average L2 Active Cycles         cycle   1212732.46
    Total L2 Elapsed Cycles          cycle    115528128
    Average SM Active Cycles         cycle   3591878.09
    Total SM Elapsed Cycles          cycle    269256638
    Average SMSP Active Cycles       cycle   3592210.17
    Total SMSP Elapsed Cycles        cycle   1077026552
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.5%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.62% above the average, while the minimum instance value is 7.97% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.46%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.57% above the average, while the minimum instance value is 7.89% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.5%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.62% above the average, while the minimum instance value is 7.97% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.62
    Elapsed Cycles                cycle         6691
    Memory Throughput                 %        45.35
    DRAM Throughput                   %        45.35
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        25.79
    L2 Cache Throughput               %        25.18
    SM Active Cycles              cycle      4380.52
    Compute (SM) Throughput           %        17.05
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.81
    Achieved Active Warps Per SM           warp        35.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.19%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23762.67
    Total DRAM Elapsed Cycles        cycle       314368
    Average L1 Active Cycles         cycle      4380.52
    Total L1 Elapsed Cycles          cycle       384382
    Average L2 Active Cycles         cycle      3953.25
    Total L2 Elapsed Cycles          cycle       166584
    Average SM Active Cycles         cycle      4380.52
    Total SM Elapsed Cycles          cycle       384382
    Average SMSP Active Cycles       cycle      4316.46
    Total SMSP Elapsed Cycles        cycle      1537528
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       783.12
    Elapsed Cycles                cycle         7198
    Memory Throughput                 %        49.28
    DRAM Throughput                   %        49.28
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        24.09
    L2 Cache Throughput               %        27.62
    SM Active Cycles              cycle      4975.16
    Compute (SM) Throughput           %        16.09
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.95
    Achieved Active Warps Per SM           warp        34.54
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.05%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27754.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4975.16
    Total L1 Elapsed Cycles          cycle       407266
    Average L2 Active Cycles         cycle      4488.04
    Total L2 Elapsed Cycles          cycle       179352
    Average SM Active Cycles         cycle      4975.16
    Total SM Elapsed Cycles          cycle       407266
    Average SMSP Active Cycles       cycle      4929.07
    Total SMSP Elapsed Cycles        cycle      1629064
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.94
    Elapsed Cycles                cycle         7249
    Memory Throughput                 %        49.03
    DRAM Throughput                   %        49.03
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.83
    L2 Cache Throughput               %        27.46
    SM Active Cycles              cycle      4888.48
    Compute (SM) Throughput           %        15.92
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.83
    Achieved Active Warps Per SM           warp        35.44
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27864
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4888.48
    Total L1 Elapsed Cycles          cycle       411758
    Average L2 Active Cycles         cycle      4481.12
    Total L2 Elapsed Cycles          cycle       180384
    Average SM Active Cycles         cycle      4888.48
    Total SM Elapsed Cycles          cycle       411758
    Average SMSP Active Cycles       cycle      4897.84
    Total SMSP Elapsed Cycles        cycle      1647032
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       782.93
    Elapsed Cycles                cycle         7272
    Memory Throughput                 %        48.76
    DRAM Throughput                   %        48.76
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.46
    L2 Cache Throughput               %        27.34
    SM Active Cycles              cycle      5033.19
    Compute (SM) Throughput           %        15.68
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.30
    Achieved Active Warps Per SM           warp        34.71
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.7%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27794.67
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle      5033.19
    Total L1 Elapsed Cycles          cycle       417912
    Average L2 Active Cycles         cycle      4481.88
    Total L2 Elapsed Cycles          cycle       181104
    Average SM Active Cycles         cycle      5033.19
    Total SM Elapsed Cycles          cycle       417912
    Average SMSP Active Cycles       cycle      4913.24
    Total SMSP Elapsed Cycles        cycle      1671648
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4640674
    Memory Throughput                 %        48.95
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.26
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3591430.90
    Compute (SM) Throughput           %        45.43
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77002.67
    Total DRAM Elapsed Cycles        cycle    218717184
    Average L1 Active Cycles         cycle   3591430.90
    Total L1 Elapsed Cycles          cycle    269203198
    Average L2 Active Cycles         cycle   1207996.25
    Total L2 Elapsed Cycles          cycle    115578984
    Average SM Active Cycles         cycle   3591430.90
    Total SM Elapsed Cycles          cycle    269203198
    Average SMSP Active Cycles       cycle   3591926.93
    Total SMSP Elapsed Cycles        cycle   1076812792
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.49%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.60% above the average, while the minimum instance value is 7.92% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.45%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.54% above the average, while the minimum instance value is 7.92% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.49%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.60% above the average, while the minimum instance value is 7.92% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.54
    Elapsed Cycles                cycle         6728
    Memory Throughput                 %        45.55
    DRAM Throughput                   %        45.55
    Duration                         us         8.58
    L1/TEX Cache Throughput           %        25.70
    L2 Cache Throughput               %        25.01
    SM Active Cycles              cycle      4395.86
    Compute (SM) Throughput           %        16.95
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.91
    Achieved Active Warps Per SM           warp        34.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.09%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24021.33
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4395.86
    Total L1 Elapsed Cycles          cycle       386622
    Average L2 Active Cycles         cycle      4100.29
    Total L2 Elapsed Cycles          cycle       167544
    Average SM Active Cycles         cycle      4395.86
    Total SM Elapsed Cycles          cycle       386622
    Average SMSP Active Cycles       cycle      4405.44
    Total SMSP Elapsed Cycles        cycle      1546488
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.60
    Elapsed Cycles                cycle         7205
    Memory Throughput                 %        49.14
    DRAM Throughput                   %        49.14
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        24.27
    L2 Cache Throughput               %        27.62
    SM Active Cycles              cycle      4848.05
    Compute (SM) Throughput           %        16.23
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 24.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.18
    Achieved Active Warps Per SM           warp        36.08
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.82%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27757.33
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      4848.05
    Total L1 Elapsed Cycles          cycle       403886
    Average L2 Active Cycles         cycle      4506.04
    Total L2 Elapsed Cycles          cycle       179328
    Average SM Active Cycles         cycle      4848.05
    Total SM Elapsed Cycles          cycle       403886
    Average SMSP Active Cycles       cycle      4905.19
    Total SMSP Elapsed Cycles        cycle      1615544
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.18
    Elapsed Cycles                cycle         7238
    Memory Throughput                 %        48.95
    DRAM Throughput                   %        48.95
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.67
    L2 Cache Throughput               %        27.47
    SM Active Cycles              cycle      4948.48
    Compute (SM) Throughput           %        15.82
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.67
    Achieved Active Warps Per SM           warp        35.36
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27818.67
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4948.48
    Total L1 Elapsed Cycles          cycle       414198
    Average L2 Active Cycles         cycle      4477.62
    Total L2 Elapsed Cycles          cycle       180168
    Average SM Active Cycles         cycle      4948.48
    Total SM Elapsed Cycles          cycle       414198
    Average SMSP Active Cycles       cycle      4884.22
    Total SMSP Elapsed Cycles        cycle      1656792
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       782.27
    Elapsed Cycles                cycle         7195
    Memory Throughput                 %        48.99
    DRAM Throughput                   %        48.99
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.91
    L2 Cache Throughput               %        27.63
    SM Active Cycles              cycle      4997.10
    Compute (SM) Throughput           %        15.99
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.82
    Achieved Active Warps Per SM           warp        35.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.18%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27760
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      4997.10
    Total L1 Elapsed Cycles          cycle       409890
    Average L2 Active Cycles         cycle      4587.38
    Total L2 Elapsed Cycles          cycle       179184
    Average SM Active Cycles         cycle      4997.10
    Total SM Elapsed Cycles          cycle       409890
    Average SMSP Active Cycles       cycle      5003.97
    Total SMSP Elapsed Cycles        cycle      1639560
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4641053
    Memory Throughput                 %        48.96
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.24
    L2 Cache Throughput               %         3.58
    SM Active Cycles              cycle   3592549.16
    Compute (SM) Throughput           %        45.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.59
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76717.33
    Total DRAM Elapsed Cycles        cycle    218733568
    Average L1 Active Cycles         cycle   3592549.16
    Total L1 Elapsed Cycles          cycle    269147282
    Average L2 Active Cycles         cycle   1223419.96
    Total L2 Elapsed Cycles          cycle    115588392
    Average SM Active Cycles         cycle   3592549.16
    Total SM Elapsed Cycles          cycle    269147282
    Average SMSP Active Cycles       cycle   3592044.30
    Total SMSP Elapsed Cycles        cycle   1076589128
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.46%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.56% above the average, while the minimum instance value is 7.94% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.47%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.58% above the average, while the minimum instance value is 7.93% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.46%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.56% above the average, while the minimum instance value is 7.94% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       781.84
    Elapsed Cycles                cycle         6611
    Memory Throughput                 %        45.77
    DRAM Throughput                   %        45.77
    Duration                         us         8.45
    L1/TEX Cache Throughput           %        25.43
    L2 Cache Throughput               %        25.44
    SM Active Cycles              cycle      4442.91
    Compute (SM) Throughput           %        17.14
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.37
    Achieved Active Warps Per SM           warp        34.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.63%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23824
    Total DRAM Elapsed Cycles        cycle       312320
    Average L1 Active Cycles         cycle      4442.91
    Total L1 Elapsed Cycles          cycle       382324
    Average L2 Active Cycles         cycle      3906.67
    Total L2 Elapsed Cycles          cycle       164712
    Average SM Active Cycles         cycle      4442.91
    Total SM Elapsed Cycles          cycle       382324
    Average SMSP Active Cycles       cycle      4299.87
    Total SMSP Elapsed Cycles        cycle      1529296
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       784.26
    Elapsed Cycles                cycle         7212
    Memory Throughput                 %        48.89
    DRAM Throughput                   %        48.89
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.92
    L2 Cache Throughput               %        27.57
    SM Active Cycles              cycle      4995.43
    Compute (SM) Throughput           %        15.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.18
    Achieved Active Warps Per SM           warp        34.16
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.82%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27786.67
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4995.43
    Total L1 Elapsed Cycles          cycle       410232
    Average L2 Active Cycles         cycle      4531.17
    Total L2 Elapsed Cycles          cycle       179448
    Average SM Active Cycles         cycle      4995.43
    Total SM Elapsed Cycles          cycle       410232
    Average SMSP Active Cycles       cycle      4939.93
    Total SMSP Elapsed Cycles        cycle      1640928
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       785.24
    Elapsed Cycles                cycle         7295
    Memory Throughput                 %        48.92
    DRAM Throughput                   %        48.92
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.64
    L2 Cache Throughput               %        27.27
    SM Active Cycles              cycle      4983.83
    Compute (SM) Throughput           %        15.80
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.78
    Achieved Active Warps Per SM           warp        35.41
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.22%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27888
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle      4983.83
    Total L1 Elapsed Cycles          cycle       414694
    Average L2 Active Cycles         cycle      4426.54
    Total L2 Elapsed Cycles          cycle       181632
    Average SM Active Cycles         cycle      4983.83
    Total SM Elapsed Cycles          cycle       414694
    Average SMSP Active Cycles       cycle      4837.50
    Total SMSP Elapsed Cycles        cycle      1658776
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       785.37
    Elapsed Cycles                cycle         7219
    Memory Throughput                 %        48.81
    DRAM Throughput                   %        48.81
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.83
    L2 Cache Throughput               %        27.50
    SM Active Cycles              cycle      5042.81
    Compute (SM) Throughput           %        15.92
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.09
    Achieved Active Warps Per SM           warp        34.60
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27738.67
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5042.81
    Total L1 Elapsed Cycles          cycle       411666
    Average L2 Active Cycles         cycle      4565.71
    Total L2 Elapsed Cycles          cycle       179952
    Average SM Active Cycles         cycle      5042.81
    Total SM Elapsed Cycles          cycle       411666
    Average SMSP Active Cycles       cycle      4953.39
    Total SMSP Elapsed Cycles        cycle      1646664
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4641242
    Memory Throughput                 %        48.99
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.26
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3591706.67
    Compute (SM) Throughput           %        45.47
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76717.33
    Total DRAM Elapsed Cycles        cycle    218741760
    Average L1 Active Cycles         cycle   3591706.67
    Total L1 Elapsed Cycles          cycle    268971104
    Average L2 Active Cycles         cycle   1209495.71
    Total L2 Elapsed Cycles          cycle    115593120
    Average SM Active Cycles         cycle   3591706.67
    Total SM Elapsed Cycles          cycle    268971104
    Average SMSP Active Cycles       cycle   3591638.65
    Total SMSP Elapsed Cycles        cycle   1075884416
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.46%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.55% above the average, while the minimum instance value is 7.87% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.55%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.66% above the average, while the minimum instance value is 7.84% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.46%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.55% above the average, while the minimum instance value is 7.87% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       786.01
    Elapsed Cycles                cycle         6747
    Memory Throughput                 %        45.44
    DRAM Throughput                   %        45.44
    Duration                         us         8.58
    L1/TEX Cache Throughput           %        25.71
    L2 Cache Throughput               %        24.95
    SM Active Cycles              cycle      4394.12
    Compute (SM) Throughput           %        17.39
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.60
    Achieved Active Warps Per SM           warp        34.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.4%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24040
    Total DRAM Elapsed Cycles        cycle       317440
    Average L1 Active Cycles         cycle      4394.12
    Total L1 Elapsed Cycles          cycle       376786
    Average L2 Active Cycles         cycle      4103.62
    Total L2 Elapsed Cycles          cycle       167856
    Average SM Active Cycles         cycle      4394.12
    Total SM Elapsed Cycles          cycle       376786
    Average SMSP Active Cycles       cycle      4446.57
    Total SMSP Elapsed Cycles        cycle      1507144
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.45
    Elapsed Cycles                cycle         7121
    Memory Throughput                 %        49.44
    DRAM Throughput                   %        49.44
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.79
    L2 Cache Throughput               %        27.92
    SM Active Cycles              cycle      4974.45
    Compute (SM) Throughput           %        15.91
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.20
    Achieved Active Warps Per SM           warp        35.14
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.8%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27760
    Total DRAM Elapsed Cycles        cycle       336896
    Average L1 Active Cycles         cycle      4974.45
    Total L1 Elapsed Cycles          cycle       412036
    Average L2 Active Cycles         cycle         4474
    Total L2 Elapsed Cycles          cycle       177384
    Average SM Active Cycles         cycle      4974.45
    Total SM Elapsed Cycles          cycle       412036
    Average SMSP Active Cycles       cycle      4884.62
    Total SMSP Elapsed Cycles        cycle      1648144
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.61
    Elapsed Cycles                cycle         7179
    Memory Throughput                 %        49.41
    DRAM Throughput                   %        49.41
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.66
    L2 Cache Throughput               %        27.70
    SM Active Cycles              cycle      4876.78
    Compute (SM) Throughput           %        15.82
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.17
    Achieved Active Warps Per SM           warp        35.60
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.83%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27826.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4876.78
    Total L1 Elapsed Cycles          cycle       414372
    Average L2 Active Cycles         cycle      4505.50
    Total L2 Elapsed Cycles          cycle       178824
    Average SM Active Cycles         cycle      4876.78
    Total SM Elapsed Cycles          cycle       414372
    Average SMSP Active Cycles       cycle      4943.09
    Total SMSP Elapsed Cycles        cycle      1657488
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.50
    Elapsed Cycles                cycle         7179
    Memory Throughput                 %        49.28
    DRAM Throughput                   %        49.28
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.93
    L2 Cache Throughput               %        27.67
    SM Active Cycles              cycle      4964.66
    Compute (SM) Throughput           %        15.99
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.58
    Achieved Active Warps Per SM           warp        35.32
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.42%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27840
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      4964.66
    Total L1 Elapsed Cycles          cycle       409906
    Average L2 Active Cycles         cycle      4574.88
    Total L2 Elapsed Cycles          cycle       178824
    Average SM Active Cycles         cycle      4964.66
    Total SM Elapsed Cycles          cycle       409906
    Average SMSP Active Cycles       cycle      5005.35
    Total SMSP Elapsed Cycles        cycle      1639624
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4640225
    Memory Throughput                 %        48.97
    DRAM Throughput                   %         0.22
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3592164.03
    Compute (SM) Throughput           %        45.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     79898.67
    Total DRAM Elapsed Cycles        cycle    218694656
    Average L1 Active Cycles         cycle   3592164.03
    Total L1 Elapsed Cycles          cycle    269099158
    Average L2 Active Cycles         cycle   1220685.21
    Total L2 Elapsed Cycles          cycle    115567536
    Average SM Active Cycles         cycle   3592164.03
    Total SM Elapsed Cycles          cycle    269099158
    Average SMSP Active Cycles       cycle   3591678.57
    Total SMSP Elapsed Cycles        cycle   1076396632
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.53%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.64% above the average, while the minimum instance value is 7.91% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.47%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.57% above the average, while the minimum instance value is 7.98% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.53%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.64% above the average, while the minimum instance value is 7.91% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       784.65
    Elapsed Cycles                cycle         6612
    Memory Throughput                 %        46.10
    DRAM Throughput                   %        46.10
    Duration                         us         8.42
    L1/TEX Cache Throughput           %        25.41
    L2 Cache Throughput               %        25.48
    SM Active Cycles              cycle      4445.98
    Compute (SM) Throughput           %        17.08
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.91
    Achieved Active Warps Per SM           warp        34.52
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.09%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23837.33
    Total DRAM Elapsed Cycles        cycle       310272
    Average L1 Active Cycles         cycle      4445.98
    Total L1 Elapsed Cycles          cycle       383632
    Average L2 Active Cycles         cycle      4056.79
    Total L2 Elapsed Cycles          cycle       164712
    Average SM Active Cycles         cycle      4445.98
    Total SM Elapsed Cycles          cycle       383632
    Average SMSP Active Cycles       cycle      4405.98
    Total SMSP Elapsed Cycles        cycle      1534528
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       782.03
    Elapsed Cycles                cycle         7239
    Memory Throughput                 %        48.72
    DRAM Throughput                   %        48.72
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.65
    L2 Cache Throughput               %        27.45
    SM Active Cycles              cycle      5008.26
    Compute (SM) Throughput           %        15.80
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.20
    Achieved Active Warps Per SM           warp        34.66
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.8%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27853.33
    Total DRAM Elapsed Cycles        cycle       343040
    Average L1 Active Cycles         cycle      5008.26
    Total L1 Elapsed Cycles          cycle       414670
    Average L2 Active Cycles         cycle      4547.17
    Total L2 Elapsed Cycles          cycle       180336
    Average SM Active Cycles         cycle      5008.26
    Total SM Elapsed Cycles          cycle       414670
    Average SMSP Active Cycles       cycle      4938.87
    Total SMSP Elapsed Cycles        cycle      1658680
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.24
    Elapsed Cycles                cycle         7313
    Memory Throughput                 %        48.58
    DRAM Throughput                   %        48.58
    Duration                         us         9.31
    L1/TEX Cache Throughput           %        24.33
    L2 Cache Throughput               %        27.19
    SM Active Cycles              cycle      4895.03
    Compute (SM) Throughput           %        16.26
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.17
    Achieved Active Warps Per SM           warp        35.60
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.83%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27856
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      4895.03
    Total L1 Elapsed Cycles          cycle       403096
    Average L2 Active Cycles         cycle      4453.04
    Total L2 Elapsed Cycles          cycle       182040
    Average SM Active Cycles         cycle      4895.03
    Total SM Elapsed Cycles          cycle       403096
    Average SMSP Active Cycles       cycle      4834.41
    Total SMSP Elapsed Cycles        cycle      1612384
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.62
    Elapsed Cycles                cycle         7153
    Memory Throughput                 %        49.32
    DRAM Throughput                   %        49.32
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.69
    L2 Cache Throughput               %        27.77
    SM Active Cycles              cycle      5003.26
    Compute (SM) Throughput           %        15.82
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.43
    Achieved Active Warps Per SM           warp        34.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.57%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27776
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5003.26
    Total L1 Elapsed Cycles          cycle       414212
    Average L2 Active Cycles         cycle      4452.46
    Total L2 Elapsed Cycles          cycle       178200
    Average SM Active Cycles         cycle      5003.26
    Total SM Elapsed Cycles          cycle       414212
    Average SMSP Active Cycles       cycle      4871.92
    Total SMSP Elapsed Cycles        cycle      1656848
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4638032
    Memory Throughput                 %        48.96
    DRAM Throughput                   %         0.21
    Duration                         ms         5.83
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3591776.48
    Compute (SM) Throughput           %        45.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76506.67
    Total DRAM Elapsed Cycles        cycle    218591232
    Average L1 Active Cycles         cycle   3591776.48
    Total L1 Elapsed Cycles          cycle    269154596
    Average L2 Active Cycles         cycle   1222499.21
    Total L2 Elapsed Cycles          cycle    115513080
    Average SM Active Cycles         cycle   3591776.48
    Total SM Elapsed Cycles          cycle    269154596
    Average SMSP Active Cycles       cycle   3591730.15
    Total SMSP Elapsed Cycles        cycle   1076618384
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.49%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.60% above the average, while the minimum instance value is 7.93% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.48%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.58% above the average, while the minimum instance value is 7.93% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.49%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.60% above the average, while the minimum instance value is 7.93% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.72
    Elapsed Cycles                cycle         6804
    Memory Throughput                 %        44.99
    DRAM Throughput                   %        44.99
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        25.28
    L2 Cache Throughput               %        24.75
    SM Active Cycles              cycle         4469
    Compute (SM) Throughput           %        17.42
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.25
    Achieved Active Warps Per SM           warp        34.68
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.75%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24034.67
    Total DRAM Elapsed Cycles        cycle       320512
    Average L1 Active Cycles         cycle         4469
    Total L1 Elapsed Cycles          cycle       376306
    Average L2 Active Cycles         cycle      4146.96
    Total L2 Elapsed Cycles          cycle       169440
    Average SM Active Cycles         cycle         4469
    Total SM Elapsed Cycles          cycle       376306
    Average SMSP Active Cycles       cycle      4486.08
    Total SMSP Elapsed Cycles        cycle      1505224
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       784.91
    Elapsed Cycles                cycle         7216
    Memory Throughput                 %        48.92
    DRAM Throughput                   %        48.92
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        24.00
    L2 Cache Throughput               %        27.57
    SM Active Cycles              cycle      4995.95
    Compute (SM) Throughput           %        16.04
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.45
    Achieved Active Warps Per SM           warp        35.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.55%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27800
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4995.95
    Total L1 Elapsed Cycles          cycle       408642
    Average L2 Active Cycles         cycle      4480.38
    Total L2 Elapsed Cycles          cycle       179664
    Average SM Active Cycles         cycle      4995.95
    Total SM Elapsed Cycles          cycle       408642
    Average SMSP Active Cycles       cycle      4900.82
    Total SMSP Elapsed Cycles        cycle      1634568
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       782.57
    Elapsed Cycles                cycle         7221
    Memory Throughput                 %        48.93
    DRAM Throughput                   %        48.93
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.50
    L2 Cache Throughput               %        27.52
    SM Active Cycles              cycle      4982.69
    Compute (SM) Throughput           %        15.71
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.21
    Achieved Active Warps Per SM           warp        35.14
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.79%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27810.67
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4982.69
    Total L1 Elapsed Cycles          cycle       417134
    Average L2 Active Cycles         cycle      4554.46
    Total L2 Elapsed Cycles          cycle       179832
    Average SM Active Cycles         cycle      4982.69
    Total SM Elapsed Cycles          cycle       417134
    Average SMSP Active Cycles       cycle      4985.30
    Total SMSP Elapsed Cycles        cycle      1668536
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       785.08
    Elapsed Cycles                cycle         7215
    Memory Throughput                 %        48.97
    DRAM Throughput                   %        48.97
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.58
    L2 Cache Throughput               %        27.60
    SM Active Cycles              cycle      4996.17
    Compute (SM) Throughput           %        15.76
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.15
    Achieved Active Warps Per SM           warp        35.11
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.85%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27832
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4996.17
    Total L1 Elapsed Cycles          cycle       415790
    Average L2 Active Cycles         cycle      4474.62
    Total L2 Elapsed Cycles          cycle       179568
    Average SM Active Cycles         cycle      4996.17
    Total SM Elapsed Cycles          cycle       415790
    Average SMSP Active Cycles       cycle      4872.62
    Total SMSP Elapsed Cycles        cycle      1663160
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4644206
    Memory Throughput                 %        48.94
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.24
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3592700.03
    Compute (SM) Throughput           %        45.42
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76565.33
    Total DRAM Elapsed Cycles        cycle    218882048
    Average L1 Active Cycles         cycle   3592700.03
    Total L1 Elapsed Cycles          cycle    269278844
    Average L2 Active Cycles         cycle   1218169.67
    Total L2 Elapsed Cycles          cycle    115666632
    Average SM Active Cycles         cycle   3592700.03
    Total SM Elapsed Cycles          cycle    269278844
    Average SMSP Active Cycles       cycle   3592161.04
    Total SMSP Elapsed Cycles        cycle   1077115376
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.54%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.66% above the average, while the minimum instance value is 7.93% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.45%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.55% above the average, while the minimum instance value is 7.89% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.54%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.66% above the average, while the minimum instance value is 7.93% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.35
    Elapsed Cycles                cycle         6668
    Memory Throughput                 %        45.56
    DRAM Throughput                   %        45.56
    Duration                         us         8.48
    L1/TEX Cache Throughput           %        25.67
    L2 Cache Throughput               %        25.24
    SM Active Cycles              cycle      4402.07
    Compute (SM) Throughput           %        17.42
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.71
    Achieved Active Warps Per SM           warp        35.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.29%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23794.67
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4402.07
    Total L1 Elapsed Cycles          cycle       376184
    Average L2 Active Cycles         cycle      4013.42
    Total L2 Elapsed Cycles          cycle       166056
    Average SM Active Cycles         cycle      4402.07
    Total SM Elapsed Cycles          cycle       376184
    Average SMSP Active Cycles       cycle      4427.95
    Total SMSP Elapsed Cycles        cycle      1504736
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.11
    Elapsed Cycles                cycle         7173
    Memory Throughput                 %        49.29
    DRAM Throughput                   %        49.29
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.90
    L2 Cache Throughput               %        27.71
    SM Active Cycles              cycle      4931.95
    Compute (SM) Throughput           %        15.97
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.93
    Achieved Active Warps Per SM           warp        35.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.07%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27762.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4931.95
    Total L1 Elapsed Cycles          cycle       410378
    Average L2 Active Cycles         cycle      4459.17
    Total L2 Elapsed Cycles          cycle       178632
    Average SM Active Cycles         cycle      4931.95
    Total SM Elapsed Cycles          cycle       410378
    Average SMSP Active Cycles       cycle      4867.90
    Total SMSP Elapsed Cycles        cycle      1641512
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       785.60
    Elapsed Cycles                cycle         7273
    Memory Throughput                 %        48.70
    DRAM Throughput                   %        48.70
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.78
    L2 Cache Throughput               %        27.35
    SM Active Cycles              cycle      4963.26
    Compute (SM) Throughput           %        15.89
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.77
    Achieved Active Warps Per SM           warp        34.93
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.23%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27845.33
    Total DRAM Elapsed Cycles        cycle       343040
    Average L1 Active Cycles         cycle      4963.26
    Total L1 Elapsed Cycles          cycle       412324
    Average L2 Active Cycles         cycle      4516.58
    Total L2 Elapsed Cycles          cycle       181056
    Average SM Active Cycles         cycle      4963.26
    Total SM Elapsed Cycles          cycle       412324
    Average SMSP Active Cycles       cycle      4923.29
    Total SMSP Elapsed Cycles        cycle      1649296
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.79
    Elapsed Cycles                cycle         7236
    Memory Throughput                 %        48.78
    DRAM Throughput                   %        48.78
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        24.03
    L2 Cache Throughput               %        27.54
    SM Active Cycles              cycle      5033.53
    Compute (SM) Throughput           %        16.05
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.96
    Achieved Active Warps Per SM           warp        34.54
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27722.67
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5033.53
    Total L1 Elapsed Cycles          cycle       408272
    Average L2 Active Cycles         cycle      4534.29
    Total L2 Elapsed Cycles          cycle       179904
    Average SM Active Cycles         cycle      5033.53
    Total SM Elapsed Cycles          cycle       408272
    Average SMSP Active Cycles       cycle      4952.86
    Total SMSP Elapsed Cycles        cycle      1633088
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       801.00
    Elapsed Cycles                cycle      4661256
    Memory Throughput                 %        48.95
    DRAM Throughput                   %         0.21
    Duration                         ms         5.81
    L1/TEX Cache Throughput           %        63.26
    L2 Cache Throughput               %         3.60
    SM Active Cycles              cycle   3591770.95
    Compute (SM) Throughput           %        45.43
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.61
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        76808
    Total DRAM Elapsed Cycles        cycle    217606144
    Average L1 Active Cycles         cycle   3591770.95
    Total L1 Elapsed Cycles          cycle    269207182
    Average L2 Active Cycles         cycle      1206967
    Total L2 Elapsed Cycles          cycle    115007232
    Average SM Active Cycles         cycle   3591770.95
    Total SM Elapsed Cycles          cycle    269207182
    Average SMSP Active Cycles       cycle   3591982.93
    Total SMSP Elapsed Cycles        cycle   1076828728
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.47%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.57% above the average, while the minimum instance value is 7.90% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.5%                                                                                           
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.61% above the average, while the minimum instance value is 7.90% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.47%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.57% above the average, while the minimum instance value is 7.90% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.87
    Elapsed Cycles                cycle         6656
    Memory Throughput                 %        45.78
    DRAM Throughput                   %        45.78
    Duration                         us         8.48
    L1/TEX Cache Throughput           %        25.24
    L2 Cache Throughput               %        25.25
    SM Active Cycles              cycle      4476.86
    Compute (SM) Throughput           %        17.13
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.14
    Achieved Active Warps Per SM           warp        34.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.86%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23984
    Total DRAM Elapsed Cycles        cycle       314368
    Average L1 Active Cycles         cycle      4476.86
    Total L1 Elapsed Cycles          cycle       382638
    Average L2 Active Cycles         cycle      4111.08
    Total L2 Elapsed Cycles          cycle       165792
    Average SM Active Cycles         cycle      4476.86
    Total SM Elapsed Cycles          cycle       382638
    Average SMSP Active Cycles       cycle      4466.48
    Total SMSP Elapsed Cycles        cycle      1530552
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.37
    Elapsed Cycles                cycle         7175
    Memory Throughput                 %        49.29
    DRAM Throughput                   %        49.29
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        24.02
    L2 Cache Throughput               %        27.71
    SM Active Cycles              cycle      5023.60
    Compute (SM) Throughput           %        16.06
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.83
    Achieved Active Warps Per SM           warp        34.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27760
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5023.60
    Total L1 Elapsed Cycles          cycle       408176
    Average L2 Active Cycles         cycle      4511.75
    Total L2 Elapsed Cycles          cycle       178800
    Average SM Active Cycles         cycle      5023.60
    Total SM Elapsed Cycles          cycle       408176
    Average SMSP Active Cycles       cycle      4922.06
    Total SMSP Elapsed Cycles        cycle      1632704
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       783.11
    Elapsed Cycles                cycle         7152
    Memory Throughput                 %        49.70
    DRAM Throughput                   %        49.70
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.80
    L2 Cache Throughput               %        27.82
    SM Active Cycles              cycle      4893.05
    Compute (SM) Throughput           %        15.91
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.21
    Achieved Active Warps Per SM           warp        35.62
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.79%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27818.67
    Total DRAM Elapsed Cycles        cycle       335872
    Average L1 Active Cycles         cycle      4893.05
    Total L1 Elapsed Cycles          cycle       411882
    Average L2 Active Cycles         cycle      4441.88
    Total L2 Elapsed Cycles          cycle       178056
    Average SM Active Cycles         cycle      4893.05
    Total SM Elapsed Cycles          cycle       411882
    Average SMSP Active Cycles       cycle      4870.79
    Total SMSP Elapsed Cycles        cycle      1647528
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.91
    Elapsed Cycles                cycle         7132
    Memory Throughput                 %        49.79
    DRAM Throughput                   %        49.79
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.79
    L2 Cache Throughput               %        27.89
    SM Active Cycles              cycle      4951.24
    Compute (SM) Throughput           %        15.91
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.69
    Achieved Active Warps Per SM           warp        34.89
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.31%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27872
    Total DRAM Elapsed Cycles        cycle       335872
    Average L1 Active Cycles         cycle      4951.24
    Total L1 Elapsed Cycles          cycle       412006
    Average L2 Active Cycles         cycle      4466.67
    Total L2 Elapsed Cycles          cycle       177552
    Average SM Active Cycles         cycle      4951.24
    Total SM Elapsed Cycles          cycle       412006
    Average SMSP Active Cycles       cycle      4879.33
    Total SMSP Elapsed Cycles        cycle      1648024
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4645008
    Memory Throughput                 %        48.95
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.58
    SM Active Cycles              cycle   3592032.05
    Compute (SM) Throughput           %        45.43
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76754.67
    Total DRAM Elapsed Cycles        cycle    218919936
    Average L1 Active Cycles         cycle   3592032.05
    Total L1 Elapsed Cycles          cycle    269214400
    Average L2 Active Cycles         cycle   1228544.67
    Total L2 Elapsed Cycles          cycle    115686888
    Average SM Active Cycles         cycle   3592032.05
    Total SM Elapsed Cycles          cycle    269214400
    Average SMSP Active Cycles       cycle   3591745.47
    Total SMSP Elapsed Cycles        cycle   1076857600
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.53%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.65% above the average, while the minimum instance value is 7.90% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.51%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.63% above the average, while the minimum instance value is 7.89% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.53%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.65% above the average, while the minimum instance value is 7.90% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.88
    Elapsed Cycles                cycle         6505
    Memory Throughput                 %        46.51
    DRAM Throughput                   %        46.51
    Duration                         us         8.29
    L1/TEX Cache Throughput           %        25.19
    L2 Cache Throughput               %        25.89
    SM Active Cycles              cycle      4484.86
    Compute (SM) Throughput           %        16.96
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.82
    Achieved Active Warps Per SM           warp        33.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.18%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23733.33
    Total DRAM Elapsed Cycles        cycle       306176
    Average L1 Active Cycles         cycle      4484.86
    Total L1 Elapsed Cycles          cycle       386472
    Average L2 Active Cycles         cycle      4115.67
    Total L2 Elapsed Cycles          cycle       162000
    Average SM Active Cycles         cycle      4484.86
    Total SM Elapsed Cycles          cycle       386472
    Average SMSP Active Cycles       cycle      4488.56
    Total SMSP Elapsed Cycles        cycle      1545888
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       785.82
    Elapsed Cycles                cycle         7304
    Memory Throughput                 %        48.42
    DRAM Throughput                   %        48.42
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.86
    L2 Cache Throughput               %        27.25
    SM Active Cycles              cycle      5002.02
    Compute (SM) Throughput           %        15.94
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.70
    Achieved Active Warps Per SM           warp        34.90
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.3%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27765.33
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      5002.02
    Total L1 Elapsed Cycles          cycle       411092
    Average L2 Active Cycles         cycle      4446.46
    Total L2 Elapsed Cycles          cycle       181752
    Average SM Active Cycles         cycle      5002.02
    Total SM Elapsed Cycles          cycle       411092
    Average SMSP Active Cycles       cycle      4854.61
    Total SMSP Elapsed Cycles        cycle      1644368
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       782.86
    Elapsed Cycles                cycle         7219
    Memory Throughput                 %        49.01
    DRAM Throughput                   %        49.01
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.65
    L2 Cache Throughput               %        27.53
    SM Active Cycles              cycle      5000.03
    Compute (SM) Throughput           %        15.81
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.03
    Achieved Active Warps Per SM           warp        34.58
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.97%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27853.33
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5000.03
    Total L1 Elapsed Cycles          cycle       414486
    Average L2 Active Cycles         cycle         4483
    Total L2 Elapsed Cycles          cycle       179736
    Average SM Active Cycles         cycle      5000.03
    Total SM Elapsed Cycles          cycle       414486
    Average SMSP Active Cycles       cycle      4868.18
    Total SMSP Elapsed Cycles        cycle      1657944
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       785.36
    Elapsed Cycles                cycle         7273
    Memory Throughput                 %        48.49
    DRAM Throughput                   %        48.49
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.85
    L2 Cache Throughput               %        27.34
    SM Active Cycles              cycle      4996.95
    Compute (SM) Throughput           %        15.94
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.86
    Achieved Active Warps Per SM           warp        34.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.14%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27725.33
    Total DRAM Elapsed Cycles        cycle       343040
    Average L1 Active Cycles         cycle      4996.95
    Total L1 Elapsed Cycles          cycle       411208
    Average L2 Active Cycles         cycle      4516.62
    Total L2 Elapsed Cycles          cycle       181008
    Average SM Active Cycles         cycle      4996.95
    Total SM Elapsed Cycles          cycle       411208
    Average SMSP Active Cycles       cycle      4939.75
    Total SMSP Elapsed Cycles        cycle      1644832
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4640054
    Memory Throughput                 %        48.91
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.24
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3592408.52
    Compute (SM) Throughput           %        45.39
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76845.33
    Total DRAM Elapsed Cycles        cycle    218686464
    Average L1 Active Cycles         cycle   3592408.52
    Total L1 Elapsed Cycles          cycle    269446318
    Average L2 Active Cycles         cycle   1207117.12
    Total L2 Elapsed Cycles          cycle    115563456
    Average SM Active Cycles         cycle   3592408.52
    Total SM Elapsed Cycles          cycle    269446318
    Average SMSP Active Cycles       cycle   3592526.58
    Total SMSP Elapsed Cycles        cycle   1077785272
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.47%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.60% above the average, while the minimum instance value is 7.97% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.44%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.55% above the average, while the minimum instance value is 7.92% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.47%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.60% above the average, while the minimum instance value is 7.97% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       798.03
    Elapsed Cycles                cycle         6822
    Memory Throughput                 %        45.82
    DRAM Throughput                   %        45.82
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        25.77
    L2 Cache Throughput               %        25.27
    SM Active Cycles              cycle      4383.84
    Compute (SM) Throughput           %        17.04
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 24.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.36
    Achieved Active Warps Per SM           warp        36.17
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.64%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24008
    Total DRAM Elapsed Cycles        cycle       314368
    Average L1 Active Cycles         cycle      4383.84
    Total L1 Elapsed Cycles          cycle       384506
    Average L2 Active Cycles         cycle      4069.79
    Total L2 Elapsed Cycles          cycle       165864
    Average SM Active Cycles         cycle      4383.84
    Total SM Elapsed Cycles          cycle       384506
    Average SMSP Active Cycles       cycle      4419.03
    Total SMSP Elapsed Cycles        cycle      1538024
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.33
    Elapsed Cycles                cycle         7523
    Memory Throughput                 %        46.98
    DRAM Throughput                   %        46.98
    Duration                         us         9.60
    L1/TEX Cache Throughput           %        23.70
    L2 Cache Throughput               %        26.50
    SM Active Cycles              cycle      4921.88
    Compute (SM) Throughput           %        15.81
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.01
    Achieved Active Warps Per SM           warp        35.05
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27741.33
    Total DRAM Elapsed Cycles        cycle       354304
    Average L1 Active Cycles         cycle      4921.88
    Total L1 Elapsed Cycles          cycle       414496
    Average L2 Active Cycles         cycle         4478
    Total L2 Elapsed Cycles          cycle       187320
    Average SM Active Cycles         cycle      4921.88
    Total SM Elapsed Cycles          cycle       414496
    Average SMSP Active Cycles       cycle      4894.19
    Total SMSP Elapsed Cycles        cycle      1657984
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.46
    Elapsed Cycles                cycle         7097
    Memory Throughput                 %        49.69
    DRAM Throughput                   %        49.69
    Duration                         us         9.06
    L1/TEX Cache Throughput           %        23.77
    L2 Cache Throughput               %        28.08
    SM Active Cycles              cycle      4932.52
    Compute (SM) Throughput           %        15.86
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.09
    Achieved Active Warps Per SM           warp        35.08
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27730.67
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      4932.52
    Total L1 Elapsed Cycles          cycle       413254
    Average L2 Active Cycles         cycle      4488.33
    Total L2 Elapsed Cycles          cycle       176688
    Average SM Active Cycles         cycle      4932.52
    Total SM Elapsed Cycles          cycle       413254
    Average SMSP Active Cycles       cycle      4930.94
    Total SMSP Elapsed Cycles        cycle      1653016
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.36
    Elapsed Cycles                cycle         7211
    Memory Throughput                 %        48.96
    DRAM Throughput                   %        48.96
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.62
    L2 Cache Throughput               %        27.62
    SM Active Cycles              cycle      5095.76
    Compute (SM) Throughput           %        15.76
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.34
    Achieved Active Warps Per SM           warp        34.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.66%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27741.33
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      5095.76
    Total L1 Elapsed Cycles          cycle       415874
    Average L2 Active Cycles         cycle      4537.58
    Total L2 Elapsed Cycles          cycle       179544
    Average SM Active Cycles         cycle      5095.76
    Total SM Elapsed Cycles          cycle       415874
    Average SMSP Active Cycles       cycle      5010.74
    Total SMSP Elapsed Cycles        cycle      1663496
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4640480
    Memory Throughput                 %        48.93
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.26
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3591651.62
    Compute (SM) Throughput           %        45.42
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        76712
    Total DRAM Elapsed Cycles        cycle    218706944
    Average L1 Active Cycles         cycle   3591651.62
    Total L1 Elapsed Cycles          cycle    269291066
    Average L2 Active Cycles         cycle   1213149.12
    Total L2 Elapsed Cycles          cycle    115573848
    Average SM Active Cycles         cycle   3591651.62
    Total SM Elapsed Cycles          cycle    269291066
    Average SMSP Active Cycles       cycle   3592245.68
    Total SMSP Elapsed Cycles        cycle   1077164264
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.43%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.53% above the average, while the minimum instance value is 7.94% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.45%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.55% above the average, while the minimum instance value is 7.92% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.43%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.53% above the average, while the minimum instance value is 7.94% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.43
    Elapsed Cycles                cycle         6552
    Memory Throughput                 %        46.34
    DRAM Throughput                   %        46.34
    Duration                         us         8.35
    L1/TEX Cache Throughput           %        25.09
    L2 Cache Throughput               %        25.69
    SM Active Cycles              cycle      4503.22
    Compute (SM) Throughput           %        17.32
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 30.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.02
    Achieved Active Warps Per SM           warp        33.61
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23805.33
    Total DRAM Elapsed Cycles        cycle       308224
    Average L1 Active Cycles         cycle      4503.22
    Total L1 Elapsed Cycles          cycle       378450
    Average L2 Active Cycles         cycle      4026.21
    Total L2 Elapsed Cycles          cycle       163104
    Average SM Active Cycles         cycle      4503.22
    Total SM Elapsed Cycles          cycle       378450
    Average SMSP Active Cycles       cycle      4389.67
    Total SMSP Elapsed Cycles        cycle      1513800
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.94
    Elapsed Cycles                cycle         7104
    Memory Throughput                 %        49.85
    DRAM Throughput                   %        49.85
    Duration                         us         9.06
    L1/TEX Cache Throughput           %        23.91
    L2 Cache Throughput               %        28.09
    SM Active Cycles              cycle      5011.60
    Compute (SM) Throughput           %        15.96
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.51
    Achieved Active Warps Per SM           warp        34.80
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.49%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27818.67
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      5011.60
    Total L1 Elapsed Cycles          cycle       410622
    Average L2 Active Cycles         cycle      4393.04
    Total L2 Elapsed Cycles          cycle       176712
    Average SM Active Cycles         cycle      5011.60
    Total SM Elapsed Cycles          cycle       410622
    Average SMSP Active Cycles       cycle      4827.77
    Total SMSP Elapsed Cycles        cycle      1642488
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       793.38
    Elapsed Cycles                cycle         7279
    Memory Throughput                 %        49.24
    DRAM Throughput                   %        49.24
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.93
    L2 Cache Throughput               %        27.75
    SM Active Cycles              cycle      4988.74
    Compute (SM) Throughput           %        15.96
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.40
    Achieved Active Warps Per SM           warp        35.71
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.6%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27733.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4988.74
    Total L1 Elapsed Cycles          cycle       410510
    Average L2 Active Cycles         cycle      4542.12
    Total L2 Elapsed Cycles          cycle       178728
    Average SM Active Cycles         cycle      4988.74
    Total SM Elapsed Cycles          cycle       410510
    Average SMSP Active Cycles       cycle      4987.07
    Total SMSP Elapsed Cycles        cycle      1642040
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       786.20
    Elapsed Cycles                cycle         7024
    Memory Throughput                 %        50.27
    DRAM Throughput                   %        50.27
    Duration                         us         8.93
    L1/TEX Cache Throughput           %        24.09
    L2 Cache Throughput               %        28.40
    SM Active Cycles              cycle      4970.52
    Compute (SM) Throughput           %        16.07
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.37
    Achieved Active Warps Per SM           warp        34.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.63%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27797.33
    Total DRAM Elapsed Cycles        cycle       331776
    Average L1 Active Cycles         cycle      4970.52
    Total L1 Elapsed Cycles          cycle       407810
    Average L2 Active Cycles         cycle      4498.50
    Total L2 Elapsed Cycles          cycle       174720
    Average SM Active Cycles         cycle      4970.52
    Total SM Elapsed Cycles          cycle       407810
    Average SMSP Active Cycles       cycle      4942.53
    Total SMSP Elapsed Cycles        cycle      1631240
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4643319
    Memory Throughput                 %        48.96
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.24
    L2 Cache Throughput               %         3.58
    SM Active Cycles              cycle   3592534.74
    Compute (SM) Throughput           %        45.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76589.33
    Total DRAM Elapsed Cycles        cycle    218840064
    Average L1 Active Cycles         cycle   3592534.74
    Total L1 Elapsed Cycles          cycle    269128896
    Average L2 Active Cycles         cycle   1220009.50
    Total L2 Elapsed Cycles          cycle    115644888
    Average SM Active Cycles         cycle   3592534.74
    Total SM Elapsed Cycles          cycle    269128896
    Average SMSP Active Cycles       cycle   3592257.64
    Total SMSP Elapsed Cycles        cycle   1076515584
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.48%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.57% above the average, while the minimum instance value is 7.88% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.56%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.68% above the average, while the minimum instance value is 7.99% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.48%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.57% above the average, while the minimum instance value is 7.88% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.54
    Elapsed Cycles                cycle         6684
    Memory Throughput                 %        45.80
    DRAM Throughput                   %        45.80
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        25.57
    L2 Cache Throughput               %        25.19
    SM Active Cycles              cycle      4418.76
    Compute (SM) Throughput           %        17.28
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.45
    Achieved Active Warps Per SM           warp        35.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.55%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23997.33
    Total DRAM Elapsed Cycles        cycle       314368
    Average L1 Active Cycles         cycle      4418.76
    Total L1 Elapsed Cycles          cycle       379244
    Average L2 Active Cycles         cycle      4087.92
    Total L2 Elapsed Cycles          cycle       166440
    Average SM Active Cycles         cycle      4418.76
    Total SM Elapsed Cycles          cycle       379244
    Average SMSP Active Cycles       cycle      4449.71
    Total SMSP Elapsed Cycles        cycle      1516976
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.28
    Elapsed Cycles                cycle         7102
    Memory Throughput                 %        49.82
    DRAM Throughput                   %        49.82
    Duration                         us         9.06
    L1/TEX Cache Throughput           %        23.58
    L2 Cache Throughput               %        28.04
    SM Active Cycles              cycle      4987.47
    Compute (SM) Throughput           %        15.73
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.23
    Achieved Active Warps Per SM           warp        34.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.77%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27802.67
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      4987.47
    Total L1 Elapsed Cycles          cycle       416692
    Average L2 Active Cycles         cycle      4520.42
    Total L2 Elapsed Cycles          cycle       176760
    Average SM Active Cycles         cycle      4987.47
    Total SM Elapsed Cycles          cycle       416692
    Average SMSP Active Cycles       cycle      4963.00
    Total SMSP Elapsed Cycles        cycle      1666768
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       782.50
    Elapsed Cycles                cycle         7146
    Memory Throughput                 %        49.40
    DRAM Throughput                   %        49.40
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.83
    L2 Cache Throughput               %        27.86
    SM Active Cycles              cycle      4976.86
    Compute (SM) Throughput           %        15.90
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.53
    Achieved Active Warps Per SM           warp        34.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.47%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27821.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4976.86
    Total L1 Elapsed Cycles          cycle       412168
    Average L2 Active Cycles         cycle      4539.50
    Total L2 Elapsed Cycles          cycle       177864
    Average SM Active Cycles         cycle      4976.86
    Total SM Elapsed Cycles          cycle       412168
    Average SMSP Active Cycles       cycle      4927.40
    Total SMSP Elapsed Cycles        cycle      1648672
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.27
    Elapsed Cycles                cycle         7235
    Memory Throughput                 %        48.70
    DRAM Throughput                   %        48.70
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.54
    L2 Cache Throughput               %        27.50
    SM Active Cycles              cycle      4968.97
    Compute (SM) Throughput           %        15.70
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.66
    Achieved Active Warps Per SM           warp        35.36
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.34%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27677.33
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4968.97
    Total L1 Elapsed Cycles          cycle       417334
    Average L2 Active Cycles         cycle      4487.12
    Total L2 Elapsed Cycles          cycle       180168
    Average SM Active Cycles         cycle      4968.97
    Total SM Elapsed Cycles          cycle       417334
    Average SMSP Active Cycles       cycle      4897.13
    Total SMSP Elapsed Cycles        cycle      1669336
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4644419
    Memory Throughput                 %        48.96
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.58
    SM Active Cycles              cycle   3592107.33
    Compute (SM) Throughput           %        45.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        76688
    Total DRAM Elapsed Cycles        cycle    218892288
    Average L1 Active Cycles         cycle   3592107.33
    Total L1 Elapsed Cycles          cycle    269134616
    Average L2 Active Cycles         cycle   1217280.42
    Total L2 Elapsed Cycles          cycle    115672224
    Average SM Active Cycles         cycle   3592107.33
    Total SM Elapsed Cycles          cycle    269134616
    Average SMSP Active Cycles       cycle   3592626.28
    Total SMSP Elapsed Cycles        cycle   1076538464
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.54%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.65% above the average, while the minimum instance value is 7.90% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.49%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.59% above the average, while the minimum instance value is 7.90% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.54%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.65% above the average, while the minimum instance value is 7.90% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       781.51
    Elapsed Cycles                cycle         6655
    Memory Throughput                 %        45.51
    DRAM Throughput                   %        45.51
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        25.39
    L2 Cache Throughput               %        25.28
    SM Active Cycles              cycle      4450.60
    Compute (SM) Throughput           %        17.11
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.76
    Achieved Active Warps Per SM           warp        34.44
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.24%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23765.33
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4450.60
    Total L1 Elapsed Cycles          cycle       383124
    Average L2 Active Cycles         cycle      4013.33
    Total L2 Elapsed Cycles          cycle       165624
    Average SM Active Cycles         cycle      4450.60
    Total SM Elapsed Cycles          cycle       383124
    Average SMSP Active Cycles       cycle      4355.44
    Total SMSP Elapsed Cycles        cycle      1532496
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       788.68
    Elapsed Cycles                cycle         7235
    Memory Throughput                 %        49.12
    DRAM Throughput                   %        49.12
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        24.15
    L2 Cache Throughput               %        27.65
    SM Active Cycles              cycle      5040.28
    Compute (SM) Throughput           %        16.12
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.09
    Achieved Active Warps Per SM           warp        34.60
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27834.67
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      5040.28
    Total L1 Elapsed Cycles          cycle       406642
    Average L2 Active Cycles         cycle      4380.29
    Total L2 Elapsed Cycles          cycle       179304
    Average SM Active Cycles         cycle      5040.28
    Total SM Elapsed Cycles          cycle       406642
    Average SMSP Active Cycles       cycle      4815.27
    Total SMSP Elapsed Cycles        cycle      1626568
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       781.79
    Elapsed Cycles                cycle         7208
    Memory Throughput                 %        49.24
    DRAM Throughput                   %        49.24
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        24.08
    L2 Cache Throughput               %        27.65
    SM Active Cycles              cycle      5010.33
    Compute (SM) Throughput           %        16.07
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.57
    Achieved Active Warps Per SM           warp        34.83
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.43%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27818.67
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      5010.33
    Total L1 Elapsed Cycles          cycle       407754
    Average L2 Active Cycles         cycle      4464.96
    Total L2 Elapsed Cycles          cycle       179400
    Average SM Active Cycles         cycle      5010.33
    Total SM Elapsed Cycles          cycle       407754
    Average SMSP Active Cycles       cycle      4897.62
    Total SMSP Elapsed Cycles        cycle      1631016
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.74
    Elapsed Cycles                cycle         7168
    Memory Throughput                 %        49.39
    DRAM Throughput                   %        49.39
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.96
    L2 Cache Throughput               %        27.79
    SM Active Cycles              cycle      4989.60
    Compute (SM) Throughput           %        15.99
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.71
    Achieved Active Warps Per SM           warp        35.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.29%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27816
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4989.60
    Total L1 Elapsed Cycles          cycle       409788
    Average L2 Active Cycles         cycle      4529.04
    Total L2 Elapsed Cycles          cycle       178392
    Average SM Active Cycles         cycle      4989.60
    Total SM Elapsed Cycles          cycle       409788
    Average SMSP Active Cycles       cycle         4995
    Total SMSP Elapsed Cycles        cycle      1639152
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       797.09
    Elapsed Cycles                cycle      4645302
    Memory Throughput                 %        48.95
    DRAM Throughput                   %         0.21
    Duration                         ms         5.82
    L1/TEX Cache Throughput           %        63.26
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3591620.16
    Compute (SM) Throughput           %        45.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77885.33
    Total DRAM Elapsed Cycles        cycle    218207232
    Average L1 Active Cycles         cycle   3591620.16
    Total L1 Elapsed Cycles          cycle    269177002
    Average L2 Active Cycles         cycle   1217571.75
    Total L2 Elapsed Cycles          cycle    115327056
    Average SM Active Cycles         cycle   3591620.16
    Total SM Elapsed Cycles          cycle    269177002
    Average SMSP Active Cycles       cycle   3591593.20
    Total SMSP Elapsed Cycles        cycle   1076708008
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.47%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.57% above the average, while the minimum instance value is 7.91% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.43%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.52% above the average, while the minimum instance value is 7.93% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.47%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.57% above the average, while the minimum instance value is 7.91% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.88
    Elapsed Cycles                cycle         6718
    Memory Throughput                 %        45.55
    DRAM Throughput                   %        45.55
    Duration                         us         8.58
    L1/TEX Cache Throughput           %        25.20
    L2 Cache Throughput               %        25.10
    SM Active Cycles              cycle      4484.47
    Compute (SM) Throughput           %        17.05
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.03
    Achieved Active Warps Per SM           warp        35.05
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.97%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24021.33
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4484.47
    Total L1 Elapsed Cycles          cycle       384488
    Average L2 Active Cycles         cycle      4098.38
    Total L2 Elapsed Cycles          cycle       167160
    Average SM Active Cycles         cycle      4484.47
    Total SM Elapsed Cycles          cycle       384488
    Average SMSP Active Cycles       cycle      4507.20
    Total SMSP Elapsed Cycles        cycle      1537952
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.47
    Elapsed Cycles                cycle         7209
    Memory Throughput                 %        48.98
    DRAM Throughput                   %        48.98
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        24.24
    L2 Cache Throughput               %        27.66
    SM Active Cycles              cycle      5012.53
    Compute (SM) Throughput           %        16.17
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.62
    Achieved Active Warps Per SM           warp        34.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.38%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27752
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      5012.53
    Total L1 Elapsed Cycles          cycle       405266
    Average L2 Active Cycles         cycle      4382.50
    Total L2 Elapsed Cycles          cycle       179400
    Average SM Active Cycles         cycle      5012.53
    Total SM Elapsed Cycles          cycle       405266
    Average SMSP Active Cycles       cycle      4805.05
    Total SMSP Elapsed Cycles        cycle      1621064
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       786.57
    Elapsed Cycles                cycle         7308
    Memory Throughput                 %        48.40
    DRAM Throughput                   %        48.40
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.53
    L2 Cache Throughput               %        27.32
    SM Active Cycles              cycle      4915.26
    Compute (SM) Throughput           %        15.70
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.86
    Achieved Active Warps Per SM           warp        35.93
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.14%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27757.33
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      4915.26
    Total L1 Elapsed Cycles          cycle       417412
    Average L2 Active Cycles         cycle      4523.25
    Total L2 Elapsed Cycles          cycle       181560
    Average SM Active Cycles         cycle      4915.26
    Total SM Elapsed Cycles          cycle       417412
    Average SMSP Active Cycles       cycle      4985.89
    Total SMSP Elapsed Cycles        cycle      1669648
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       785.56
    Elapsed Cycles                cycle         7194
    Memory Throughput                 %        48.97
    DRAM Throughput                   %        48.97
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.79
    L2 Cache Throughput               %        27.72
    SM Active Cycles              cycle      4887.59
    Compute (SM) Throughput           %        15.87
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.02
    Achieved Active Warps Per SM           warp        35.53
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27746.67
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      4887.59
    Total L1 Elapsed Cycles          cycle       412884
    Average L2 Active Cycles         cycle      4511.12
    Total L2 Elapsed Cycles          cycle       178944
    Average SM Active Cycles         cycle      4887.59
    Total SM Elapsed Cycles          cycle       412884
    Average SMSP Active Cycles       cycle      4941.53
    Total SMSP Elapsed Cycles        cycle      1651536
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       801.32
    Elapsed Cycles                cycle      4657450
    Memory Throughput                 %        48.96
    DRAM Throughput                   %         0.21
    Duration                         ms         5.80
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.61
    SM Active Cycles              cycle   3592306.53
    Compute (SM) Throughput           %        45.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77530.67
    Total DRAM Elapsed Cycles        cycle    217322496
    Average L1 Active Cycles         cycle   3592306.53
    Total L1 Elapsed Cycles          cycle    269141344
    Average L2 Active Cycles         cycle   1233483.29
    Total L2 Elapsed Cycles          cycle    114872736
    Average SM Active Cycles         cycle   3592306.53
    Total SM Elapsed Cycles          cycle    269141344
    Average SMSP Active Cycles       cycle   3592701.85
    Total SMSP Elapsed Cycles        cycle   1076565376
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.48%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.58% above the average, while the minimum instance value is 7.91% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.46%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.55% above the average, while the minimum instance value is 7.84% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.48%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.58% above the average, while the minimum instance value is 7.91% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.76
    Elapsed Cycles                cycle         6499
    Memory Throughput                 %        46.58
    DRAM Throughput                   %        46.58
    Duration                         us         8.29
    L1/TEX Cache Throughput           %        25.28
    L2 Cache Throughput               %        25.95
    SM Active Cycles              cycle      4468.88
    Compute (SM) Throughput           %        17.26
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.62
    Achieved Active Warps Per SM           warp        33.90
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.38%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23770.67
    Total DRAM Elapsed Cycles        cycle       306176
    Average L1 Active Cycles         cycle      4468.88
    Total L1 Elapsed Cycles          cycle       379608
    Average L2 Active Cycles         cycle      3941.79
    Total L2 Elapsed Cycles          cycle       161664
    Average SM Active Cycles         cycle      4468.88
    Total SM Elapsed Cycles          cycle       379608
    Average SMSP Active Cycles       cycle      4310.00
    Total SMSP Elapsed Cycles        cycle      1518432
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       785.51
    Elapsed Cycles                cycle         7094
    Memory Throughput                 %        49.82
    DRAM Throughput                   %        49.82
    Duration                         us         9.02
    L1/TEX Cache Throughput           %        23.92
    L2 Cache Throughput               %        28.12
    SM Active Cycles              cycle      5023.53
    Compute (SM) Throughput           %        15.96
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.36
    Achieved Active Warps Per SM           warp        34.73
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.64%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27805.33
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      5023.53
    Total L1 Elapsed Cycles          cycle       410742
    Average L2 Active Cycles         cycle      4441.75
    Total L2 Elapsed Cycles          cycle       176400
    Average SM Active Cycles         cycle      5023.53
    Total SM Elapsed Cycles          cycle       410742
    Average SMSP Active Cycles       cycle      4962.68
    Total SMSP Elapsed Cycles        cycle      1642968
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       795.22
    Elapsed Cycles                cycle         7242
    Memory Throughput                 %        49.83
    DRAM Throughput                   %        49.83
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.34
    L2 Cache Throughput               %        28.00
    SM Active Cycles              cycle      4976.03
    Compute (SM) Throughput           %        15.58
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.55
    Achieved Active Warps Per SM           warp        35.78
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.45%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27810.67
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      4976.03
    Total L1 Elapsed Cycles          cycle       420686
    Average L2 Active Cycles         cycle      4466.50
    Total L2 Elapsed Cycles          cycle       177216
    Average SM Active Cycles         cycle      4976.03
    Total SM Elapsed Cycles          cycle       420686
    Average SMSP Active Cycles       cycle      4901.96
    Total SMSP Elapsed Cycles        cycle      1682744
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       782.50
    Elapsed Cycles                cycle         7138
    Memory Throughput                 %        49.37
    DRAM Throughput                   %        49.37
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        24.02
    L2 Cache Throughput               %        27.93
    SM Active Cycles              cycle      5016.28
    Compute (SM) Throughput           %        16.03
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.42
    Achieved Active Warps Per SM           warp        34.76
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.58%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27802.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5016.28
    Total L1 Elapsed Cycles          cycle       408908
    Average L2 Active Cycles         cycle      4496.33
    Total L2 Elapsed Cycles          cycle       177552
    Average SM Active Cycles         cycle      5016.28
    Total SM Elapsed Cycles          cycle       408908
    Average SMSP Active Cycles       cycle      4939.12
    Total SMSP Elapsed Cycles        cycle      1635632
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4646083
    Memory Throughput                 %        48.98
    DRAM Throughput                   %         0.22
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.58
    SM Active Cycles              cycle   3591834.21
    Compute (SM) Throughput           %        45.46
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        80160
    Total DRAM Elapsed Cycles        cycle    218971136
    Average L1 Active Cycles         cycle   3591834.21
    Total L1 Elapsed Cycles          cycle    269026852
    Average L2 Active Cycles         cycle   1219018.33
    Total L2 Elapsed Cycles          cycle    115713672
    Average SM Active Cycles         cycle   3591834.21
    Total SM Elapsed Cycles          cycle    269026852
    Average SMSP Active Cycles       cycle   3591236.30
    Total SMSP Elapsed Cycles        cycle   1076107408
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.46%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.54% above the average, while the minimum instance value is 7.91% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.48%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.57% above the average, while the minimum instance value is 7.90% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.46%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.54% above the average, while the minimum instance value is 7.91% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.25
    Elapsed Cycles                cycle         6649
    Memory Throughput                 %        45.98
    DRAM Throughput                   %        45.98
    Duration                         us         8.48
    L1/TEX Cache Throughput           %        25.43
    L2 Cache Throughput               %        25.31
    SM Active Cycles              cycle      4443.60
    Compute (SM) Throughput           %        17.36
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.45
    Achieved Active Warps Per SM           warp        34.78
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.55%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24013.33
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4443.60
    Total L1 Elapsed Cycles          cycle       377566
    Average L2 Active Cycles         cycle      3975.96
    Total L2 Elapsed Cycles          cycle       165576
    Average SM Active Cycles         cycle      4443.60
    Total SM Elapsed Cycles          cycle       377566
    Average SMSP Active Cycles       cycle      4353.16
    Total SMSP Elapsed Cycles        cycle      1510264
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.22
    Elapsed Cycles                cycle         7227
    Memory Throughput                 %        48.79
    DRAM Throughput                   %        48.79
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.90
    L2 Cache Throughput               %        27.56
    SM Active Cycles              cycle      4996.76
    Compute (SM) Throughput           %        15.95
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.74
    Achieved Active Warps Per SM           warp        34.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.26%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27728
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4996.76
    Total L1 Elapsed Cycles          cycle       410836
    Average L2 Active Cycles         cycle      4508.92
    Total L2 Elapsed Cycles          cycle       179952
    Average SM Active Cycles         cycle      4996.76
    Total SM Elapsed Cycles          cycle       410836
    Average SMSP Active Cycles       cycle      4895.15
    Total SMSP Elapsed Cycles        cycle      1643344
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       784.37
    Elapsed Cycles                cycle         7138
    Memory Throughput                 %        49.85
    DRAM Throughput                   %        49.85
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.82
    L2 Cache Throughput               %        27.91
    SM Active Cycles              cycle      5006.69
    Compute (SM) Throughput           %        15.90
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.24
    Achieved Active Warps Per SM           warp        34.20
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.76%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27818.67
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      5006.69
    Total L1 Elapsed Cycles          cycle       412136
    Average L2 Active Cycles         cycle      4378.33
    Total L2 Elapsed Cycles          cycle       177720
    Average SM Active Cycles         cycle      5006.69
    Total SM Elapsed Cycles          cycle       412136
    Average SMSP Active Cycles       cycle      4854.08
    Total SMSP Elapsed Cycles        cycle      1648544
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       790.14
    Elapsed Cycles                cycle         7197
    Memory Throughput                 %        49.56
    DRAM Throughput                   %        49.56
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.87
    L2 Cache Throughput               %        27.98
    SM Active Cycles              cycle      5061.53
    Compute (SM) Throughput           %        15.93
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.20
    Achieved Active Warps Per SM           warp        34.65
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.8%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27744
    Total DRAM Elapsed Cycles        cycle       335872
    Average L1 Active Cycles         cycle      5061.53
    Total L1 Elapsed Cycles          cycle       411494
    Average L2 Active Cycles         cycle      4491.88
    Total L2 Elapsed Cycles          cycle       177384
    Average SM Active Cycles         cycle      5061.53
    Total SM Elapsed Cycles          cycle       411494
    Average SMSP Active Cycles       cycle      4914.35
    Total SMSP Elapsed Cycles        cycle      1645976
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.70
    Elapsed Cycles                cycle      4643939
    Memory Throughput                 %        48.96
    DRAM Throughput                   %         0.22
    Duration                         ms         5.83
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3592214.31
    Compute (SM) Throughput           %        45.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     79309.33
    Total DRAM Elapsed Cycles        cycle    218612736
    Average L1 Active Cycles         cycle   3592214.31
    Total L1 Elapsed Cycles          cycle    269138052
    Average L2 Active Cycles         cycle   1221411.92
    Total L2 Elapsed Cycles          cycle    115526472
    Average SM Active Cycles         cycle   3592214.31
    Total SM Elapsed Cycles          cycle    269138052
    Average SMSP Active Cycles       cycle   3591873.01
    Total SMSP Elapsed Cycles        cycle   1076552208
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.49%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.60% above the average, while the minimum instance value is 7.95% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.51%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.62% above the average, while the minimum instance value is 7.98% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.49%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.60% above the average, while the minimum instance value is 7.95% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       781.06
    Elapsed Cycles                cycle         6626
    Memory Throughput                 %        45.49
    DRAM Throughput                   %        45.49
    Duration                         us         8.48
    L1/TEX Cache Throughput           %        25.70
    L2 Cache Throughput               %        25.44
    SM Active Cycles              cycle      4397.05
    Compute (SM) Throughput           %        17.28
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.03
    Achieved Active Warps Per SM           warp        35.05
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.97%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23754.67
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4397.05
    Total L1 Elapsed Cycles          cycle       379360
    Average L2 Active Cycles         cycle      3892.88
    Total L2 Elapsed Cycles          cycle       164736
    Average SM Active Cycles         cycle      4397.05
    Total SM Elapsed Cycles          cycle       379360
    Average SMSP Active Cycles       cycle      4253.90
    Total SMSP Elapsed Cycles        cycle      1517440
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       782.23
    Elapsed Cycles                cycle         7064
    Memory Throughput                 %        50.18
    DRAM Throughput                   %        50.18
    Duration                         us         9.02
    L1/TEX Cache Throughput           %        24.11
    L2 Cache Throughput               %        28.24
    SM Active Cycles              cycle      4996.29
    Compute (SM) Throughput           %        16.10
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.59
    Achieved Active Warps Per SM           warp        34.36
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.41%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27749.33
    Total DRAM Elapsed Cycles        cycle       331776
    Average L1 Active Cycles         cycle      4996.29
    Total L1 Elapsed Cycles          cycle       407134
    Average L2 Active Cycles         cycle      4465.58
    Total L2 Elapsed Cycles          cycle       175680
    Average SM Active Cycles         cycle      4996.29
    Total SM Elapsed Cycles          cycle       407134
    Average SMSP Active Cycles       cycle      4881.55
    Total SMSP Elapsed Cycles        cycle      1628536
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.78
    Elapsed Cycles                cycle         7213
    Memory Throughput                 %        49.13
    DRAM Throughput                   %        49.13
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        24.08
    L2 Cache Throughput               %        27.62
    SM Active Cycles              cycle      4981.59
    Compute (SM) Throughput           %        16.07
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.56
    Achieved Active Warps Per SM           warp        34.83
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.44%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27752
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      4981.59
    Total L1 Elapsed Cycles          cycle       407704
    Average L2 Active Cycles         cycle      4472.92
    Total L2 Elapsed Cycles          cycle       179688
    Average SM Active Cycles         cycle      4981.59
    Total SM Elapsed Cycles          cycle       407704
    Average SMSP Active Cycles       cycle      4917.81
    Total SMSP Elapsed Cycles        cycle      1630816
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.97
    Elapsed Cycles                cycle         7210
    Memory Throughput                 %        49.23
    DRAM Throughput                   %        49.23
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.96
    L2 Cache Throughput               %        27.65
    SM Active Cycles              cycle      4992.14
    Compute (SM) Throughput           %        15.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.57
    Achieved Active Warps Per SM           warp        35.31
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.43%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27808
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      4992.14
    Total L1 Elapsed Cycles          cycle       410114
    Average L2 Active Cycles         cycle      4398.79
    Total L2 Elapsed Cycles          cycle       179352
    Average SM Active Cycles         cycle      4992.14
    Total SM Elapsed Cycles          cycle       410114
    Average SMSP Active Cycles       cycle      4830.02
    Total SMSP Elapsed Cycles        cycle      1640456
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4646130
    Memory Throughput                 %        48.97
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.26
    L2 Cache Throughput               %         3.58
    SM Active Cycles              cycle   3591728.07
    Compute (SM) Throughput           %        45.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76762.67
    Total DRAM Elapsed Cycles        cycle    218972160
    Average L1 Active Cycles         cycle   3591728.07
    Total L1 Elapsed Cycles          cycle    269087092
    Average L2 Active Cycles         cycle   1218208.08
    Total L2 Elapsed Cycles          cycle    115714632
    Average SM Active Cycles         cycle   3591728.07
    Total SM Elapsed Cycles          cycle    269087092
    Average SMSP Active Cycles       cycle   3591698.28
    Total SMSP Elapsed Cycles        cycle   1076348368
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.55%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.67% above the average, while the minimum instance value is 7.87% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.46%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.55% above the average, while the minimum instance value is 8.02% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.55%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.67% above the average, while the minimum instance value is 7.87% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       781.46
    Elapsed Cycles                cycle         6784
    Memory Throughput                 %        44.76
    DRAM Throughput                   %        44.76
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        25.05
    L2 Cache Throughput               %        24.81
    SM Active Cycles              cycle      4510.22
    Compute (SM) Throughput           %        17.18
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.02
    Achieved Active Warps Per SM           warp        35.05
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23984
    Total DRAM Elapsed Cycles        cycle       321536
    Average L1 Active Cycles         cycle      4510.22
    Total L1 Elapsed Cycles          cycle       381490
    Average L2 Active Cycles         cycle      4092.12
    Total L2 Elapsed Cycles          cycle       168960
    Average SM Active Cycles         cycle      4510.22
    Total SM Elapsed Cycles          cycle       381490
    Average SMSP Active Cycles       cycle      4432.56
    Total SMSP Elapsed Cycles        cycle      1525960
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.49
    Elapsed Cycles                cycle         7265
    Memory Throughput                 %        48.78
    DRAM Throughput                   %        48.78
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        24.14
    L2 Cache Throughput               %        27.43
    SM Active Cycles              cycle      4985.93
    Compute (SM) Throughput           %        16.09
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.57
    Achieved Active Warps Per SM           warp        35.31
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.43%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27808
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle      4985.93
    Total L1 Elapsed Cycles          cycle       407230
    Average L2 Active Cycles         cycle      4530.92
    Total L2 Elapsed Cycles          cycle       180792
    Average SM Active Cycles         cycle      4985.93
    Total SM Elapsed Cycles          cycle       407230
    Average SMSP Active Cycles       cycle      4959.44
    Total SMSP Elapsed Cycles        cycle      1628920
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.42
    Elapsed Cycles                cycle         7249
    Memory Throughput                 %        48.83
    DRAM Throughput                   %        48.83
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.75
    L2 Cache Throughput               %        27.49
    SM Active Cycles              cycle      5004.28
    Compute (SM) Throughput           %        15.84
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.72
    Achieved Active Warps Per SM           warp        34.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.28%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27749.33
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5004.28
    Total L1 Elapsed Cycles          cycle       413714
    Average L2 Active Cycles         cycle      4522.96
    Total L2 Elapsed Cycles          cycle       180432
    Average SM Active Cycles         cycle      5004.28
    Total SM Elapsed Cycles          cycle       413714
    Average SMSP Active Cycles       cycle      4918.84
    Total SMSP Elapsed Cycles        cycle      1654856
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.49
    Elapsed Cycles                cycle         7201
    Memory Throughput                 %        49.02
    DRAM Throughput                   %        49.02
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.78
    L2 Cache Throughput               %        27.67
    SM Active Cycles              cycle      4868.69
    Compute (SM) Throughput           %        15.87
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.58
    Achieved Active Warps Per SM           warp        35.80
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.42%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27773.33
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      4868.69
    Total L1 Elapsed Cycles          cycle       412948
    Average L2 Active Cycles         cycle      4447.42
    Total L2 Elapsed Cycles          cycle       179232
    Average SM Active Cycles         cycle      4868.69
    Total SM Elapsed Cycles          cycle       412948
    Average SMSP Active Cycles       cycle      4896.74
    Total SMSP Elapsed Cycles        cycle      1651792
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4643976
    Memory Throughput                 %        48.98
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.58
    SM Active Cycles              cycle   3592092.52
    Compute (SM) Throughput           %        45.46
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        76544
    Total DRAM Elapsed Cycles        cycle    218870784
    Average L1 Active Cycles         cycle   3592092.52
    Total L1 Elapsed Cycles          cycle    269032556
    Average L2 Active Cycles         cycle   1210647.67
    Total L2 Elapsed Cycles          cycle    115661040
    Average SM Active Cycles         cycle   3592092.52
    Total SM Elapsed Cycles          cycle    269032556
    Average SMSP Active Cycles       cycle   3592031.59
    Total SMSP Elapsed Cycles        cycle   1076130224
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.51%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.61% above the average, while the minimum instance value is 7.98% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.51%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.61% above the average, while the minimum instance value is 8.03% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.51%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.61% above the average, while the minimum instance value is 7.98% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       781.87
    Elapsed Cycles                cycle         6612
    Memory Throughput                 %        45.67
    DRAM Throughput                   %        45.67
    Duration                         us         8.45
    L1/TEX Cache Throughput           %        25.98
    L2 Cache Throughput               %        25.44
    SM Active Cycles              cycle      4348.57
    Compute (SM) Throughput           %        17.09
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.18
    Achieved Active Warps Per SM           warp        35.13
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.82%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23773.33
    Total DRAM Elapsed Cycles        cycle       312320
    Average L1 Active Cycles         cycle      4348.57
    Total L1 Elapsed Cycles          cycle       383388
    Average L2 Active Cycles         cycle      4076.21
    Total L2 Elapsed Cycles          cycle       164616
    Average SM Active Cycles         cycle      4348.57
    Total SM Elapsed Cycles          cycle       383388
    Average SMSP Active Cycles       cycle      4416.50
    Total SMSP Elapsed Cycles        cycle      1533552
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.72
    Elapsed Cycles                cycle         7256
    Memory Throughput                 %        48.57
    DRAM Throughput                   %        48.57
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.84
    L2 Cache Throughput               %        27.46
    SM Active Cycles              cycle      5014.66
    Compute (SM) Throughput           %        15.91
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.78
    Achieved Active Warps Per SM           warp        34.93
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.22%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27770.67
    Total DRAM Elapsed Cycles        cycle       343040
    Average L1 Active Cycles         cycle      5014.66
    Total L1 Elapsed Cycles          cycle       411882
    Average L2 Active Cycles         cycle      4495.75
    Total L2 Elapsed Cycles          cycle       180720
    Average SM Active Cycles         cycle      5014.66
    Total SM Elapsed Cycles          cycle       411882
    Average SMSP Active Cycles       cycle      4920.93
    Total SMSP Elapsed Cycles        cycle      1647528
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.51
    Elapsed Cycles                cycle         7073
    Memory Throughput                 %        49.84
    DRAM Throughput                   %        49.84
    Duration                         us         9.02
    L1/TEX Cache Throughput           %        24.29
    L2 Cache Throughput               %        28.17
    SM Active Cycles              cycle      5012.88
    Compute (SM) Throughput           %        16.20
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.46
    Achieved Active Warps Per SM           warp        34.78
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.54%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27730.67
    Total DRAM Elapsed Cycles        cycle       333824
    Average L1 Active Cycles         cycle      5012.88
    Total L1 Elapsed Cycles          cycle       404494
    Average L2 Active Cycles         cycle      4500.54
    Total L2 Elapsed Cycles          cycle       176064
    Average SM Active Cycles         cycle      5012.88
    Total SM Elapsed Cycles          cycle       404494
    Average SMSP Active Cycles       cycle      5030.18
    Total SMSP Elapsed Cycles        cycle      1617976
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.01
    Elapsed Cycles                cycle         7145
    Memory Throughput                 %        49.62
    DRAM Throughput                   %        49.62
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        24.45
    L2 Cache Throughput               %        27.94
    SM Active Cycles              cycle      4974.22
    Compute (SM) Throughput           %        16.32
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.79
    Achieved Active Warps Per SM           warp        35.42
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.21%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27776
    Total DRAM Elapsed Cycles        cycle       335872
    Average L1 Active Cycles         cycle      4974.22
    Total L1 Elapsed Cycles          cycle       401626
    Average L2 Active Cycles         cycle      4496.75
    Total L2 Elapsed Cycles          cycle       177576
    Average SM Active Cycles         cycle      4974.22
    Total SM Elapsed Cycles          cycle       401626
    Average SMSP Active Cycles       cycle      4923.03
    Total SMSP Elapsed Cycles        cycle      1606504
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       797.38
    Elapsed Cycles                cycle      4650969
    Memory Throughput                 %        48.95
    DRAM Throughput                   %         0.21
    Duration                         ms         5.83
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3591900.72
    Compute (SM) Throughput           %        45.43
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        76680
    Total DRAM Elapsed Cycles        cycle    218348544
    Average L1 Active Cycles         cycle   3591900.72
    Total L1 Elapsed Cycles          cycle    269204302
    Average L2 Active Cycles         cycle   1212674.54
    Total L2 Elapsed Cycles          cycle    115385424
    Average SM Active Cycles         cycle   3591900.72
    Total SM Elapsed Cycles          cycle    269204302
    Average SMSP Active Cycles       cycle   3592869.37
    Total SMSP Elapsed Cycles        cycle   1076817208
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.51%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.63% above the average, while the minimum instance value is 7.93% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.46%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.56% above the average, while the minimum instance value is 7.97% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.51%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.63% above the average, while the minimum instance value is 7.93% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.86
    Elapsed Cycles                cycle         6693
    Memory Throughput                 %        45.76
    DRAM Throughput                   %        45.76
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        25.19
    L2 Cache Throughput               %        25.17
    SM Active Cycles              cycle      4484.84
    Compute (SM) Throughput           %        17.40
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.88
    Achieved Active Warps Per SM           warp        34.50
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.12%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23976
    Total DRAM Elapsed Cycles        cycle       314368
    Average L1 Active Cycles         cycle      4484.84
    Total L1 Elapsed Cycles          cycle       376542
    Average L2 Active Cycles         cycle      4026.04
    Total L2 Elapsed Cycles          cycle       166488
    Average SM Active Cycles         cycle      4484.84
    Total SM Elapsed Cycles          cycle       376542
    Average SMSP Active Cycles       cycle      4400.11
    Total SMSP Elapsed Cycles        cycle      1506168
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       782.37
    Elapsed Cycles                cycle         7137
    Memory Throughput                 %        49.65
    DRAM Throughput                   %        49.65
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.61
    L2 Cache Throughput               %        27.96
    SM Active Cycles              cycle      4990.10
    Compute (SM) Throughput           %        15.75
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.13
    Achieved Active Warps Per SM           warp        34.14
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.87%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27792
    Total DRAM Elapsed Cycles        cycle       335872
    Average L1 Active Cycles         cycle      4990.10
    Total L1 Elapsed Cycles          cycle       416014
    Average L2 Active Cycles         cycle      4400.75
    Total L2 Elapsed Cycles          cycle       177456
    Average SM Active Cycles         cycle      4990.10
    Total SM Elapsed Cycles          cycle       416014
    Average SMSP Active Cycles       cycle      4822.98
    Total SMSP Elapsed Cycles        cycle      1664056
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.85
    Elapsed Cycles                cycle         7118
    Memory Throughput                 %        49.71
    DRAM Throughput                   %        49.71
    Duration                         us         9.06
    L1/TEX Cache Throughput           %        23.88
    L2 Cache Throughput               %        28.07
    SM Active Cycles              cycle      5034.72
    Compute (SM) Throughput           %        15.92
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.72
    Achieved Active Warps Per SM           warp        34.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.28%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27741.33
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      5034.72
    Total L1 Elapsed Cycles          cycle       411530
    Average L2 Active Cycles         cycle      4543.75
    Total L2 Elapsed Cycles          cycle       176664
    Average SM Active Cycles         cycle      5034.72
    Total SM Elapsed Cycles          cycle       411530
    Average SMSP Active Cycles       cycle      4974.95
    Total SMSP Elapsed Cycles        cycle      1646120
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.34
    Elapsed Cycles                cycle         7204
    Memory Throughput                 %        49.04
    DRAM Throughput                   %        49.04
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        24.02
    L2 Cache Throughput               %        27.66
    SM Active Cycles              cycle      4956.07
    Compute (SM) Throughput           %        16.03
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.29
    Achieved Active Warps Per SM           warp        35.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.71%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27784
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      4956.07
    Total L1 Elapsed Cycles          cycle       408934
    Average L2 Active Cycles         cycle      4485.29
    Total L2 Elapsed Cycles          cycle       179208
    Average SM Active Cycles         cycle      4956.07
    Total SM Elapsed Cycles          cycle       408934
    Average SMSP Active Cycles       cycle      4931.01
    Total SMSP Elapsed Cycles        cycle      1635736
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4640053
    Memory Throughput                 %        48.90
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3591940.14
    Compute (SM) Throughput           %        45.39
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76597.33
    Total DRAM Elapsed Cycles        cycle    218686464
    Average L1 Active Cycles         cycle   3591940.14
    Total L1 Elapsed Cycles          cycle    269493650
    Average L2 Active Cycles         cycle   1210958.17
    Total L2 Elapsed Cycles          cycle    115563336
    Average SM Active Cycles         cycle   3591940.14
    Total SM Elapsed Cycles          cycle    269493650
    Average SMSP Active Cycles       cycle   3591941.04
    Total SMSP Elapsed Cycles        cycle   1077974600
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.46%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.59% above the average, while the minimum instance value is 7.91% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.49%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.62% above the average, while the minimum instance value is 7.88% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.46%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.59% above the average, while the minimum instance value is 7.91% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       783.46
    Elapsed Cycles                cycle         6601
    Memory Throughput                 %        45.71
    DRAM Throughput                   %        45.71
    Duration                         us         8.42
    L1/TEX Cache Throughput           %        25.55
    L2 Cache Throughput               %        25.50
    SM Active Cycles              cycle      4423.24
    Compute (SM) Throughput           %        17.32
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.63
    Achieved Active Warps Per SM           warp        34.86
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.37%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23792
    Total DRAM Elapsed Cycles        cycle       312320
    Average L1 Active Cycles         cycle      4423.24
    Total L1 Elapsed Cycles          cycle       378366
    Average L2 Active Cycles         cycle      4000.25
    Total L2 Elapsed Cycles          cycle       164304
    Average SM Active Cycles         cycle      4423.24
    Total SM Elapsed Cycles          cycle       378366
    Average SMSP Active Cycles       cycle      4357.06
    Total SMSP Elapsed Cycles        cycle      1513464
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       786.54
    Elapsed Cycles                cycle         7232
    Memory Throughput                 %        49.30
    DRAM Throughput                   %        49.30
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.82
    L2 Cache Throughput               %        27.66
    SM Active Cycles              cycle      5039.12
    Compute (SM) Throughput           %        15.90
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.65
    Achieved Active Warps Per SM           warp        34.87
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27848
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      5039.12
    Total L1 Elapsed Cycles          cycle       412212
    Average L2 Active Cycles         cycle      4398.12
    Total L2 Elapsed Cycles          cycle       179352
    Average SM Active Cycles         cycle      5039.12
    Total SM Elapsed Cycles          cycle       412212
    Average SMSP Active Cycles       cycle      4905.61
    Total SMSP Elapsed Cycles        cycle      1648848
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       781.97
    Elapsed Cycles                cycle         7186
    Memory Throughput                 %        49.23
    DRAM Throughput                   %        49.23
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        24.39
    L2 Cache Throughput               %        27.75
    SM Active Cycles              cycle      4981.05
    Compute (SM) Throughput           %        16.28
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.53
    Achieved Active Warps Per SM           warp        35.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.47%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27808
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      4981.05
    Total L1 Elapsed Cycles          cycle       402606
    Average L2 Active Cycles         cycle      4512.46
    Total L2 Elapsed Cycles          cycle       178800
    Average SM Active Cycles         cycle      4981.05
    Total SM Elapsed Cycles          cycle       402606
    Average SMSP Active Cycles       cycle      5002.62
    Total SMSP Elapsed Cycles        cycle      1610424
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       781.58
    Elapsed Cycles                cycle         7187
    Memory Throughput                 %        49.40
    DRAM Throughput                   %        49.40
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.87
    L2 Cache Throughput               %        27.72
    SM Active Cycles              cycle      5021.60
    Compute (SM) Throughput           %        15.93
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.62
    Achieved Active Warps Per SM           warp        34.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.38%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27824
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5021.60
    Total L1 Elapsed Cycles          cycle       411468
    Average L2 Active Cycles         cycle      4462.46
    Total L2 Elapsed Cycles          cycle       178920
    Average SM Active Cycles         cycle      5021.60
    Total SM Elapsed Cycles          cycle       411468
    Average SMSP Active Cycles       cycle      4890.95
    Total SMSP Elapsed Cycles        cycle      1645872
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4642677
    Memory Throughput                 %        48.98
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.26
    L2 Cache Throughput               %         3.58
    SM Active Cycles              cycle   3591471.83
    Compute (SM) Throughput           %        45.46
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.61
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76674.67
    Total DRAM Elapsed Cycles        cycle    218809344
    Average L1 Active Cycles         cycle   3591471.83
    Total L1 Elapsed Cycles          cycle    269041724
    Average L2 Active Cycles         cycle   1210430.62
    Total L2 Elapsed Cycles          cycle    115628736
    Average SM Active Cycles         cycle   3591471.83
    Total SM Elapsed Cycles          cycle    269041724
    Average SMSP Active Cycles       cycle   3592686.56
    Total SMSP Elapsed Cycles        cycle   1076166896
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.46%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.55% above the average, while the minimum instance value is 7.90% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.46%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.55% above the average, while the minimum instance value is 7.98% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.46%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.55% above the average, while the minimum instance value is 7.90% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.15
    Elapsed Cycles                cycle         6675
    Memory Throughput                 %        45.65
    DRAM Throughput                   %        45.65
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        24.94
    L2 Cache Throughput               %        25.22
    SM Active Cycles              cycle      4530.41
    Compute (SM) Throughput           %        17.43
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.33
    Achieved Active Warps Per SM           warp        34.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.67%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23997.33
    Total DRAM Elapsed Cycles        cycle       315392
    Average L1 Active Cycles         cycle      4530.41
    Total L1 Elapsed Cycles          cycle       376026
    Average L2 Active Cycles         cycle      4121.17
    Total L2 Elapsed Cycles          cycle       166200
    Average SM Active Cycles         cycle      4530.41
    Total SM Elapsed Cycles          cycle       376026
    Average SMSP Active Cycles       cycle      4479.49
    Total SMSP Elapsed Cycles        cycle      1504104
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.46
    Elapsed Cycles                cycle         7239
    Memory Throughput                 %        48.91
    DRAM Throughput                   %        48.91
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.50
    L2 Cache Throughput               %        27.54
    SM Active Cycles              cycle      5007.98
    Compute (SM) Throughput           %        15.68
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.83
    Achieved Active Warps Per SM           warp        34.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27794.67
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5007.98
    Total L1 Elapsed Cycles          cycle       418042
    Average L2 Active Cycles         cycle      4551.12
    Total L2 Elapsed Cycles          cycle       180120
    Average SM Active Cycles         cycle      5007.98
    Total SM Elapsed Cycles          cycle       418042
    Average SMSP Active Cycles       cycle      4990.29
    Total SMSP Elapsed Cycles        cycle      1672168
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       785.42
    Elapsed Cycles                cycle         7121
    Memory Throughput                 %        49.53
    DRAM Throughput                   %        49.53
    Duration                         us         9.06
    L1/TEX Cache Throughput           %        24.04
    L2 Cache Throughput               %        27.97
    SM Active Cycles              cycle      4998.02
    Compute (SM) Throughput           %        16.04
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.58
    Achieved Active Warps Per SM           warp        34.36
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.42%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27808
    Total DRAM Elapsed Cycles        cycle       336896
    Average L1 Active Cycles         cycle      4998.02
    Total L1 Elapsed Cycles          cycle       408564
    Average L2 Active Cycles         cycle      4474.17
    Total L2 Elapsed Cycles          cycle       177288
    Average SM Active Cycles         cycle      4998.02
    Total SM Elapsed Cycles          cycle       408564
    Average SMSP Active Cycles       cycle      4912.54
    Total SMSP Elapsed Cycles        cycle      1634256
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.00
    Elapsed Cycles                cycle         7216
    Memory Throughput                 %        49.04
    DRAM Throughput                   %        49.04
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.48
    L2 Cache Throughput               %        27.60
    SM Active Cycles              cycle      5003.26
    Compute (SM) Throughput           %        15.66
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.23
    Achieved Active Warps Per SM           warp        34.67
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.77%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27789.33
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      5003.26
    Total L1 Elapsed Cycles          cycle       418548
    Average L2 Active Cycles         cycle      4470.12
    Total L2 Elapsed Cycles          cycle       179712
    Average SM Active Cycles         cycle      5003.26
    Total SM Elapsed Cycles          cycle       418548
    Average SMSP Active Cycles       cycle      4893.91
    Total SMSP Elapsed Cycles        cycle      1674192
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4641601
    Memory Throughput                 %        48.88
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.58
    SM Active Cycles              cycle   3592132.71
    Compute (SM) Throughput           %        45.37
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76709.33
    Total DRAM Elapsed Cycles        cycle    218758144
    Average L1 Active Cycles         cycle   3592132.71
    Total L1 Elapsed Cycles          cycle    269576208
    Average L2 Active Cycles         cycle   1246369.21
    Total L2 Elapsed Cycles          cycle    115602096
    Average SM Active Cycles         cycle   3592132.71
    Total SM Elapsed Cycles          cycle    269576208
    Average SMSP Active Cycles       cycle   3591856.69
    Total SMSP Elapsed Cycles        cycle   1078304832
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.49%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.63% above the average, while the minimum instance value is 7.95% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.44%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.57% above the average, while the minimum instance value is 7.85% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.49%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.63% above the average, while the minimum instance value is 7.95% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.11
    SM Frequency                    Mhz       780.62
    Elapsed Cycles                cycle         6677
    Memory Throughput                 %        45.53
    DRAM Throughput                   %        45.53
    Duration                         us         8.54
    L1/TEX Cache Throughput           %        25.70
    L2 Cache Throughput               %        25.24
    SM Active Cycles              cycle      4395.90
    Compute (SM) Throughput           %        17.31
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.22
    Achieved Active Warps Per SM           warp        35.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.78%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23778.67
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4395.90
    Total L1 Elapsed Cycles          cycle       378686
    Average L2 Active Cycles         cycle      3924.33
    Total L2 Elapsed Cycles          cycle       166200
    Average SM Active Cycles         cycle      4395.90
    Total SM Elapsed Cycles          cycle       378686
    Average SMSP Active Cycles       cycle      4295.37
    Total SMSP Elapsed Cycles        cycle      1514744
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.81
    Elapsed Cycles                cycle         7237
    Memory Throughput                 %        48.98
    DRAM Throughput                   %        48.98
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.82
    L2 Cache Throughput               %        27.51
    SM Active Cycles              cycle      4988.10
    Compute (SM) Throughput           %        15.90
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.79
    Achieved Active Warps Per SM           warp        35.42
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.21%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27837.33
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4988.10
    Total L1 Elapsed Cycles          cycle       412184
    Average L2 Active Cycles         cycle      4515.21
    Total L2 Elapsed Cycles          cycle       180240
    Average SM Active Cycles         cycle      4988.10
    Total SM Elapsed Cycles          cycle       412184
    Average SMSP Active Cycles       cycle      4950.57
    Total SMSP Elapsed Cycles        cycle      1648736
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.27
    Elapsed Cycles                cycle         7153
    Memory Throughput                 %        49.63
    DRAM Throughput                   %        49.63
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.92
    L2 Cache Throughput               %        27.85
    SM Active Cycles              cycle      5065.45
    Compute (SM) Throughput           %        15.96
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.43
    Achieved Active Warps Per SM           warp        34.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.57%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27869.33
    Total DRAM Elapsed Cycles        cycle       336896
    Average L1 Active Cycles         cycle      5065.45
    Total L1 Elapsed Cycles          cycle       410616
    Average L2 Active Cycles         cycle      4485.88
    Total L2 Elapsed Cycles          cycle       178104
    Average SM Active Cycles         cycle      5065.45
    Total SM Elapsed Cycles          cycle       410616
    Average SMSP Active Cycles       cycle      4918.97
    Total SMSP Elapsed Cycles        cycle      1642464
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.39
    Elapsed Cycles                cycle         7176
    Memory Throughput                 %        49.34
    DRAM Throughput                   %        49.34
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        24.45
    L2 Cache Throughput               %        27.77
    SM Active Cycles              cycle         4930
    Compute (SM) Throughput           %        16.31
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.23
    Achieved Active Warps Per SM           warp        35.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.77%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27786.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle         4930
    Total L1 Elapsed Cycles          cycle       401798
    Average L2 Active Cycles         cycle      4504.75
    Total L2 Elapsed Cycles          cycle       178488
    Average SM Active Cycles         cycle         4930
    Total SM Elapsed Cycles          cycle       401798
    Average SMSP Active Cycles       cycle      4932.25
    Total SMSP Elapsed Cycles        cycle      1607192
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4641752
    Memory Throughput                 %        48.96
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.26
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3591521.48
    Compute (SM) Throughput           %        45.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        76456
    Total DRAM Elapsed Cycles        cycle    218766336
    Average L1 Active Cycles         cycle   3591521.48
    Total L1 Elapsed Cycles          cycle    269131062
    Average L2 Active Cycles         cycle   1208259.83
    Total L2 Elapsed Cycles          cycle    115605816
    Average SM Active Cycles         cycle   3591521.48
    Total SM Elapsed Cycles          cycle    269131062
    Average SMSP Active Cycles       cycle   3592325.83
    Total SMSP Elapsed Cycles        cycle   1076524248
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.45%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.54% above the average, while the minimum instance value is 7.89% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.47%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.57% above the average, while the minimum instance value is 7.91% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.45%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.54% above the average, while the minimum instance value is 7.89% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       783.37
    Elapsed Cycles                cycle         6599
    Memory Throughput                 %        46.26
    DRAM Throughput                   %        46.26
    Duration                         us         8.42
    L1/TEX Cache Throughput           %        25.56
    L2 Cache Throughput               %        25.50
    SM Active Cycles              cycle      4421.21
    Compute (SM) Throughput           %        17.07
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.55
    Achieved Active Warps Per SM           warp        34.83
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.45%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24077.33
    Total DRAM Elapsed Cycles        cycle       312320
    Average L1 Active Cycles         cycle      4421.21
    Total L1 Elapsed Cycles          cycle       383942
    Average L2 Active Cycles         cycle         3965
    Total L2 Elapsed Cycles          cycle       164352
    Average SM Active Cycles         cycle      4421.21
    Total SM Elapsed Cycles          cycle       383942
    Average SMSP Active Cycles       cycle      4340.10
    Total SMSP Elapsed Cycles        cycle      1535768
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.75
    Elapsed Cycles                cycle         7233
    Memory Throughput                 %        48.83
    DRAM Throughput                   %        48.83
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.34
    L2 Cache Throughput               %        27.53
    SM Active Cycles              cycle      5000.28
    Compute (SM) Throughput           %        15.57
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.90
    Achieved Active Warps Per SM           warp        34.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.1%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27749.33
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5000.28
    Total L1 Elapsed Cycles          cycle       420876
    Average L2 Active Cycles         cycle      4490.71
    Total L2 Elapsed Cycles          cycle       180048
    Average SM Active Cycles         cycle      5000.28
    Total SM Elapsed Cycles          cycle       420876
    Average SMSP Active Cycles       cycle      4918.20
    Total SMSP Elapsed Cycles        cycle      1683504
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.69
    Elapsed Cycles                cycle         7198
    Memory Throughput                 %        49.31
    DRAM Throughput                   %        49.31
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.87
    L2 Cache Throughput               %        27.68
    SM Active Cycles              cycle      5014.48
    Compute (SM) Throughput           %        15.94
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.62
    Achieved Active Warps Per SM           warp        34.86
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.38%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27856
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      5014.48
    Total L1 Elapsed Cycles          cycle       411220
    Average L2 Active Cycles         cycle      4513.08
    Total L2 Elapsed Cycles          cycle       179160
    Average SM Active Cycles         cycle      5014.48
    Total SM Elapsed Cycles          cycle       411220
    Average SMSP Active Cycles       cycle      4972.38
    Total SMSP Elapsed Cycles        cycle      1644880
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.011%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.29% above the average, while the minimum instance value is 3.13% below the        
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.88
    Elapsed Cycles                cycle         7222
    Memory Throughput                 %        49.01
    DRAM Throughput                   %        49.01
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.24
    L2 Cache Throughput               %        27.54
    SM Active Cycles              cycle      5008.38
    Compute (SM) Throughput           %        15.51
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.61
    Achieved Active Warps Per SM           warp        34.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.39%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27770.67
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      5008.38
    Total L1 Elapsed Cycles          cycle       422658
    Average L2 Active Cycles         cycle      4501.46
    Total L2 Elapsed Cycles          cycle       179976
    Average SM Active Cycles         cycle      5008.38
    Total SM Elapsed Cycles          cycle       422658
    Average SMSP Active Cycles       cycle      4901.06
    Total SMSP Elapsed Cycles        cycle      1690632
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4643499
    Memory Throughput                 %        48.95
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3592004.74
    Compute (SM) Throughput           %        45.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76746.67
    Total DRAM Elapsed Cycles        cycle    218848256
    Average L1 Active Cycles         cycle   3592004.74
    Total L1 Elapsed Cycles          cycle    269198920
    Average L2 Active Cycles         cycle   1210487.79
    Total L2 Elapsed Cycles          cycle    115649184
    Average SM Active Cycles         cycle   3592004.74
    Total SM Elapsed Cycles          cycle    269198920
    Average SMSP Active Cycles       cycle   3591946.62
    Total SMSP Elapsed Cycles        cycle   1076795680
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.54%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.66% above the average, while the minimum instance value is 7.89% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.46%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.56% above the average, while the minimum instance value is 7.94% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.54%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.66% above the average, while the minimum instance value is 7.89% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       786.32
    Elapsed Cycles                cycle         6651
    Memory Throughput                 %        45.60
    DRAM Throughput                   %        45.60
    Duration                         us         8.45
    L1/TEX Cache Throughput           %        25.58
    L2 Cache Throughput               %        25.34
    SM Active Cycles              cycle      4417.41
    Compute (SM) Throughput           %        17.27
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.53
    Achieved Active Warps Per SM           warp        34.81
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.47%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23813.33
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4417.41
    Total L1 Elapsed Cycles          cycle       379468
    Average L2 Active Cycles         cycle      3915.46
    Total L2 Elapsed Cycles          cycle       165432
    Average SM Active Cycles         cycle      4417.41
    Total SM Elapsed Cycles          cycle       379468
    Average SMSP Active Cycles       cycle      4285.15
    Total SMSP Elapsed Cycles        cycle      1517872
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.39
    Elapsed Cycles                cycle         7168
    Memory Throughput                 %        49.38
    DRAM Throughput                   %        49.38
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.71
    L2 Cache Throughput               %        27.80
    SM Active Cycles              cycle      4857.05
    Compute (SM) Throughput           %        15.82
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.66
    Achieved Active Warps Per SM           warp        35.84
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.34%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27810.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4857.05
    Total L1 Elapsed Cycles          cycle       414136
    Average L2 Active Cycles         cycle      4478.67
    Total L2 Elapsed Cycles          cycle       178464
    Average SM Active Cycles         cycle      4857.05
    Total SM Elapsed Cycles          cycle       414136
    Average SMSP Active Cycles       cycle      4915.14
    Total SMSP Elapsed Cycles        cycle      1656544
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       781.49
    Elapsed Cycles                cycle         7137
    Memory Throughput                 %        49.72
    DRAM Throughput                   %        49.72
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        24.36
    L2 Cache Throughput               %        27.90
    SM Active Cycles              cycle      4891.59
    Compute (SM) Throughput           %        16.26
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.40
    Achieved Active Warps Per SM           warp        35.71
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.6%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27832
    Total DRAM Elapsed Cycles        cycle       335872
    Average L1 Active Cycles         cycle      4891.59
    Total L1 Elapsed Cycles          cycle       403120
    Average L2 Active Cycles         cycle      4497.83
    Total L2 Elapsed Cycles          cycle       177720
    Average SM Active Cycles         cycle      4891.59
    Total SM Elapsed Cycles          cycle       403120
    Average SMSP Active Cycles       cycle      4915.30
    Total SMSP Elapsed Cycles        cycle      1612480
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.85
    Elapsed Cycles                cycle         7166
    Memory Throughput                 %        49.31
    DRAM Throughput                   %        49.31
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.95
    L2 Cache Throughput               %        27.80
    SM Active Cycles              cycle      5010.29
    Compute (SM) Throughput           %        15.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.69
    Achieved Active Warps Per SM           warp        34.89
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.31%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27770.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5010.29
    Total L1 Elapsed Cycles          cycle       410240
    Average L2 Active Cycles         cycle      4507.33
    Total L2 Elapsed Cycles          cycle       178416
    Average SM Active Cycles         cycle      5010.29
    Total SM Elapsed Cycles          cycle       410240
    Average SMSP Active Cycles       cycle      4930.41
    Total SMSP Elapsed Cycles        cycle      1640960
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4646077
    Memory Throughput                 %        48.94
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.58
    SM Active Cycles              cycle   3591796.24
    Compute (SM) Throughput           %        45.42
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        77344
    Total DRAM Elapsed Cycles        cycle    218970112
    Average L1 Active Cycles         cycle   3591796.24
    Total L1 Elapsed Cycles          cycle    269267210
    Average L2 Active Cycles         cycle   1212343.42
    Total L2 Elapsed Cycles          cycle    115713288
    Average SM Active Cycles         cycle   3591796.24
    Total SM Elapsed Cycles          cycle    269267210
    Average SMSP Active Cycles       cycle   3592361.08
    Total SMSP Elapsed Cycles        cycle   1077068840
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.48%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.60% above the average, while the minimum instance value is 8.01% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.57%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.71% above the average, while the minimum instance value is 7.85% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.48%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.60% above the average, while the minimum instance value is 8.01% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.21
    SM Frequency                    Mhz       784.37
    Elapsed Cycles                cycle         6588
    Memory Throughput                 %        46.20
    DRAM Throughput                   %        46.20
    Duration                         us         8.38
    L1/TEX Cache Throughput           %        25.22
    L2 Cache Throughput               %        25.58
    SM Active Cycles              cycle      4480.17
    Compute (SM) Throughput           %        17.47
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.59
    Achieved Active Warps Per SM           warp        34.36
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.41%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24050.67
    Total DRAM Elapsed Cycles        cycle       312320
    Average L1 Active Cycles         cycle      4480.17
    Total L1 Elapsed Cycles          cycle       375186
    Average L2 Active Cycles         cycle      3972.79
    Total L2 Elapsed Cycles          cycle       163944
    Average SM Active Cycles         cycle      4480.17
    Total SM Elapsed Cycles          cycle       375186
    Average SMSP Active Cycles       cycle      4364.96
    Total SMSP Elapsed Cycles        cycle      1500744
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.67
    Elapsed Cycles                cycle         7381
    Memory Throughput                 %        47.96
    DRAM Throughput                   %        47.96
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        23.59
    L2 Cache Throughput               %        26.98
    SM Active Cycles              cycle      4887.26
    Compute (SM) Throughput           %        15.75
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.37
    Achieved Active Warps Per SM           warp        35.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.63%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27746.67
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      4887.26
    Total L1 Elapsed Cycles          cycle       416082
    Average L2 Active Cycles         cycle      4463.54
    Total L2 Elapsed Cycles          cycle       183792
    Average SM Active Cycles         cycle      4887.26
    Total SM Elapsed Cycles          cycle       416082
    Average SMSP Active Cycles       cycle      4889.84
    Total SMSP Elapsed Cycles        cycle      1664328
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       782.42
    Elapsed Cycles                cycle         7219
    Memory Throughput                 %        49.20
    DRAM Throughput                   %        49.20
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.70
    L2 Cache Throughput               %        27.60
    SM Active Cycles              cycle      4858.17
    Compute (SM) Throughput           %        15.81
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 24.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.86
    Achieved Active Warps Per SM           warp        36.41
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.14%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27794.67
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      4858.17
    Total L1 Elapsed Cycles          cycle       414472
    Average L2 Active Cycles         cycle      4544.50
    Total L2 Elapsed Cycles          cycle       179688
    Average SM Active Cycles         cycle      4858.17
    Total SM Elapsed Cycles          cycle       414472
    Average SMSP Active Cycles       cycle      4966.34
    Total SMSP Elapsed Cycles        cycle      1657888
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.05
    Elapsed Cycles                cycle         7233
    Memory Throughput                 %        48.92
    DRAM Throughput                   %        48.92
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.78
    L2 Cache Throughput               %        27.57
    SM Active Cycles              cycle      4977.72
    Compute (SM) Throughput           %        15.87
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.21
    Achieved Active Warps Per SM           warp        34.66
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.79%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27800
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4977.72
    Total L1 Elapsed Cycles          cycle       412872
    Average L2 Active Cycles         cycle      4508.38
    Total L2 Elapsed Cycles          cycle       179952
    Average SM Active Cycles         cycle      4977.72
    Total SM Elapsed Cycles          cycle       412872
    Average SMSP Active Cycles       cycle      4920.19
    Total SMSP Elapsed Cycles        cycle      1651488
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4644148
    Memory Throughput                 %        48.83
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.26
    L2 Cache Throughput               %         3.58
    SM Active Cycles              cycle   3591537.84
    Compute (SM) Throughput           %        45.32
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77037.33
    Total DRAM Elapsed Cycles        cycle    218880000
    Average L1 Active Cycles         cycle   3591537.84
    Total L1 Elapsed Cycles          cycle    269879640
    Average L2 Active Cycles         cycle   1221341.62
    Total L2 Elapsed Cycles          cycle    115665408
    Average SM Active Cycles         cycle   3591537.84
    Total SM Elapsed Cycles          cycle    269879640
    Average SMSP Active Cycles       cycle   3592380.47
    Total SMSP Elapsed Cycles        cycle   1079518560
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.39%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.54% above the average, while the minimum instance value is 7.92% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.44%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.59% above the average, while the minimum instance value is 7.96% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.39%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.54% above the average, while the minimum instance value is 7.92% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       780.88
    Elapsed Cycles                cycle         6676
    Memory Throughput                 %        45.09
    DRAM Throughput                   %        45.09
    Duration                         us         8.54
    L1/TEX Cache Throughput           %        25.42
    L2 Cache Throughput               %        25.24
    SM Active Cycles              cycle      4445.71
    Compute (SM) Throughput           %        17.43
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.95
    Achieved Active Warps Per SM           warp        34.54
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.05%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23778.67
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4445.71
    Total L1 Elapsed Cycles          cycle       375906
    Average L2 Active Cycles         cycle      4060.67
    Total L2 Elapsed Cycles          cycle       166272
    Average SM Active Cycles         cycle      4445.71
    Total SM Elapsed Cycles          cycle       375906
    Average SMSP Active Cycles       cycle      4460.09
    Total SMSP Elapsed Cycles        cycle      1503624
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       785.08
    Elapsed Cycles                cycle         7262
    Memory Throughput                 %        48.54
    DRAM Throughput                   %        48.54
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.68
    L2 Cache Throughput               %        27.42
    SM Active Cycles              cycle      4995.33
    Compute (SM) Throughput           %        15.80
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.98
    Achieved Active Warps Per SM           warp        35.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27752
    Total DRAM Elapsed Cycles        cycle       343040
    Average L1 Active Cycles         cycle      4995.33
    Total L1 Elapsed Cycles          cycle       414750
    Average L2 Active Cycles         cycle      4479.04
    Total L2 Elapsed Cycles          cycle       180888
    Average SM Active Cycles         cycle      4995.33
    Total SM Elapsed Cycles          cycle       414750
    Average SMSP Active Cycles       cycle      4942.38
    Total SMSP Elapsed Cycles        cycle      1659000
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       784.58
    Elapsed Cycles                cycle         7160
    Memory Throughput                 %        49.60
    DRAM Throughput                   %        49.60
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.85
    L2 Cache Throughput               %        27.88
    SM Active Cycles              cycle      4861.81
    Compute (SM) Throughput           %        15.92
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.83
    Achieved Active Warps Per SM           warp        35.92
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27765.33
    Total DRAM Elapsed Cycles        cycle       335872
    Average L1 Active Cycles         cycle      4861.81
    Total L1 Elapsed Cycles          cycle       411646
    Average L2 Active Cycles         cycle      4500.54
    Total L2 Elapsed Cycles          cycle       178176
    Average SM Active Cycles         cycle      4861.81
    Total SM Elapsed Cycles          cycle       411646
    Average SMSP Active Cycles       cycle      4947.20
    Total SMSP Elapsed Cycles        cycle      1646584
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.48
    Elapsed Cycles                cycle         7101
    Memory Throughput                 %        49.93
    DRAM Throughput                   %        49.93
    Duration                         us         9.06
    L1/TEX Cache Throughput           %        24.50
    L2 Cache Throughput               %        28.05
    SM Active Cycles              cycle      4867.81
    Compute (SM) Throughput           %        16.35
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.46
    Achieved Active Warps Per SM           warp        35.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.54%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27864
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      4867.81
    Total L1 Elapsed Cycles          cycle       400722
    Average L2 Active Cycles         cycle      4463.79
    Total L2 Elapsed Cycles          cycle       176784
    Average SM Active Cycles         cycle      4867.81
    Total SM Elapsed Cycles          cycle       400722
    Average SMSP Active Cycles       cycle      4901.22
    Total SMSP Elapsed Cycles        cycle      1602888
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4640859
    Memory Throughput                 %        48.97
    DRAM Throughput                   %         0.22
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3591939.22
    Compute (SM) Throughput           %        45.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        79352
    Total DRAM Elapsed Cycles        cycle    218724352
    Average L1 Active Cycles         cycle   3591939.22
    Total L1 Elapsed Cycles          cycle    269119326
    Average L2 Active Cycles         cycle   1214180.58
    Total L2 Elapsed Cycles          cycle    115583328
    Average SM Active Cycles         cycle   3591939.22
    Total SM Elapsed Cycles          cycle    269119326
    Average SMSP Active Cycles       cycle   3592525.10
    Total SMSP Elapsed Cycles        cycle   1076477304
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.5%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.61% above the average, while the minimum instance value is 7.90% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.46%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.55% above the average, while the minimum instance value is 7.96% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.5%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.61% above the average, while the minimum instance value is 7.90% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       781.79
    Elapsed Cycles                cycle         6713
    Memory Throughput                 %        45.63
    DRAM Throughput                   %        45.63
    Duration                         us         8.58
    L1/TEX Cache Throughput           %        25.01
    L2 Cache Throughput               %        25.06
    SM Active Cycles              cycle      4518.40
    Compute (SM) Throughput           %        17.13
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.05
    Achieved Active Warps Per SM           warp        34.58
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.95%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24061.33
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4518.40
    Total L1 Elapsed Cycles          cycle       382550
    Average L2 Active Cycles         cycle      4207.62
    Total L2 Elapsed Cycles          cycle       167040
    Average SM Active Cycles         cycle      4518.40
    Total SM Elapsed Cycles          cycle       382550
    Average SMSP Active Cycles       cycle      4541.41
    Total SMSP Elapsed Cycles        cycle      1530200
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       792.69
    Elapsed Cycles                cycle         7217
    Memory Throughput                 %        49.39
    DRAM Throughput                   %        49.39
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.85
    L2 Cache Throughput               %        27.93
    SM Active Cycles              cycle      4975.02
    Compute (SM) Throughput           %        15.91
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.66
    Achieved Active Warps Per SM           warp        34.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.34%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27733.33
    Total DRAM Elapsed Cycles        cycle       336896
    Average L1 Active Cycles         cycle      4975.02
    Total L1 Elapsed Cycles          cycle       411828
    Average L2 Active Cycles         cycle      4495.21
    Total L2 Elapsed Cycles          cycle       177504
    Average SM Active Cycles         cycle      4975.02
    Total SM Elapsed Cycles          cycle       411828
    Average SMSP Active Cycles       cycle      4959.78
    Total SMSP Elapsed Cycles        cycle      1647312
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       784.33
    Elapsed Cycles                cycle         7134
    Memory Throughput                 %        49.73
    DRAM Throughput                   %        49.73
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.80
    L2 Cache Throughput               %        27.90
    SM Active Cycles              cycle      5019.26
    Compute (SM) Throughput           %        15.88
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.75
    Achieved Active Warps Per SM           warp        34.44
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.25%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27754.67
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      5019.26
    Total L1 Elapsed Cycles          cycle       412690
    Average L2 Active Cycles         cycle      4510.79
    Total L2 Elapsed Cycles          cycle       177624
    Average SM Active Cycles         cycle      5019.26
    Total SM Elapsed Cycles          cycle       412690
    Average SMSP Active Cycles       cycle      4930.08
    Total SMSP Elapsed Cycles        cycle      1650760
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       789.14
    Elapsed Cycles                cycle         7208
    Memory Throughput                 %        49.49
    DRAM Throughput                   %        49.49
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.83
    L2 Cache Throughput               %        27.83
    SM Active Cycles              cycle      4955.09
    Compute (SM) Throughput           %        15.90
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.71
    Achieved Active Warps Per SM           warp        35.86
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.29%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27874.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4955.09
    Total L1 Elapsed Cycles          cycle       412270
    Average L2 Active Cycles         cycle      4576.33
    Total L2 Elapsed Cycles          cycle       178296
    Average SM Active Cycles         cycle      4955.09
    Total SM Elapsed Cycles          cycle       412270
    Average SMSP Active Cycles       cycle      5006.14
    Total SMSP Elapsed Cycles        cycle      1649080
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4636110
    Memory Throughput                 %        48.90
    DRAM Throughput                   %         0.21
    Duration                         ms         5.83
    L1/TEX Cache Throughput           %        63.26
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3591728.76
    Compute (SM) Throughput           %        45.39
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76885.33
    Total DRAM Elapsed Cycles        cycle    218501120
    Average L1 Active Cycles         cycle   3591728.76
    Total L1 Elapsed Cycles          cycle    269456552
    Average L2 Active Cycles         cycle   1214476.46
    Total L2 Elapsed Cycles          cycle    115465296
    Average SM Active Cycles         cycle   3591728.76
    Total SM Elapsed Cycles          cycle    269456552
    Average SMSP Active Cycles       cycle   3591914.51
    Total SMSP Elapsed Cycles        cycle   1077826208
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.45%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.58% above the average, while the minimum instance value is 7.92% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.44%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.56% above the average, while the minimum instance value is 7.94% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.45%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.58% above the average, while the minimum instance value is 7.92% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.51
    Elapsed Cycles                cycle         6650
    Memory Throughput                 %        45.52
    DRAM Throughput                   %        45.52
    Duration                         us         8.48
    L1/TEX Cache Throughput           %        25.46
    L2 Cache Throughput               %        25.32
    SM Active Cycles              cycle      4438.50
    Compute (SM) Throughput           %        17.24
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.33
    Achieved Active Warps Per SM           warp        34.72
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.67%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23770.67
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4438.50
    Total L1 Elapsed Cycles          cycle       380160
    Average L2 Active Cycles         cycle      3913.88
    Total L2 Elapsed Cycles          cycle       165528
    Average SM Active Cycles         cycle      4438.50
    Total SM Elapsed Cycles          cycle       380160
    Average SMSP Active Cycles       cycle      4276.69
    Total SMSP Elapsed Cycles        cycle      1520640
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.59
    Elapsed Cycles                cycle         7175
    Memory Throughput                 %        49.31
    DRAM Throughput                   %        49.31
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.68
    L2 Cache Throughput               %        27.80
    SM Active Cycles              cycle      4990.34
    Compute (SM) Throughput           %        15.80
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.05
    Achieved Active Warps Per SM           warp        34.58
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.95%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27773.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4990.34
    Total L1 Elapsed Cycles          cycle       414708
    Average L2 Active Cycles         cycle      4496.42
    Total L2 Elapsed Cycles          cycle       178488
    Average SM Active Cycles         cycle      4990.34
    Total SM Elapsed Cycles          cycle       414708
    Average SMSP Active Cycles       cycle      4934.51
    Total SMSP Elapsed Cycles        cycle      1658832
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.33
    Elapsed Cycles                cycle         7172
    Memory Throughput                 %        49.18
    DRAM Throughput                   %        49.18
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        24.11
    L2 Cache Throughput               %        27.79
    SM Active Cycles              cycle      4916.33
    Compute (SM) Throughput           %        16.09
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.82
    Achieved Active Warps Per SM           warp        35.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.18%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27784
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      4916.33
    Total L1 Elapsed Cycles          cycle       407434
    Average L2 Active Cycles         cycle      4413.75
    Total L2 Elapsed Cycles          cycle       178584
    Average SM Active Cycles         cycle      4916.33
    Total SM Elapsed Cycles          cycle       407434
    Average SMSP Active Cycles       cycle      4845.52
    Total SMSP Elapsed Cycles        cycle      1629736
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       786.12
    Elapsed Cycles                cycle         7205
    Memory Throughput                 %        49.17
    DRAM Throughput                   %        49.17
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.78
    L2 Cache Throughput               %        27.69
    SM Active Cycles              cycle      5048.12
    Compute (SM) Throughput           %        15.88
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.85
    Achieved Active Warps Per SM           warp        34.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.15%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27776
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      5048.12
    Total L1 Elapsed Cycles          cycle       412786
    Average L2 Active Cycles         cycle      4515.25
    Total L2 Elapsed Cycles          cycle       179136
    Average SM Active Cycles         cycle      5048.12
    Total SM Elapsed Cycles          cycle       412786
    Average SMSP Active Cycles       cycle      4933.00
    Total SMSP Elapsed Cycles        cycle      1651144
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       798.67
    Elapsed Cycles                cycle      4653579
    Memory Throughput                 %        48.92
    DRAM Throughput                   %         0.22
    Duration                         ms         5.82
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.60
    SM Active Cycles              cycle   3592208.21
    Compute (SM) Throughput           %        45.41
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        79672
    Total DRAM Elapsed Cycles        cycle    218044416
    Average L1 Active Cycles         cycle   3592208.21
    Total L1 Elapsed Cycles          cycle    269349926
    Average L2 Active Cycles         cycle   1210607.79
    Total L2 Elapsed Cycles          cycle    115245672
    Average SM Active Cycles         cycle   3592208.21
    Total SM Elapsed Cycles          cycle    269349926
    Average SMSP Active Cycles       cycle   3592101.10
    Total SMSP Elapsed Cycles        cycle   1077399704
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.53%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.66% above the average, while the minimum instance value is 7.98% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.41%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.51% above the average, while the minimum instance value is 7.96% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.53%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.66% above the average, while the minimum instance value is 7.98% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.43
    Elapsed Cycles                cycle         6696
    Memory Throughput                 %        45.53
    DRAM Throughput                   %        45.53
    Duration                         us         8.54
    L1/TEX Cache Throughput           %        25.38
    L2 Cache Throughput               %        25.16
    SM Active Cycles              cycle      4452.22
    Compute (SM) Throughput           %        17.16
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.07
    Achieved Active Warps Per SM           warp        35.07
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24010.67
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4452.22
    Total L1 Elapsed Cycles          cycle       381804
    Average L2 Active Cycles         cycle      4094.96
    Total L2 Elapsed Cycles          cycle       166632
    Average SM Active Cycles         cycle      4452.22
    Total SM Elapsed Cycles          cycle       381804
    Average SMSP Active Cycles       cycle      4460.04
    Total SMSP Elapsed Cycles        cycle      1527216
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.59
    Elapsed Cycles                cycle         7241
    Memory Throughput                 %        48.89
    DRAM Throughput                   %        48.89
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.48
    L2 Cache Throughput               %        27.50
    SM Active Cycles              cycle      4965.93
    Compute (SM) Throughput           %        15.66
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.61
    Achieved Active Warps Per SM           warp        35.33
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.39%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27784
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4965.93
    Total L1 Elapsed Cycles          cycle       418448
    Average L2 Active Cycles         cycle      4379.75
    Total L2 Elapsed Cycles          cycle       180312
    Average SM Active Cycles         cycle      4965.93
    Total SM Elapsed Cycles          cycle       418448
    Average SMSP Active Cycles       cycle      4828.88
    Total SMSP Elapsed Cycles        cycle      1673792
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.25
    Elapsed Cycles                cycle         7287
    Memory Throughput                 %        48.44
    DRAM Throughput                   %        48.44
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.80
    L2 Cache Throughput               %        27.37
    SM Active Cycles              cycle      5035.76
    Compute (SM) Throughput           %        15.88
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.65
    Achieved Active Warps Per SM           warp        34.39
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27778.67
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      5035.76
    Total L1 Elapsed Cycles          cycle       412768
    Average L2 Active Cycles         cycle      4460.75
    Total L2 Elapsed Cycles          cycle       181296
    Average SM Active Cycles         cycle      5035.76
    Total SM Elapsed Cycles          cycle       412768
    Average SMSP Active Cycles       cycle      4867.31
    Total SMSP Elapsed Cycles        cycle      1651072
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.75
    Elapsed Cycles                cycle         7151
    Memory Throughput                 %        49.37
    DRAM Throughput                   %        49.37
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.81
    L2 Cache Throughput               %        27.88
    SM Active Cycles              cycle      4940.53
    Compute (SM) Throughput           %        15.88
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.24
    Achieved Active Warps Per SM           warp        35.16
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.76%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27805.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4940.53
    Total L1 Elapsed Cycles          cycle       412750
    Average L2 Active Cycles         cycle         4502
    Total L2 Elapsed Cycles          cycle       177912
    Average SM Active Cycles         cycle      4940.53
    Total SM Elapsed Cycles          cycle       412750
    Average SMSP Active Cycles       cycle      4910.44
    Total SMSP Elapsed Cycles        cycle      1651000
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4644088
    Memory Throughput                 %        48.94
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.26
    L2 Cache Throughput               %         3.58
    SM Active Cycles              cycle   3591642.72
    Compute (SM) Throughput           %        45.43
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76562.67
    Total DRAM Elapsed Cycles        cycle    218876928
    Average L1 Active Cycles         cycle   3591642.72
    Total L1 Elapsed Cycles          cycle    269257032
    Average L2 Active Cycles         cycle      1206697
    Total L2 Elapsed Cycles          cycle    115664040
    Average SM Active Cycles         cycle   3591642.72
    Total SM Elapsed Cycles          cycle    269257032
    Average SMSP Active Cycles       cycle   3592182.08
    Total SMSP Elapsed Cycles        cycle   1077028128
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.47%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.58% above the average, while the minimum instance value is 7.90% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.47%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.57% above the average, while the minimum instance value is 7.92% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.47%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.58% above the average, while the minimum instance value is 7.90% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       790.05
    Elapsed Cycles                cycle         6716
    Memory Throughput                 %        45.63
    DRAM Throughput                   %        45.63
    Duration                         us         8.48
    L1/TEX Cache Throughput           %        25.42
    L2 Cache Throughput               %        25.30
    SM Active Cycles              cycle      4445.07
    Compute (SM) Throughput           %        17.34
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.23
    Achieved Active Warps Per SM           warp        35.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.77%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23832
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4445.07
    Total L1 Elapsed Cycles          cycle       377986
    Average L2 Active Cycles         cycle      4048.46
    Total L2 Elapsed Cycles          cycle       165624
    Average SM Active Cycles         cycle      4445.07
    Total SM Elapsed Cycles          cycle       377986
    Average SMSP Active Cycles       cycle      4397.73
    Total SMSP Elapsed Cycles        cycle      1511944
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       787.46
    Elapsed Cycles                cycle         7220
    Memory Throughput                 %        49.48
    DRAM Throughput                   %        49.48
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        24.11
    L2 Cache Throughput               %        27.86
    SM Active Cycles              cycle      4947.03
    Compute (SM) Throughput           %        16.09
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.91
    Achieved Active Warps Per SM           warp        35.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.09%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27866.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4947.03
    Total L1 Elapsed Cycles          cycle       407304
    Average L2 Active Cycles         cycle      4468.21
    Total L2 Elapsed Cycles          cycle       177984
    Average SM Active Cycles         cycle      4947.03
    Total SM Elapsed Cycles          cycle       407304
    Average SMSP Active Cycles       cycle      4902.69
    Total SMSP Elapsed Cycles        cycle      1629216
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.44
    Elapsed Cycles                cycle         7240
    Memory Throughput                 %        48.84
    DRAM Throughput                   %        48.84
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        24.08
    L2 Cache Throughput               %        27.52
    SM Active Cycles              cycle      4990.74
    Compute (SM) Throughput           %        16.08
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.48
    Achieved Active Warps Per SM           warp        34.79
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.52%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27754.67
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4990.74
    Total L1 Elapsed Cycles          cycle       407614
    Average L2 Active Cycles         cycle      4414.12
    Total L2 Elapsed Cycles          cycle       180096
    Average SM Active Cycles         cycle      4990.74
    Total SM Elapsed Cycles          cycle       407614
    Average SMSP Active Cycles       cycle      4848.42
    Total SMSP Elapsed Cycles        cycle      1630456
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.39
    Elapsed Cycles                cycle         7157
    Memory Throughput                 %        49.25
    DRAM Throughput                   %        49.25
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.71
    L2 Cache Throughput               %        27.84
    SM Active Cycles              cycle      5057.91
    Compute (SM) Throughput           %        15.82
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.84
    Achieved Active Warps Per SM           warp        34.49
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.16%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27736
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5057.91
    Total L1 Elapsed Cycles          cycle       414362
    Average L2 Active Cycles         cycle      4489.33
    Total L2 Elapsed Cycles          cycle       178104
    Average SM Active Cycles         cycle      5057.91
    Total SM Elapsed Cycles          cycle       414362
    Average SMSP Active Cycles       cycle      4934.37
    Total SMSP Elapsed Cycles        cycle      1657448
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       797.01
    Elapsed Cycles                cycle      4639296
    Memory Throughput                 %        48.92
    DRAM Throughput                   %         0.21
    Duration                         ms         5.82
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.60
    SM Active Cycles              cycle   3592070.41
    Compute (SM) Throughput           %        45.41
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77426.67
    Total DRAM Elapsed Cycles        cycle    217961472
    Average L1 Active Cycles         cycle   3592070.41
    Total L1 Elapsed Cycles          cycle    269375762
    Average L2 Active Cycles         cycle   1205558.71
    Total L2 Elapsed Cycles          cycle    115192344
    Average SM Active Cycles         cycle   3592070.41
    Total SM Elapsed Cycles          cycle    269375762
    Average SMSP Active Cycles       cycle   3592432.50
    Total SMSP Elapsed Cycles        cycle   1077503048
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.44%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.55% above the average, while the minimum instance value is 7.89% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.45%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.56% above the average, while the minimum instance value is 8.02% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.44%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.55% above the average, while the minimum instance value is 7.89% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.75
    Elapsed Cycles                cycle         6764
    Memory Throughput                 %        45.07
    DRAM Throughput                   %        45.07
    Duration                         us         8.64
    L1/TEX Cache Throughput           %        25.26
    L2 Cache Throughput               %        24.85
    SM Active Cycles              cycle      4472.71
    Compute (SM) Throughput           %        17.16
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.20
    Achieved Active Warps Per SM           warp        35.14
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.8%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24000
    Total DRAM Elapsed Cycles        cycle       319488
    Average L1 Active Cycles         cycle      4472.71
    Total L1 Elapsed Cycles          cycle       381896
    Average L2 Active Cycles         cycle      4070.58
    Total L2 Elapsed Cycles          cycle       168384
    Average SM Active Cycles         cycle      4472.71
    Total SM Elapsed Cycles          cycle       381896
    Average SMSP Active Cycles       cycle      4473.36
    Total SMSP Elapsed Cycles        cycle      1527584
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.30
    Elapsed Cycles                cycle         7257
    Memory Throughput                 %        48.72
    DRAM Throughput                   %        48.72
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.71
    L2 Cache Throughput               %        27.47
    SM Active Cycles              cycle      5076.38
    Compute (SM) Throughput           %        15.82
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.55
    Achieved Active Warps Per SM           warp        34.34
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.45%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27773.33
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle      5076.38
    Total L1 Elapsed Cycles          cycle       414364
    Average L2 Active Cycles         cycle      4488.50
    Total L2 Elapsed Cycles          cycle       180552
    Average SM Active Cycles         cycle      5076.38
    Total SM Elapsed Cycles          cycle       414364
    Average SMSP Active Cycles       cycle      4939.86
    Total SMSP Elapsed Cycles        cycle      1657456
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.85
    Elapsed Cycles                cycle         7128
    Memory Throughput                 %        49.61
    DRAM Throughput                   %        49.61
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.63
    L2 Cache Throughput               %        27.97
    SM Active Cycles              cycle      4974.50
    Compute (SM) Throughput           %        15.77
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.57
    Achieved Active Warps Per SM           warp        34.84
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.43%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27773.33
    Total DRAM Elapsed Cycles        cycle       335872
    Average L1 Active Cycles         cycle      4974.50
    Total L1 Elapsed Cycles          cycle       415648
    Average L2 Active Cycles         cycle      4520.79
    Total L2 Elapsed Cycles          cycle       177432
    Average SM Active Cycles         cycle      4974.50
    Total SM Elapsed Cycles          cycle       415648
    Average SMSP Active Cycles       cycle      4937.64
    Total SMSP Elapsed Cycles        cycle      1662592
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.91
    Elapsed Cycles                cycle         7255
    Memory Throughput                 %        48.75
    DRAM Throughput                   %        48.75
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.65
    L2 Cache Throughput               %        27.45
    SM Active Cycles              cycle      5041.74
    Compute (SM) Throughput           %        15.79
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.86
    Achieved Active Warps Per SM           warp        34.49
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.14%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27786.67
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle      5041.74
    Total L1 Elapsed Cycles          cycle       414964
    Average L2 Active Cycles         cycle         4487
    Total L2 Elapsed Cycles          cycle       180576
    Average SM Active Cycles         cycle      5041.74
    Total SM Elapsed Cycles          cycle       414964
    Average SMSP Active Cycles       cycle      4909.10
    Total SMSP Elapsed Cycles        cycle      1659856
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4643664
    Memory Throughput                 %        48.96
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.58
    SM Active Cycles              cycle   3591792.55
    Compute (SM) Throughput           %        45.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        76680
    Total DRAM Elapsed Cycles        cycle    218855424
    Average L1 Active Cycles         cycle   3591792.55
    Total L1 Elapsed Cycles          cycle    269166382
    Average L2 Active Cycles         cycle   1217972.67
    Total L2 Elapsed Cycles          cycle    115653432
    Average SM Active Cycles         cycle   3591792.55
    Total SM Elapsed Cycles          cycle    269166382
    Average SMSP Active Cycles       cycle   3592387.38
    Total SMSP Elapsed Cycles        cycle   1076665528
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.44%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.53% above the average, while the minimum instance value is 7.86% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.54%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.67% above the average, while the minimum instance value is 8.03% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.44%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.53% above the average, while the minimum instance value is 7.86% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       782.85
    Elapsed Cycles                cycle         6565
    Memory Throughput                 %        46.08
    DRAM Throughput                   %        46.08
    Duration                         us         8.38
    L1/TEX Cache Throughput           %        25.50
    L2 Cache Throughput               %        25.63
    SM Active Cycles              cycle      4430.71
    Compute (SM) Throughput           %        17.38
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.69
    Achieved Active Warps Per SM           warp        34.41
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.31%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23826.67
    Total DRAM Elapsed Cycles        cycle       310272
    Average L1 Active Cycles         cycle      4430.71
    Total L1 Elapsed Cycles          cycle       377122
    Average L2 Active Cycles         cycle         4065
    Total L2 Elapsed Cycles          cycle       163536
    Average SM Active Cycles         cycle      4430.71
    Total SM Elapsed Cycles          cycle       377122
    Average SMSP Active Cycles       cycle      4434.32
    Total SMSP Elapsed Cycles        cycle      1508488
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       781.75
    Elapsed Cycles                cycle         7185
    Memory Throughput                 %        49.00
    DRAM Throughput                   %        49.00
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        24.01
    L2 Cache Throughput               %        27.73
    SM Active Cycles              cycle      5029.14
    Compute (SM) Throughput           %        16.03
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.46
    Achieved Active Warps Per SM           warp        34.78
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.54%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27762.67
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      5029.14
    Total L1 Elapsed Cycles          cycle       408902
    Average L2 Active Cycles         cycle      4472.96
    Total L2 Elapsed Cycles          cycle       178848
    Average SM Active Cycles         cycle      5029.14
    Total SM Elapsed Cycles          cycle       408902
    Average SMSP Active Cycles       cycle      4928.38
    Total SMSP Elapsed Cycles        cycle      1635608
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.14
    Elapsed Cycles                cycle         7220
    Memory Throughput                 %        48.95
    DRAM Throughput                   %        48.95
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        24.06
    L2 Cache Throughput               %        27.59
    SM Active Cycles              cycle      4983.40
    Compute (SM) Throughput           %        16.05
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.95
    Achieved Active Warps Per SM           warp        35.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.05%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27818.67
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4983.40
    Total L1 Elapsed Cycles          cycle       408220
    Average L2 Active Cycles         cycle      4501.96
    Total L2 Elapsed Cycles          cycle       179784
    Average SM Active Cycles         cycle      4983.40
    Total SM Elapsed Cycles          cycle       408220
    Average SMSP Active Cycles       cycle      4893.18
    Total SMSP Elapsed Cycles        cycle      1632880
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       782.38
    Elapsed Cycles                cycle         7188
    Memory Throughput                 %        48.93
    DRAM Throughput                   %        48.93
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        24.27
    L2 Cache Throughput               %        27.72
    SM Active Cycles              cycle      4979.93
    Compute (SM) Throughput           %        16.19
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.15
    Achieved Active Warps Per SM           warp        35.11
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.85%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27725.33
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      4979.93
    Total L1 Elapsed Cycles          cycle       404796
    Average L2 Active Cycles         cycle      4399.79
    Total L2 Elapsed Cycles          cycle       178920
    Average SM Active Cycles         cycle      4979.93
    Total SM Elapsed Cycles          cycle       404796
    Average SMSP Active Cycles       cycle      4835.22
    Total SMSP Elapsed Cycles        cycle      1619184
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.02
    Elapsed Cycles                cycle      4659529
    Memory Throughput                 %        48.92
    DRAM Throughput                   %         0.22
    Duration                         ms         5.80
    L1/TEX Cache Throughput           %        63.26
    L2 Cache Throughput               %         3.61
    SM Active Cycles              cycle   3591770.71
    Compute (SM) Throughput           %        45.41
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     79165.33
    Total DRAM Elapsed Cycles        cycle    217228288
    Average L1 Active Cycles         cycle   3591770.71
    Total L1 Elapsed Cycles          cycle    269362490
    Average L2 Active Cycles         cycle   1200343.42
    Total L2 Elapsed Cycles          cycle    114832200
    Average SM Active Cycles         cycle   3591770.71
    Total SM Elapsed Cycles          cycle    269362490
    Average SMSP Active Cycles       cycle   3591837.97
    Total SMSP Elapsed Cycles        cycle   1077449960
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.44%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.55% above the average, while the minimum instance value is 7.89% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.45%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.56% above the average, while the minimum instance value is 8.00% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.44%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.55% above the average, while the minimum instance value is 7.89% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       781.71
    Elapsed Cycles                cycle         6757
    Memory Throughput                 %        45.36
    DRAM Throughput                   %        45.36
    Duration                         us         8.64
    L1/TEX Cache Throughput           %        24.98
    L2 Cache Throughput               %        24.88
    SM Active Cycles              cycle      4524.21
    Compute (SM) Throughput           %        17.16
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.41
    Achieved Active Warps Per SM           warp        34.76
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.59%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24074.67
    Total DRAM Elapsed Cycles        cycle       318464
    Average L1 Active Cycles         cycle      4524.21
    Total L1 Elapsed Cycles          cycle       381936
    Average L2 Active Cycles         cycle      4102.17
    Total L2 Elapsed Cycles          cycle       168264
    Average SM Active Cycles         cycle      4524.21
    Total SM Elapsed Cycles          cycle       381936
    Average SMSP Active Cycles       cycle      4488.09
    Total SMSP Elapsed Cycles        cycle      1527744
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.03
    Elapsed Cycles                cycle         7189
    Memory Throughput                 %        49.34
    DRAM Throughput                   %        49.34
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.74
    L2 Cache Throughput               %        27.70
    SM Active Cycles              cycle      4987.76
    Compute (SM) Throughput           %        15.83
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.38
    Achieved Active Warps Per SM           warp        35.22
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.62%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27874.67
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      4987.76
    Total L1 Elapsed Cycles          cycle       413888
    Average L2 Active Cycles         cycle      4460.46
    Total L2 Elapsed Cycles          cycle       178848
    Average SM Active Cycles         cycle      4987.76
    Total SM Elapsed Cycles          cycle       413888
    Average SMSP Active Cycles       cycle      4965.59
    Total SMSP Elapsed Cycles        cycle      1655552
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.48
    Elapsed Cycles                cycle         7066
    Memory Throughput                 %        50.17
    DRAM Throughput                   %        50.17
    Duration                         us         8.99
    L1/TEX Cache Throughput           %        23.78
    L2 Cache Throughput               %        28.22
    SM Active Cycles              cycle      5007.79
    Compute (SM) Throughput           %        15.86
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.06
    Achieved Active Warps Per SM           warp        34.11
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.94%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27829.33
    Total DRAM Elapsed Cycles        cycle       332800
    Average L1 Active Cycles         cycle      5007.79
    Total L1 Elapsed Cycles          cycle       413096
    Average L2 Active Cycles         cycle      4528.29
    Total L2 Elapsed Cycles          cycle       175800
    Average SM Active Cycles         cycle      5007.79
    Total SM Elapsed Cycles          cycle       413096
    Average SMSP Active Cycles       cycle      4927.92
    Total SMSP Elapsed Cycles        cycle      1652384
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       782.57
    Elapsed Cycles                cycle         7140
    Memory Throughput                 %        49.29
    DRAM Throughput                   %        49.29
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        24.35
    L2 Cache Throughput               %        27.89
    SM Active Cycles              cycle      5012.67
    Compute (SM) Throughput           %        16.25
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.67
    Achieved Active Warps Per SM           warp        34.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27762.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5012.67
    Total L1 Elapsed Cycles          cycle       403340
    Average L2 Active Cycles         cycle      4567.83
    Total L2 Elapsed Cycles          cycle       177768
    Average SM Active Cycles         cycle      5012.67
    Total SM Elapsed Cycles          cycle       403340
    Average SMSP Active Cycles       cycle      5022.58
    Total SMSP Elapsed Cycles        cycle      1613360
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       800.91
    Elapsed Cycles                cycle      4651972
    Memory Throughput                 %        48.91
    DRAM Throughput                   %         0.21
    Duration                         ms         5.80
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.60
    SM Active Cycles              cycle   3592263.81
    Compute (SM) Throughput           %        45.40
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76698.67
    Total DRAM Elapsed Cycles        cycle    217279488
    Average L1 Active Cycles         cycle   3592263.81
    Total L1 Elapsed Cycles          cycle    269393904
    Average L2 Active Cycles         cycle   1211051.21
    Total L2 Elapsed Cycles          cycle    114868272
    Average SM Active Cycles         cycle   3592263.81
    Total SM Elapsed Cycles          cycle    269393904
    Average SMSP Active Cycles       cycle   3591842.77
    Total SMSP Elapsed Cycles        cycle   1077575616
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.42%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.52% above the average, while the minimum instance value is 7.83% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.45%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.57% above the average, while the minimum instance value is 7.88% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.42%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.52% above the average, while the minimum instance value is 7.83% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       781.20
    Elapsed Cycles                cycle         6678
    Memory Throughput                 %        45.10
    DRAM Throughput                   %        45.10
    Duration                         us         8.54
    L1/TEX Cache Throughput           %        25.60
    L2 Cache Throughput               %        25.22
    SM Active Cycles              cycle      4413.47
    Compute (SM) Throughput           %        17.07
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.34
    Achieved Active Warps Per SM           warp        35.20
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.66%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23706.67
    Total DRAM Elapsed Cycles        cycle       315392
    Average L1 Active Cycles         cycle      4413.47
    Total L1 Elapsed Cycles          cycle       383814
    Average L2 Active Cycles         cycle      3949.67
    Total L2 Elapsed Cycles          cycle       166200
    Average SM Active Cycles         cycle      4413.47
    Total SM Elapsed Cycles          cycle       383814
    Average SMSP Active Cycles       cycle      4332.38
    Total SMSP Elapsed Cycles        cycle      1535256
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.43
    Elapsed Cycles                cycle         7155
    Memory Throughput                 %        49.37
    DRAM Throughput                   %        49.37
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        24.02
    L2 Cache Throughput               %        27.84
    SM Active Cycles              cycle      4998.22
    Compute (SM) Throughput           %        16.03
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.04
    Achieved Active Warps Per SM           warp        35.06
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27805.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4998.22
    Total L1 Elapsed Cycles          cycle       408720
    Average L2 Active Cycles         cycle      4473.17
    Total L2 Elapsed Cycles          cycle       178056
    Average SM Active Cycles         cycle      4998.22
    Total SM Elapsed Cycles          cycle       408720
    Average SMSP Active Cycles       cycle      4907.83
    Total SMSP Elapsed Cycles        cycle      1634880
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       786.04
    Elapsed Cycles                cycle         7123
    Memory Throughput                 %        49.78
    DRAM Throughput                   %        49.78
    Duration                         us         9.06
    L1/TEX Cache Throughput           %        24.21
    L2 Cache Throughput               %        28.00
    SM Active Cycles              cycle      5104.59
    Compute (SM) Throughput           %        16.16
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.77
    Achieved Active Warps Per SM           warp        34.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.23%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27781.33
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      5104.59
    Total L1 Elapsed Cycles          cycle       405650
    Average L2 Active Cycles         cycle      4462.08
    Total L2 Elapsed Cycles          cycle       177216
    Average SM Active Cycles         cycle      5104.59
    Total SM Elapsed Cycles          cycle       405650
    Average SMSP Active Cycles       cycle      4912.72
    Total SMSP Elapsed Cycles        cycle      1622600
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.26
    Elapsed Cycles                cycle         7263
    Memory Throughput                 %        48.75
    DRAM Throughput                   %        48.75
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.87
    L2 Cache Throughput               %        27.45
    SM Active Cycles              cycle      5046.62
    Compute (SM) Throughput           %        15.93
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.58
    Achieved Active Warps Per SM           warp        34.84
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.42%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27869.33
    Total DRAM Elapsed Cycles        cycle       343040
    Average L1 Active Cycles         cycle      5046.62
    Total L1 Elapsed Cycles          cycle       411438
    Average L2 Active Cycles         cycle      4520.71
    Total L2 Elapsed Cycles          cycle       180768
    Average SM Active Cycles         cycle      5046.62
    Total SM Elapsed Cycles          cycle       411438
    Average SMSP Active Cycles       cycle      4954.91
    Total SMSP Elapsed Cycles        cycle      1645752
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       800.44
    Elapsed Cycles                cycle      4654759
    Memory Throughput                 %        48.94
    DRAM Throughput                   %         0.21
    Duration                         ms         5.81
    L1/TEX Cache Throughput           %        63.24
    L2 Cache Throughput               %         3.60
    SM Active Cycles              cycle   3592636.02
    Compute (SM) Throughput           %        45.43
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77453.33
    Total DRAM Elapsed Cycles        cycle    217545728
    Average L1 Active Cycles         cycle   3592636.02
    Total L1 Elapsed Cycles          cycle    269235494
    Average L2 Active Cycles         cycle   1218096.42
    Total L2 Elapsed Cycles          cycle    114993744
    Average SM Active Cycles         cycle   3592636.02
    Total SM Elapsed Cycles          cycle    269235494
    Average SMSP Active Cycles       cycle   3590988.22
    Total SMSP Elapsed Cycles        cycle   1076941976
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.56%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.68% above the average, while the minimum instance value is 7.93% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.43%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.53% above the average, while the minimum instance value is 7.89% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.56%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.68% above the average, while the minimum instance value is 7.93% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.35
    Elapsed Cycles                cycle         6650
    Memory Throughput                 %        45.96
    DRAM Throughput                   %        45.96
    Duration                         us         8.48
    L1/TEX Cache Throughput           %        25.54
    L2 Cache Throughput               %        25.29
    SM Active Cycles              cycle      4424.07
    Compute (SM) Throughput           %        17.18
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.24
    Achieved Active Warps Per SM           warp        34.67
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.76%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24002.67
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4424.07
    Total L1 Elapsed Cycles          cycle       381566
    Average L2 Active Cycles         cycle      4099.83
    Total L2 Elapsed Cycles          cycle       165696
    Average SM Active Cycles         cycle      4424.07
    Total SM Elapsed Cycles          cycle       381566
    Average SMSP Active Cycles       cycle      4441.47
    Total SMSP Elapsed Cycles        cycle      1526264
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.20
    Elapsed Cycles                cycle         7184
    Memory Throughput                 %        49.45
    DRAM Throughput                   %        49.45
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.75
    L2 Cache Throughput               %        27.70
    SM Active Cycles              cycle      4976.71
    Compute (SM) Throughput           %        15.84
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.30
    Achieved Active Warps Per SM           warp        34.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.7%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27850.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4976.71
    Total L1 Elapsed Cycles          cycle       413612
    Average L2 Active Cycles         cycle      4397.33
    Total L2 Elapsed Cycles          cycle       178968
    Average SM Active Cycles         cycle      4976.71
    Total SM Elapsed Cycles          cycle       413612
    Average SMSP Active Cycles       cycle      4825.22
    Total SMSP Elapsed Cycles        cycle      1654448
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.87
    Elapsed Cycles                cycle         7127
    Memory Throughput                 %        49.73
    DRAM Throughput                   %        49.73
    Duration                         us         9.06
    L1/TEX Cache Throughput           %        23.47
    L2 Cache Throughput               %        27.96
    SM Active Cycles              cycle      4975.74
    Compute (SM) Throughput           %        15.67
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.42
    Achieved Active Warps Per SM           warp        34.76
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.58%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27752
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      4975.74
    Total L1 Elapsed Cycles          cycle       418148
    Average L2 Active Cycles         cycle      4498.79
    Total L2 Elapsed Cycles          cycle       177480
    Average SM Active Cycles         cycle      4975.74
    Total SM Elapsed Cycles          cycle       418148
    Average SMSP Active Cycles       cycle      4887.97
    Total SMSP Elapsed Cycles        cycle      1672592
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.16
    Elapsed Cycles                cycle         7152
    Memory Throughput                 %        49.41
    DRAM Throughput                   %        49.41
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.77
    L2 Cache Throughput               %        27.86
    SM Active Cycles              cycle      4955.91
    Compute (SM) Throughput           %        15.87
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.36
    Achieved Active Warps Per SM           warp        35.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.64%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27826.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4955.91
    Total L1 Elapsed Cycles          cycle       412878
    Average L2 Active Cycles         cycle      4450.75
    Total L2 Elapsed Cycles          cycle       178056
    Average SM Active Cycles         cycle      4955.91
    Total SM Elapsed Cycles          cycle       412878
    Average SMSP Active Cycles       cycle      4859.51
    Total SMSP Elapsed Cycles        cycle      1651512
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4643295
    Memory Throughput                 %        48.96
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.58
    SM Active Cycles              cycle   3591906.12
    Compute (SM) Throughput           %        45.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76546.67
    Total DRAM Elapsed Cycles        cycle    218839040
    Average L1 Active Cycles         cycle   3591906.12
    Total L1 Elapsed Cycles          cycle    269152882
    Average L2 Active Cycles         cycle   1216897.33
    Total L2 Elapsed Cycles          cycle    115644192
    Average SM Active Cycles         cycle   3591906.12
    Total SM Elapsed Cycles          cycle    269152882
    Average SMSP Active Cycles       cycle   3591753.13
    Total SMSP Elapsed Cycles        cycle   1076611528
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.49%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.60% above the average, while the minimum instance value is 7.84% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.49%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.60% above the average, while the minimum instance value is 7.89% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.49%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.60% above the average, while the minimum instance value is 7.84% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       786.03
    Elapsed Cycles                cycle         6647
    Memory Throughput                 %        45.52
    DRAM Throughput                   %        45.52
    Duration                         us         8.45
    L1/TEX Cache Throughput           %        25.53
    L2 Cache Throughput               %        25.34
    SM Active Cycles              cycle      4425.95
    Compute (SM) Throughput           %        17.05
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.25
    Achieved Active Warps Per SM           warp        34.68
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.75%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23773.33
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4425.95
    Total L1 Elapsed Cycles          cycle       384306
    Average L2 Active Cycles         cycle      3926.50
    Total L2 Elapsed Cycles          cycle       165456
    Average SM Active Cycles         cycle      4425.95
    Total SM Elapsed Cycles          cycle       384306
    Average SMSP Active Cycles       cycle      4315.99
    Total SMSP Elapsed Cycles        cycle      1537224
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.80
    Elapsed Cycles                cycle         7218
    Memory Throughput                 %        49.21
    DRAM Throughput                   %        49.21
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.59
    L2 Cache Throughput               %        27.60
    SM Active Cycles              cycle      4956.17
    Compute (SM) Throughput           %        15.74
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.08
    Achieved Active Warps Per SM           warp        35.56
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.92%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27800
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      4956.17
    Total L1 Elapsed Cycles          cycle       416404
    Average L2 Active Cycles         cycle      4524.88
    Total L2 Elapsed Cycles          cycle       179712
    Average SM Active Cycles         cycle      4956.17
    Total SM Elapsed Cycles          cycle       416404
    Average SMSP Active Cycles       cycle      4965.85
    Total SMSP Elapsed Cycles        cycle      1665616
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.59
    Elapsed Cycles                cycle         7099
    Memory Throughput                 %        49.80
    DRAM Throughput                   %        49.80
    Duration                         us         9.06
    L1/TEX Cache Throughput           %        24.17
    L2 Cache Throughput               %        28.05
    SM Active Cycles              cycle         4942
    Compute (SM) Throughput           %        16.13
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.39
    Achieved Active Warps Per SM           warp        34.75
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.61%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27792
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle         4942
    Total L1 Elapsed Cycles          cycle       406370
    Average L2 Active Cycles         cycle      4507.92
    Total L2 Elapsed Cycles          cycle       176616
    Average SM Active Cycles         cycle         4942
    Total SM Elapsed Cycles          cycle       406370
    Average SMSP Active Cycles       cycle      4941.01
    Total SMSP Elapsed Cycles        cycle      1625480
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       791.01
    Elapsed Cycles                cycle         7282
    Memory Throughput                 %        49.00
    DRAM Throughput                   %        49.00
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        24.25
    L2 Cache Throughput               %        27.68
    SM Active Cycles              cycle      5042.93
    Compute (SM) Throughput           %        16.19
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.32
    Achieved Active Warps Per SM           warp        35.67
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.68%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27765.33
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      5042.93
    Total L1 Elapsed Cycles          cycle       404918
    Average L2 Active Cycles         cycle      4580.21
    Total L2 Elapsed Cycles          cycle       179208
    Average SM Active Cycles         cycle      5042.93
    Total SM Elapsed Cycles          cycle       404918
    Average SMSP Active Cycles       cycle      4944.37
    Total SMSP Elapsed Cycles        cycle      1619672
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4643114
    Memory Throughput                 %        48.94
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3592025.64
    Compute (SM) Throughput           %        45.43
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76901.33
    Total DRAM Elapsed Cycles        cycle    218830848
    Average L1 Active Cycles         cycle   3592025.64
    Total L1 Elapsed Cycles          cycle    269247482
    Average L2 Active Cycles         cycle   1207540.38
    Total L2 Elapsed Cycles          cycle    115639752
    Average SM Active Cycles         cycle   3592025.64
    Total SM Elapsed Cycles          cycle    269247482
    Average SMSP Active Cycles       cycle   3592187.62
    Total SMSP Elapsed Cycles        cycle   1076989928
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.43%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.53% above the average, while the minimum instance value is 7.86% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.53%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.65% above the average, while the minimum instance value is 7.93% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.43%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.53% above the average, while the minimum instance value is 7.86% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.30
    Elapsed Cycles                cycle         6713
    Memory Throughput                 %        45.58
    DRAM Throughput                   %        45.58
    Duration                         us         8.54
    L1/TEX Cache Throughput           %        24.63
    L2 Cache Throughput               %        25.08
    SM Active Cycles              cycle      4588.26
    Compute (SM) Throughput           %        17.17
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.96
    Achieved Active Warps Per SM           warp        34.06
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24034.67
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4588.26
    Total L1 Elapsed Cycles          cycle       381602
    Average L2 Active Cycles         cycle      4098.25
    Total L2 Elapsed Cycles          cycle       167064
    Average SM Active Cycles         cycle      4588.26
    Total SM Elapsed Cycles          cycle       381602
    Average SMSP Active Cycles       cycle      4493.32
    Total SMSP Elapsed Cycles        cycle      1526408
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       786.91
    Elapsed Cycles                cycle         7180
    Memory Throughput                 %        49.21
    DRAM Throughput                   %        49.21
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.88
    L2 Cache Throughput               %        27.74
    SM Active Cycles              cycle      4959.29
    Compute (SM) Throughput           %        15.94
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.51
    Achieved Active Warps Per SM           warp        35.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.49%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27717.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4959.29
    Total L1 Elapsed Cycles          cycle       411206
    Average L2 Active Cycles         cycle      4477.50
    Total L2 Elapsed Cycles          cycle       178728
    Average SM Active Cycles         cycle      4959.29
    Total SM Elapsed Cycles          cycle       411206
    Average SMSP Active Cycles       cycle      4896.73
    Total SMSP Elapsed Cycles        cycle      1644824
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       784.46
    Elapsed Cycles                cycle         7131
    Memory Throughput                 %        49.50
    DRAM Throughput                   %        49.50
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.49
    L2 Cache Throughput               %        27.94
    SM Active Cycles              cycle      4965.88
    Compute (SM) Throughput           %        15.67
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.87
    Achieved Active Warps Per SM           warp        34.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.13%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27877.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4965.88
    Total L1 Elapsed Cycles          cycle       418184
    Average L2 Active Cycles         cycle      4459.75
    Total L2 Elapsed Cycles          cycle       177576
    Average SM Active Cycles         cycle      4965.88
    Total SM Elapsed Cycles          cycle       418184
    Average SMSP Active Cycles       cycle      4873.62
    Total SMSP Elapsed Cycles        cycle      1672736
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       784.22
    Elapsed Cycles                cycle         7130
    Memory Throughput                 %        49.43
    DRAM Throughput                   %        49.43
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.69
    L2 Cache Throughput               %        27.93
    SM Active Cycles              cycle      4972.91
    Compute (SM) Throughput           %        15.81
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.15
    Achieved Active Warps Per SM           warp        34.63
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.85%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27840
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4972.91
    Total L1 Elapsed Cycles          cycle       414636
    Average L2 Active Cycles         cycle      4516.88
    Total L2 Elapsed Cycles          cycle       177456
    Average SM Active Cycles         cycle      4972.91
    Total SM Elapsed Cycles          cycle       414636
    Average SMSP Active Cycles       cycle      4934.03
    Total SMSP Elapsed Cycles        cycle      1658544
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       799.01
    Elapsed Cycles                cycle      4651594
    Memory Throughput                 %        48.85
    DRAM Throughput                   %         0.21
    Duration                         ms         5.81
    L1/TEX Cache Throughput           %        63.23
    L2 Cache Throughput               %         3.60
    SM Active Cycles              cycle   3593008.74
    Compute (SM) Throughput           %        45.34
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     76565.33
    Total DRAM Elapsed Cycles        cycle    217847808
    Average L1 Active Cycles         cycle   3593008.74
    Total L1 Elapsed Cycles          cycle    269748318
    Average L2 Active Cycles         cycle   1228686.08
    Total L2 Elapsed Cycles          cycle    115134288
    Average SM Active Cycles         cycle   3593008.74
    Total SM Elapsed Cycles          cycle    269748318
    Average SMSP Active Cycles       cycle   3592487.89
    Total SMSP Elapsed Cycles        cycle   1078993272
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.46%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.61% above the average, while the minimum instance value is 7.86% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.46%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.61% above the average, while the minimum instance value is 7.97% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.46%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.61% above the average, while the minimum instance value is 7.86% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       781.23
    Elapsed Cycles                cycle         6653
    Memory Throughput                 %        45.18
    DRAM Throughput                   %        45.18
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        25.34
    L2 Cache Throughput               %        25.32
    SM Active Cycles              cycle      4458.52
    Compute (SM) Throughput           %        17.63
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.97
    Achieved Active Warps Per SM           warp        34.55
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.03%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23746.67
    Total DRAM Elapsed Cycles        cycle       315392
    Average L1 Active Cycles         cycle      4458.52
    Total L1 Elapsed Cycles          cycle       371714
    Average L2 Active Cycles         cycle      4028.17
    Total L2 Elapsed Cycles          cycle       165552
    Average SM Active Cycles         cycle      4458.52
    Total SM Elapsed Cycles          cycle       371714
    Average SMSP Active Cycles       cycle      4378.35
    Total SMSP Elapsed Cycles        cycle      1486856
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.87
    Elapsed Cycles                cycle         7175
    Memory Throughput                 %        49.19
    DRAM Throughput                   %        49.19
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.95
    L2 Cache Throughput               %        27.76
    SM Active Cycles              cycle      5054.43
    Compute (SM) Throughput           %        15.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.68
    Achieved Active Warps Per SM           warp        34.40
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.32%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27789.33
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      5054.43
    Total L1 Elapsed Cycles          cycle       410178
    Average L2 Active Cycles         cycle      4445.83
    Total L2 Elapsed Cycles          cycle       178632
    Average SM Active Cycles         cycle      5054.43
    Total SM Elapsed Cycles          cycle       410178
    Average SMSP Active Cycles       cycle      4908.44
    Total SMSP Elapsed Cycles        cycle      1640712
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.62
    Elapsed Cycles                cycle         7183
    Memory Throughput                 %        49.42
    DRAM Throughput                   %        49.42
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.88
    L2 Cache Throughput               %        27.71
    SM Active Cycles              cycle      5013.36
    Compute (SM) Throughput           %        15.94
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.19
    Achieved Active Warps Per SM           warp        34.65
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.81%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27832
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5013.36
    Total L1 Elapsed Cycles          cycle       411154
    Average L2 Active Cycles         cycle      4517.38
    Total L2 Elapsed Cycles          cycle       178824
    Average SM Active Cycles         cycle      5013.36
    Total SM Elapsed Cycles          cycle       411154
    Average SMSP Active Cycles       cycle      4964.96
    Total SMSP Elapsed Cycles        cycle      1644616
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.27
    Elapsed Cycles                cycle         7301
    Memory Throughput                 %        48.56
    DRAM Throughput                   %        48.56
    Duration                         us         9.31
    L1/TEX Cache Throughput           %        23.81
    L2 Cache Throughput               %        27.26
    SM Active Cycles              cycle      5006.10
    Compute (SM) Throughput           %        15.89
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.12
    Achieved Active Warps Per SM           warp        35.10
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.88%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27845.33
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      5006.10
    Total L1 Elapsed Cycles          cycle       412358
    Average L2 Active Cycles         cycle      4508.54
    Total L2 Elapsed Cycles          cycle       181872
    Average SM Active Cycles         cycle      5006.10
    Total SM Elapsed Cycles          cycle       412358
    Average SMSP Active Cycles       cycle      5016.23
    Total SMSP Elapsed Cycles        cycle      1649432
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4638065
    Memory Throughput                 %        48.97
    DRAM Throughput                   %         0.22
    Duration                         ms         5.83
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3591925.67
    Compute (SM) Throughput           %        45.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     79890.67
    Total DRAM Elapsed Cycles        cycle    218591232
    Average L1 Active Cycles         cycle   3591925.67
    Total L1 Elapsed Cycles          cycle    269110576
    Average L2 Active Cycles         cycle   1197241.50
    Total L2 Elapsed Cycles          cycle    115513752
    Average SM Active Cycles         cycle   3591925.67
    Total SM Elapsed Cycles          cycle    269110576
    Average SMSP Active Cycles       cycle   3592014.23
    Total SMSP Elapsed Cycles        cycle   1076442304
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.49%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.59% above the average, while the minimum instance value is 7.98% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.44%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.53% above the average, while the minimum instance value is 7.92% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.49%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.59% above the average, while the minimum instance value is 7.98% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       784.22
    Elapsed Cycles                cycle         6603
    Memory Throughput                 %        46.44
    DRAM Throughput                   %        46.44
    Duration                         us         8.42
    L1/TEX Cache Throughput           %        25.11
    L2 Cache Throughput               %        25.48
    SM Active Cycles              cycle      4499.72
    Compute (SM) Throughput           %        17.16
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.02
    Achieved Active Warps Per SM           warp        34.57
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24016
    Total DRAM Elapsed Cycles        cycle       310272
    Average L1 Active Cycles         cycle      4499.72
    Total L1 Elapsed Cycles          cycle       381890
    Average L2 Active Cycles         cycle      3963.96
    Total L2 Elapsed Cycles          cycle       164424
    Average SM Active Cycles         cycle      4499.72
    Total SM Elapsed Cycles          cycle       381890
    Average SMSP Active Cycles       cycle      4345.07
    Total SMSP Elapsed Cycles        cycle      1527560
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.96
    Elapsed Cycles                cycle         7167
    Memory Throughput                 %        49.37
    DRAM Throughput                   %        49.37
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.86
    L2 Cache Throughput               %        27.80
    SM Active Cycles              cycle      5017.48
    Compute (SM) Throughput           %        15.91
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.85
    Achieved Active Warps Per SM           warp        34.49
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.15%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27805.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5017.48
    Total L1 Elapsed Cycles          cycle       411878
    Average L2 Active Cycles         cycle      4396.12
    Total L2 Elapsed Cycles          cycle       178464
    Average SM Active Cycles         cycle      5017.48
    Total SM Elapsed Cycles          cycle       411878
    Average SMSP Active Cycles       cycle      4809.35
    Total SMSP Elapsed Cycles        cycle      1647512
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.50
    Elapsed Cycles                cycle         7253
    Memory Throughput                 %        49.00
    DRAM Throughput                   %        49.00
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.92
    L2 Cache Throughput               %        27.45
    SM Active Cycles              cycle      4877.28
    Compute (SM) Throughput           %        15.96
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.83
    Achieved Active Warps Per SM           warp        35.92
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27848
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4877.28
    Total L1 Elapsed Cycles          cycle       410606
    Average L2 Active Cycles         cycle      4400.83
    Total L2 Elapsed Cycles          cycle       180600
    Average SM Active Cycles         cycle      4877.28
    Total SM Elapsed Cycles          cycle       410606
    Average SMSP Active Cycles       cycle      4807.09
    Total SMSP Elapsed Cycles        cycle      1642424
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.89
    Elapsed Cycles                cycle         7182
    Memory Throughput                 %        49.33
    DRAM Throughput                   %        49.33
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.48
    L2 Cache Throughput               %        27.74
    SM Active Cycles              cycle      5061.41
    Compute (SM) Throughput           %        15.67
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.45
    Achieved Active Warps Per SM           warp        34.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.55%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27784
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5061.41
    Total L1 Elapsed Cycles          cycle       418288
    Average L2 Active Cycles         cycle      4520.79
    Total L2 Elapsed Cycles          cycle       178968
    Average SM Active Cycles         cycle      5061.41
    Total SM Elapsed Cycles          cycle       418288
    Average SMSP Active Cycles       cycle      4937.96
    Total SMSP Elapsed Cycles        cycle      1673152
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4643327
    Memory Throughput                 %        48.98
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.58
    SM Active Cycles              cycle   3592275.90
    Compute (SM) Throughput           %        45.46
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        76712
    Total DRAM Elapsed Cycles        cycle    218841088
    Average L1 Active Cycles         cycle   3592275.90
    Total L1 Elapsed Cycles          cycle    269038564
    Average L2 Active Cycles         cycle   1213454.54
    Total L2 Elapsed Cycles          cycle    115645128
    Average SM Active Cycles         cycle   3592275.90
    Total SM Elapsed Cycles          cycle    269038564
    Average SMSP Active Cycles       cycle   3591919.13
    Total SMSP Elapsed Cycles        cycle   1076154256
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.44%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.52% above the average, while the minimum instance value is 7.92% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.54%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.65% above the average, while the minimum instance value is 7.95% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.44%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.52% above the average, while the minimum instance value is 7.92% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       782.45
    Elapsed Cycles                cycle         6639
    Memory Throughput                 %        45.86
    DRAM Throughput                   %        45.86
    Duration                         us         8.48
    L1/TEX Cache Throughput           %        25.48
    L2 Cache Throughput               %        25.38
    SM Active Cycles              cycle      4434.07
    Compute (SM) Throughput           %        17.16
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.71
    Achieved Active Warps Per SM           warp        34.42
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.29%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23792
    Total DRAM Elapsed Cycles        cycle       311296
    Average L1 Active Cycles         cycle      4434.07
    Total L1 Elapsed Cycles          cycle       381902
    Average L2 Active Cycles         cycle      4021.04
    Total L2 Elapsed Cycles          cycle       165144
    Average SM Active Cycles         cycle      4434.07
    Total SM Elapsed Cycles          cycle       381902
    Average SMSP Active Cycles       cycle      4418.80
    Total SMSP Elapsed Cycles        cycle      1527608
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       783.75
    Elapsed Cycles                cycle         7152
    Memory Throughput                 %        49.50
    DRAM Throughput                   %        49.50
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        24.15
    L2 Cache Throughput               %        27.85
    SM Active Cycles              cycle      4980.66
    Compute (SM) Throughput           %        16.12
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.97
    Achieved Active Warps Per SM           warp        35.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.03%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27709.33
    Total DRAM Elapsed Cycles        cycle       335872
    Average L1 Active Cycles         cycle      4980.66
    Total L1 Elapsed Cycles          cycle       406520
    Average L2 Active Cycles         cycle      4389.54
    Total L2 Elapsed Cycles          cycle       177984
    Average SM Active Cycles         cycle      4980.66
    Total SM Elapsed Cycles          cycle       406520
    Average SMSP Active Cycles       cycle      4827.30
    Total SMSP Elapsed Cycles        cycle      1626080
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       781.90
    Elapsed Cycles                cycle         7185
    Memory Throughput                 %        49.08
    DRAM Throughput                   %        49.08
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.86
    L2 Cache Throughput               %        27.77
    SM Active Cycles              cycle      4891.81
    Compute (SM) Throughput           %        15.92
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.69
    Achieved Active Warps Per SM           warp        35.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.31%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27810.67
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      4891.81
    Total L1 Elapsed Cycles          cycle       411566
    Average L2 Active Cycles         cycle      4504.42
    Total L2 Elapsed Cycles          cycle       178776
    Average SM Active Cycles         cycle      4891.81
    Total SM Elapsed Cycles          cycle       411566
    Average SMSP Active Cycles       cycle      4942.56
    Total SMSP Elapsed Cycles        cycle      1646264
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.49
    Elapsed Cycles                cycle         7314
    Memory Throughput                 %        48.43
    DRAM Throughput                   %        48.43
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        23.68
    L2 Cache Throughput               %        27.22
    SM Active Cycles              cycle      4991.45
    Compute (SM) Throughput           %        15.80
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.25
    Achieved Active Warps Per SM           warp        35.16
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.75%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27853.33
    Total DRAM Elapsed Cycles        cycle       345088
    Average L1 Active Cycles         cycle      4991.45
    Total L1 Elapsed Cycles          cycle       414848
    Average L2 Active Cycles         cycle      4495.54
    Total L2 Elapsed Cycles          cycle       182112
    Average SM Active Cycles         cycle      4991.45
    Total SM Elapsed Cycles          cycle       414848
    Average SMSP Active Cycles       cycle      4900.44
    Total SMSP Elapsed Cycles        cycle      1659392
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 128, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(128, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4639302
    Memory Throughput                 %        48.95
    DRAM Throughput                   %         0.21
    Duration                         ms         5.84
    L1/TEX Cache Throughput           %        63.25
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle   3591872.59
    Compute (SM) Throughput           %        45.43
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           29.44
    # SMs                                         SM              58
    Threads                                   thread           16384
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        17.60
    Achieved Active Warps Per SM           warp         8.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        76464
    Total DRAM Elapsed Cycles        cycle    218651648
    Average L1 Active Cycles         cycle   3591872.59
    Total L1 Elapsed Cycles          cycle    269211180
    Average L2 Active Cycles         cycle   1211151.96
    Total L2 Elapsed Cycles          cycle    115544592
    Average SM Active Cycles         cycle   3591872.59
    Total SM Elapsed Cycles          cycle    269211180
    Average SMSP Active Cycles       cycle   3591749.16
    Total SMSP Elapsed Cycles        cycle   1076844720
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 17.43%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 22.52% above the average, while the minimum instance value is 7.87% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.49%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 22.61% above the average, while the minimum instance value is 7.96% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.43%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.52% above the average, while the minimum instance value is 7.87% below the       
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       782.85
    Elapsed Cycles                cycle         6672
    Memory Throughput                 %        45.97
    DRAM Throughput                   %        45.97
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        25.09
    L2 Cache Throughput               %        25.24
    SM Active Cycles              cycle      4503.88
    Compute (SM) Throughput           %        17.46
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.24
    Achieved Active Warps Per SM           warp        34.67
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.76%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24008
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4503.88
    Total L1 Elapsed Cycles          cycle       375306
    Average L2 Active Cycles         cycle      4030.62
    Total L2 Elapsed Cycles          cycle       166152
    Average SM Active Cycles         cycle      4503.88
    Total SM Elapsed Cycles          cycle       375306
    Average SMSP Active Cycles       cycle      4424.25
    Total SMSP Elapsed Cycles        cycle      1501224
    -------------------------- ----------- ------------

